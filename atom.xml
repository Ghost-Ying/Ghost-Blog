<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>爱影客</title>
  
  
  <link href="https://aiyingke.cn/atom.xml" rel="self"/>
  
  <link href="https://aiyingke.cn/"/>
  <updated>2023-02-20T10:07:09.566Z</updated>
  <id>https://aiyingke.cn/</id>
  
  <author>
    <name>Rupert-Tears</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark 常用算子</title>
    <link href="https://aiyingke.cn/blog/bda70396.html/"/>
    <id>https://aiyingke.cn/blog/bda70396.html/</id>
    <published>2023-02-20T08:43:05.000Z</published>
    <updated>2023-02-20T10:07:09.566Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：算子概述"><a href="#一：算子概述" class="headerlink" title="一：算子概述"></a>一：算子概述</h1><h2 id="1-1-什么是算子？"><a href="#1-1-什么是算子？" class="headerlink" title="1.1 什么是算子？"></a>1.1 什么是算子？</h2><ul><li><p>英文：Operator</p></li><li><p>狭义：一个函数空间到另一个函数空间的映射</p></li><li><p>广义：一个空间到另个一个空间的映射</p></li><li><p>白话：一个事物从一个状态到另一个状态的过程</p></li><li><p>实质：映射，即关系</p></li></ul><h2 id="1-2-算子的重要作用"><a href="#1-2-算子的重要作用" class="headerlink" title="1.2 算子的重要作用"></a>1.2 算子的重要作用</h2><ul><li>算子越多，灵活性越高，编程的可选方式就越多</li><li>算子越多，表现能力强，可以灵活应对各种复杂场景</li></ul><h2 id="1-3-MapReduce-和-Spark-算子比较"><a href="#1-3-MapReduce-和-Spark-算子比较" class="headerlink" title="1.3 MapReduce 和 Spark 算子比较"></a>1.3 MapReduce 和 Spark 算子比较</h2><ul><li>MapReduce 只有2个算子，map和reduce，绝大多数场景下，需要复杂的编程来完成业务需求</li><li>Spark 有80多个算子，可以灵活组合应对不同的业务场景</li></ul><h1 id="二：Spark算子"><a href="#二：Spark算子" class="headerlink" title="二：Spark算子"></a>二：Spark算子</h1><h2 id="2-1-转换算子（transformation）"><a href="#2-1-转换算子（transformation）" class="headerlink" title="2.1 转换算子（transformation）"></a>2.1 转换算子（transformation）</h2><p>此种算子不会真正的触发提交作业，只有作业被提交后才会触发转换计算</p><ul><li>value型转换算子（处理的数据项是value型）<ul><li>输入分区：输出分区 &#x3D; 1 ： 1<ul><li>map算子</li><li>flatMap算子</li><li>mapPartitions算子</li></ul></li><li>输入分区：输出分区 &#x3D; n ： 1<ul><li>union算子</li><li>cartesian算子</li></ul></li><li>输入分区 ：输出分区 &#x3D; n ： n<ul><li>groupBy算子</li></ul></li><li>输出分区为输入分区的子集<ul><li>filter算子</li><li>distinct算子</li><li>substract算子</li><li>sample算子</li><li>takeSample算子</li></ul></li><li>cache型算子<ul><li>cache算子</li><li>persist算子</li></ul></li></ul></li><li>key-value型转换算子（处理的数据类型是key-value型）<ul><li>输入分区：输出分区 &#x3D; 1： 1<ul><li>mapValues 算子</li></ul></li><li>对单个RDD聚集<ul><li>combineByKey算子</li><li>reduceByKey算子</li><li>partitionBy算子</li></ul></li><li>对两个RDD聚合<ul><li>cogroup算子</li></ul></li><li>连接<ul><li>join算子</li><li>leftOutJoin算子</li><li>rightOutJoin算子</li></ul></li></ul></li></ul><h2 id="2-2-行动算子（action）"><a href="#2-2-行动算子（action）" class="headerlink" title="2.2 行动算子（action）"></a>2.2 行动算子（action）</h2><p>这种算子会触发sparkContent提交作业；</p><ul><li>无输出（不生成文件）<ul><li>foreach算子</li></ul></li><li>HDFS<ul><li>saveAsTextFile算子</li><li>saveAsObjectFile算子</li></ul></li><li>scala集合和数据类型<ul><li>collect算子</li><li>collectAsMap算子</li><li>reduceByKeyLocally算子</li><li>lookup算子</li><li>count算子</li><li>top算子</li><li>reduce算子</li><li>fold算子</li><li>aggregate算子</li></ul></li></ul><h1 id="三：常见算子的应用场景"><a href="#三：常见算子的应用场景" class="headerlink" title="三：常见算子的应用场景"></a>三：常见算子的应用场景</h1><h2 id="3-1-转换算子（transformation）"><a href="#3-1-转换算子（transformation）" class="headerlink" title="3.1 转换算子（transformation）"></a>3.1 转换算子（transformation）</h2><h3 id="3-1-1-value型转换算子"><a href="#3-1-1-value型转换算子" class="headerlink" title="3.1.1 value型转换算子"></a>3.1.1 value型转换算子</h3><h4 id="（1）map"><a href="#（1）map" class="headerlink" title="（1）map"></a>（1）map</h4><p>类比mapreduce中的map操作，给定一个输入，由map函数操作后，成为一个新的元素输出；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;Hello&quot;,&quot;Word&quot;,&quot;你好&quot;,&quot;世界&quot;),2)</span><br><span class="line">val second = first.map(_.length)</span><br><span class="line">second.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361528641-4cccbb47-74f0-4033-a43f-a80da85ecadd.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(1 to 5,2)</span><br><span class="line">first.map(1 to _).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361687877-18444583-2032-4606-b1c2-59d7a858da27.png"></p><h4 id="（2）flatMap"><a href="#（2）flatMap" class="headerlink" title="（2）flatMap"></a>（2）flatMap</h4><p>给定一个二维的输入（线式输入），将返回的所有结果打平成一个一维的集合结构（点式集合输出）;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(1 to 5,2)</span><br><span class="line">first.flatMap(1 to _).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361925095-d5967335-e5d8-4136-b9dc-40aeaf574e03.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;),2)</span><br><span class="line">first.flatMap(x =&gt;List(x,x,x)).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362079746-1298c815-0023-41a3-9c3c-d3adf7f565c4.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;),2)</span><br><span class="line">first.flatMap(x =&gt; List(x+&quot;_1&quot;,x+&quot;_2&quot;,x+&quot;_3&quot;)).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362222555-086c43f1-848f-4990-9fbd-d369bdb90f42.png"></p><h4 id="（3）mapPartitions"><a href="#（3）mapPartitions" class="headerlink" title="（3）mapPartitions"></a>（3）mapPartitions</h4><p>以分区为单位进行计算处理；</p><p>在map过程中，需要频繁创建额外对象时，如文件输出流操作、jdbc操作、socket操作，使用mapPartitions算子；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Seq(1,2,3,4,5),3)</span><br><span class="line">var rdd2 = rdd.mapPartitions(partition =&gt; &#123;</span><br><span class="line">// 在此处可以加入jdbc一次初始化多少次使用的代码</span><br><span class="line">partition.map(num =&gt; num * num)</span><br><span class="line">&#125;)</span><br><span class="line">rdd2.max</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362591147-55198903-ddac-4824-9735-ad45e0648084.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Seq(1,2,3,4),3)</span><br><span class="line">var rdd2 = rdd.mapPartitions(partition =&gt;&#123;</span><br><span class="line">partition.flatMap(1 to _)</span><br><span class="line">&#125;)</span><br><span class="line">rdd2.count</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362787238-09fde06d-4c8a-4a76-bf4f-81cd63d4888d.png"></p><h4 id="（4）glom"><a href="#（4）glom" class="headerlink" title="（4）glom"></a>（4）glom</h4><p>以分区为单位，将每个分区的值形成一个数组；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(Seq(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;,&quot;four&quot;,&quot;five&quot;),3)</span><br><span class="line">a.glom.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362965109-f019c760-0cca-48e7-b77e-72a72b865c73.png"></p><p>由上诉得到：分组的依据是平均分组</p><h4 id="（5）union"><a href="#（5）union" class="headerlink" title="（5）union"></a>（5）union</h4><p>将2个rdd合并成一个rdd，不去重；有时可能会发生多个分区合并成一个分区的情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 4,2)</span><br><span class="line">val b = sc.parallelize(6 to 10,2)</span><br><span class="line">a.union(b).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363148863-5e326e0c-8090-44dc-9235-fc502b1b020d.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a union b).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363185922-78c5a488-f745-423f-ba9f-5a1a8aed72f0.png"></p><h4 id="（6）groupBy"><a href="#（6）groupBy" class="headerlink" title="（6）groupBy"></a>（6）groupBy</h4><p>输入分区和输出分区 n : n型</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(Seq(1,2,3,4,5,56,67),3)</span><br><span class="line">a.groupBy(x =&gt; &#123;if(x&gt;10) &quot;&gt;10&quot; else &quot;&lt;=10&quot;&#125;).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363353742-3a6f1e5a-9e2b-4830-aa7a-6746670268cf.png"></p><h4 id="（7）filter"><a href="#（7）filter" class="headerlink" title="（7）filter"></a>（7）filter</h4><p>输出为输入的子集；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 4,3)</span><br><span class="line">val b = a.filter(_%4 == 0)</span><br><span class="line">b.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363470345-d3527b6f-d8cc-4cda-9075-5295be2fc72f.png"></p><h4 id="（8）distinct"><a href="#（8）distinct" class="headerlink" title="（8）distinct"></a>（8）distinct</h4><p>输出分区为输入分区的子集，全局去重；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,3)</span><br><span class="line">val b = sc.parallelize(2 to 9,3)</span><br><span class="line">a.union(b).distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363723963-576ad83b-d590-475d-81b6-09e20703585d.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val c = sc.parallelize(List(&quot;小红&quot;,&quot;消化&quot;,&quot;不良&quot;,&quot;消化&quot;))</span><br><span class="line">c.distinct.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363870738-97f17c6b-c2c6-4145-be17-a744e0697ecb.png"></p><h4 id="（9）cache"><a href="#（9）cache" class="headerlink" title="（9）cache"></a>（9）cache</h4><p>cache将rdd元素从磁盘缓存到内存中，数据反复被使用的场景使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,2)</span><br><span class="line">val b = sc.parallelize(2 to 4,2)</span><br><span class="line">a.union(b).count</span><br><span class="line">a.distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629365250564-1438ac38-78bf-4c84-b7c5-1337feebeeca.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,2)</span><br><span class="line">val b = sc.parallelize(2 to 5,3)</span><br><span class="line">val c = a.union(b).cache</span><br><span class="line">c.count</span><br><span class="line">c.distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629365410401-cff04c88-5a41-4aa0-a8cd-3d9bc4f8c045.png"></p><h3 id="3-1-2-key-value型转换算子"><a href="#3-1-2-key-value型转换算子" class="headerlink" title="3.1.2 key-value型转换算子"></a>3.1.2 key-value型转换算子</h3><h4 id="（1）mapValues"><a href="#（1）mapValues" class="headerlink" title="（1）mapValues"></a>（1）mapValues</h4><p>输入分区：输出分区 &#x3D; 1 ： 1</p><p>针对key-value型数据中的value进行map操作，而不对key进行处理；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张三&quot;,1),(&quot;李四&quot;,2),(&quot;王五&quot;,3)),2)</span><br><span class="line">val second = first.mapValues(x =&gt; x+1)</span><br><span class="line">second.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629366240118-49f73664-486d-4acd-a779-46fe1aa28042.png"></p><h4 id="（2）★-combineByKey-★"><a href="#（2）★-combineByKey-★" class="headerlink" title="（2）★ combineByKey ★"></a>（2）★ combineByKey ★</h4><p>定义</p><p>def combineByKey [C] (</p><p>createCombiner: (V) &#x3D;&gt; C,</p><p>mergeValue: (C,V) &#x3D;&gt; C,</p><p>mergeCombiners: (C,C) &#x3D;&gt; C): RDD[(String, C)]</p><p>元素</p><p>createCombiner对每个分区内的同组元素如何聚合，形成一个累加器</p><p>mergeValue 将累加器与新遇到的值进行合并的方法</p><p>mergeCombiners 每个分区都是独立处理，故同一个键可以有多个累加器。如果两个或者多个分区都对应同一个键的累加器，用方法将各个分区的结果进行合并。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张一&quot;,4),(&quot;张一&quot;,2),(&quot;张三&quot;,3)),2)</span><br><span class="line">val second =first.combineByKey(List(_), (x:List[Int],y:Int) =&gt; y::x, (x:List[Int], y:List[Int]) =&gt; x:::y)</span><br><span class="line">second.collect </span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629367854547-9a9fa874-7f66-4d7d-a216-a511bfb5b502.png"></p><h4 id="（3）reduceByKey"><a href="#（3）reduceByKey" class="headerlink" title="（3）reduceByKey"></a>（3）reduceByKey</h4><p>按key聚合后对组进行规约处理，求和</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;小米&quot;,&quot;华硕&quot;,&quot;很郁闷&quot;),2)</span><br><span class="line">val second = first.map(x =&gt; (x,1))</span><br><span class="line">second.reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629370722139-e07aa367-7b31-4818-a81e-fb58087606bf.png"></p><h4 id="（4）join"><a href="#（4）join" class="headerlink" title="（4）join"></a>（4）join</h4><p>对 key-value 结构的rdd进行按key的join操作，最后将V部分做flatMap打平操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张三&quot;,11),(&quot;李四&quot;,12)),2)</span><br><span class="line">val seconed = sc.parallelize(List((&quot;张一&quot;,2),(&quot;李二&quot;,3),(&quot;李四&quot;,3)),3)</span><br><span class="line">first.join(seconed).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629371167534-5d370cc0-f9b8-4785-a30a-f89d51d0d802.png"></p><h2 id="3-2-行动算子-action"><a href="#3-2-行动算子-action" class="headerlink" title="3.2 行动算子 action"></a>3.2 行动算子 action</h2><p>该类型算子会触发SparkContext提交作业，触发RDD DAG的执行</p><h4 id="（1）无输出型，不落地本地文件或hsfs文件"><a href="#（1）无输出型，不落地本地文件或hsfs文件" class="headerlink" title="（1）无输出型，不落地本地文件或hsfs文件"></a>（1）无输出型，不落地本地文件或hsfs文件</h4><p>foreach算子</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;华米&quot;,&quot;小米&quot;,&quot;苹果&quot;,&quot;华米&quot;,&quot;三星&quot;),2)</span><br><span class="line">first.foreach(println _)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629371520895-6d080f91-c686-4d4f-9dfa-514612d51c7e.png"></p><h4 id="（2）HDFS输出型"><a href="#（2）HDFS输出型" class="headerlink" title="（2）HDFS输出型"></a>（2）HDFS输出型</h4><p>saveAsTextFile算子</p><h4 id="（3）scala集合和数据类型"><a href="#（3）scala集合和数据类型" class="headerlink" title="（3）scala集合和数据类型"></a>（3）scala集合和数据类型</h4><h4 id="（4）collectAsMap算子"><a href="#（4）collectAsMap算子" class="headerlink" title="（4）collectAsMap算子"></a>（4）collectAsMap算子</h4><h4 id="（5）lookup算子"><a href="#（5）lookup算子" class="headerlink" title="（5）lookup算子"></a>（5）lookup算子</h4><h4 id="（6）reduce算子"><a href="#（6）reduce算子" class="headerlink" title="（6）reduce算子"></a>（6）reduce算子</h4><h4 id="（7）fold算子fold算子："><a href="#（7）fold算子fold算子：" class="headerlink" title="（7）fold算子fold算子："></a>（7）fold算子fold算子：</h4><p>def fold (zeroValue: T)(op: (T, T) &#x3D;&gt; T) : T</p><p>先对rdd分区的每一个分区进行op函数</p><p>在调用op函数过程中将zeroValue参与计算</p><p>最后在对所有分区的结果调用op函数，同事在此处进行zeroValue再次参与计算</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是41 公式：（1+2+3+4+5+6+10）+10</span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),1).fold(10)(_+_)</span><br></pre></td></tr></table></figure><h1 id="四：其他常用算子"><a href="#四：其他常用算子" class="headerlink" title="四：其他常用算子"></a>四：其他常用算子</h1><ul><li>cartesian算子</li><li>subtract算子</li><li>sample算子</li><li>takeSample算子</li><li>persist算子</li><li>cogroup算子</li><li>leftOuterJoin算子</li><li>rightOuterJoin算子</li><li>saveAsObjectFile算子</li><li>count算子</li><li>top算子</li><li>aggregate算子</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：算子概述&quot;&gt;&lt;a href=&quot;#一：算子概述&quot; class=&quot;headerlink&quot; title=&quot;一：算子概述&quot;&gt;&lt;/a&gt;一：算子概述&lt;/h1&gt;&lt;h2 id=&quot;1-1-什么是算子？&quot;&gt;&lt;a href=&quot;#1-1-什么是算子？&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Flink 架构设计</title>
    <link href="https://aiyingke.cn/blog/50ac08de.html/"/>
    <id>https://aiyingke.cn/blog/50ac08de.html/</id>
    <published>2023-02-19T17:27:27.000Z</published>
    <updated>2023-02-19T18:36:19.393Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Flink-架构设计图"><a href="#一：Flink-架构设计图" class="headerlink" title="一：Flink 架构设计图"></a>一：Flink 架构设计图</h1><p><img src="/blog/50ac08de.html/1630318437801-84613c7b-a14d-4311-b66b-9b3ac56824bd.png"></p><h2 id="1-1-分层设计说明"><a href="#1-1-分层设计说明" class="headerlink" title="1.1 分层设计说明"></a>1.1 分层设计说明</h2><ul><li>物理部署层 -deploy层<ul><li>负责解决Flink的部署模式问题</li><li>支持多种部署模式：本地部署、集群部署（standalone&#x2F;yarn&#x2F;mesos）、云（GCE&#x2F;EC2）以及 kubernetes 。</li><li>通过该层支持不同平台的部署，用户可以根据自身场景和需求使用对应的部署模式；</li></ul></li><li>Runtime核心层<ul><li>是Flink分布式计算框架的核心实现层，负责对上层不同接口提供基础服务。</li><li>支持分布式steam作业的执行、jobGraph到ExecutionGraph的映射转换以及任务调度等。</li><li>将DataStream和DataSet转成统一的可执行的Task Operator，达到在流式计算引擎下同时处理批量计算和流式计算的目的。</li></ul></li><li>API &amp; Libraries 层<ul><li>负责更好的开发用户体验，包括易用性、开发效率、执行效率、状态管理等方面。</li><li>Flink同时提供了支撑流计算和批处理的接口，同时在这基础上抽象出不同的应用类型的组件库，如：<ul><li>基于流处理的CEP（复杂事件处理库）</li><li>Table &amp; Sql库</li><li>基于批处理的FlinkML（机器学习库）</li><li>图处理库(Gelly)</li></ul></li><li>API层包括两部分<ul><li>流计算应用的DataStream API</li><li>批处理应用的DataSet API</li><li>统一的API，方便用于直接操作状态和时间等底层数据<ul><li>提供了丰富的数据处理高级API，例如Map、FllatMap操作等</li><li>并提供了比较低级的Process Function API</li></ul></li></ul></li></ul></li></ul><h1 id="二：运行模式"><a href="#二：运行模式" class="headerlink" title="二：运行模式"></a>二：运行模式</h1><h2 id="2-1-运行模式核心区分点"><a href="#2-1-运行模式核心区分点" class="headerlink" title="2.1 运行模式核心区分点"></a>2.1 运行模式核心区分点</h2><ul><li>集群生命周期和资源隔离保证</li><li>应用程序的main()方法是在客户端还是在集群上执行</li></ul><h2 id="2-2-所有模式分类说明"><a href="#2-2-所有模式分类说明" class="headerlink" title="2.2 所有模式分类说明"></a>2.2 所有模式分类说明</h2><ul><li>本地运行模式</li><li>standalone模式</li><li>集群运行模式<ul><li>经常是指flink on yarn集群模式</li><li>yarn也可以换成mesos,Kubernetes(k8s)等资源管理平台替换</li><li>共三种<ul><li>session 模式</li><li>per-job 模式</li><li>application 模式</li></ul></li></ul></li></ul><h2 id="2-3-本地运行模式"><a href="#2-3-本地运行模式" class="headerlink" title="2.3 本地运行模式"></a>2.3 本地运行模式</h2><ul><li>运行过程：一个机器启动一个进程的多线程来模拟分布式计算。</li><li>主要用于代码测试</li></ul><h2 id="2-4-standalone模式"><a href="#2-4-standalone模式" class="headerlink" title="2.4 standalone模式"></a>2.4 standalone模式</h2><ul><li>运行过程：完全独立的Flink集群的模式，各个环节均Flink自己搞定。并没有yarn、mesos的统一资源调度平台。</li><li>主要是只有纯Flink纯计算的场景，商用场景极少。</li></ul><h2 id="2-5-集群运行模式"><a href="#2-5-集群运行模式" class="headerlink" title="2.5 集群运行模式"></a>2.5 集群运行模式</h2><h3 id="（1）Flink-Session-集群（会话模式）"><a href="#（1）Flink-Session-集群（会话模式）" class="headerlink" title="（1）Flink Session 集群（会话模式）"></a>（1）Flink Session 集群（会话模式）</h3><ul><li><ul><li>集群生命周期：<ul><li>在 Flink Session 集群中，客户端连接到一个预先存在的、长期运行的集群；</li><li>该集群可以接受多个作业提交。即使所有作业完成后，集群（和 JobManager）仍将继续运行直到手动停止 session 为止。</li><li>因此，<code>Flink Session 集群的寿命不受任何 Flink 作业寿命的约束</code>。</li></ul></li><li>TaskManager slot 由 ResourceManager 在提交作业时分配，并在作业完成时释放。<ul><li>由于所有作业都共享同一集群，因此在集群资源方面存在一些竞争——例如提交工作阶段的网络带宽。</li><li>此共享设置的局限性在于，如果 TaskManager 崩溃，则在此 TaskManager 上运行 task 的所有作业都将失败；</li><li>再比如，如果 JobManager 上发生一些致命错误，它将影响集群中正在运行的所有作业。</li></ul></li><li>其他注意事项：<ul><li>拥有一个预先存在的集群可以节省大量时间申请资源和启动 TaskManager。</li><li>有种场景很重要，作业执行时间短并且启动时间长会对端到端的用户体验产生负面的影响。<ul><li>就像对简短查询的交互式分析一样，希望作业可以使用现有资源快速执行计算。</li></ul></li></ul></li><li>Flink Session 集群也被称为 session 模式下的 Flink 集群。</li><li>工作模式<ul><li>附加模式（默认）<ul><li>特点<ul><li>客户端与Flink作业集群相互同步</li></ul></li><li>细节描述<ul><li>yarn-session.sh 客户端将 Flink 集群提交给 YARN，但客户端保持运行，跟踪集群的状态。</li><li>如果集群失败，客户端将显示错误。如果客户端被终止，它也会通知集群关闭。</li></ul></li></ul></li><li>分离模式（-d或–detached）<ul><li>特点<ul><li>客户端与Flink作业集群相互异步，客户端提交完成后即可退出</li></ul></li><li>细节描述<ul><li>yarn-session.sh 客户端将Flink集群提交给YARN，然后客户端返回。</li><li>需要再次调用客户端或 YARN 工具来停止 Flink 集群。</li></ul></li></ul></li></ul></li><li>工作流程特征说明<ul><li>多个不同的FlinkJob向同一个Flink Session会话上提交作业，由这一个统一个的FlinkSession管理所有的Flink作业。</li><li>工作流程示意图</li></ul></li></ul></li><li><p><img src="/blog/50ac08de.html/1630323312227-b12cae9d-16c2-47b1-a1cf-e697d719c689.png"></p></li></ul><h3 id="（2）Flink-Job-集群（per-job模式）"><a href="#（2）Flink-Job-集群（per-job模式）" class="headerlink" title="（2）Flink Job 集群（per-job模式）"></a>（2）Flink Job 集群（per-job模式）</h3><ul><li>Flink Job 集群也被称为 job (or per-job) 模式下的 Flink 集群。</li><li>集群生命周期：<ul><li>在 Flink Job 集群中，可用的集群管理器（例如 YARN）用于为每个提交的作业启动一个集群，并且该集群仅可用于该作业。</li><li>在这里客户端首先从集群管理器请求资源启动 JobManager，然后将作业提交给在这个进程中运行的 Dispatcher。然后根据作业的资源请求惰性的分配 TaskManager。</li><li>一旦作业完成，Flink Job 集群将被拆除。</li></ul></li><li>资源隔离：<ul><li>JobManager 中的致命错误仅影响在 Flink Job 集群中运行的一个作业。</li></ul></li><li>其他注意事项：<ul><li>由于 ResourceManager 必须应用并等待外部资源管理组件来启动 TaskManager 进程和分配资源，所以其实时计算性并没有session模式强</li><li>因此 Flink Job 集群更适合长期运行、具有高稳定性要求且对较长的启动时间不敏感的大型作业；</li></ul></li><li>工作流程特征说明：<ul><li>多个不同的FlinkJob向各自由统一资源管理器(Yarn)生成的专用Flink Session会话上提交作业，由作业所属的FlinkSession管理自己的Flink作业。</li><li>工作流程示意图</li></ul></li></ul><p><img src="/blog/50ac08de.html/1630323597224-1c423290-947a-44b4-9101-7adce28b5202.png"></p><h3 id="（3）Flink-Application-集群（应用模式）"><a href="#（3）Flink-Application-集群（应用模式）" class="headerlink" title="（3）Flink Application 集群（应用模式）"></a>（3）Flink Application 集群（应用模式）</h3><ul><li>Flink Job 集群可以看做是 Flink Application 集群”客户端运行“的替代方案。</li><li>该模式为yarn session和yarn per-job模式的折中选择。</li><li>集群生命周期：<ul><li>Flink Application 集群是与Flink作业执行直接相关的运行模式，并且 <code>main()方法在集群上而不是客户端上运行</code>。</li><li>提交作业是一个单步骤过程：<ul><li>无需先启动 Flink 集群，然后将作业提交到现有的 session 集群。</li><li>将应用程序逻辑和依赖打包成一个可执行的作业 JAR 中，由入口机客户端提交jar包和相关资源到hdfs当中。</li><li>由调度启动的集群当中JobManager来拉取已由上一步上传完成的运行代码所需要的所有资源。</li><li>如果有JobManager HA设置的话，将会同时启动多个JobManager作HA保障，此时将面临JobManager的选择，最终由一个胜出的JobManager作为Active进程推进下边的执行。</li><li>由运行JobManager进程的集群入口点节点机器（ApplicationClusterEntryPoint）负责调用 main()方法来提取 JobGraph，即作为用户程序的解析和提交的客户端程序与集群进行交互，直到作业运行完成。</li><li>如果一个main()方法中有多个env.execute()&#x2F;executeAsync()调用，在Application模式下，这些作业会被视为属于同一个应用，在同一个集群中执行（如果在Per-Job模式下，就会启动多个集群）</li><li>该模式也允许我们像在 Kubernetes 上部署任何其他应用程序一样部署 Flink 应用程序。</li><li>因此，Flink Application 集群的寿命与 Flink 应用程序的寿命有关。</li></ul></li></ul></li><li>资源隔离：<ul><li>在 Flink Application 集群中，ResourceManager 和 Dispatcher 作用于单个的 Flink 应用程序，相比于 Flink Session 集群，它提供了更好的隔离。</li></ul></li><li>工作流程特征说明：<ul><li>将各个环节更进一步进行专用化处理，相当于每个FlinkJob都有一套专用的服务角色进程。</li></ul></li></ul><h3 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h3><ul><li>应用场景<ul><li>本地布署模式：demo、代码测试场景。</li><li>Session模式：集群资源充分、频繁任务提交、小作业居多、实时性要求高的场景。(该模式较少）</li><li>Per-Job模式：作业少、大作业、实时性要求低的场景。</li><li>Application模式：实时性要求不太高、安全性有一定要求均可以使用，普遍适用性最强。</li></ul></li><li>生产环境使用说明<ul><li>建议用per-job或是application模式，提供了更好的资源隔离性和安全性。</li></ul></li></ul><h1 id="三：运行流程"><a href="#三：运行流程" class="headerlink" title="三：运行流程"></a>三：运行流程</h1><h2 id="3-1-工作流程图"><a href="#3-1-工作流程图" class="headerlink" title="3.1 工作流程图"></a>3.1 工作流程图</h2><p><img src="/blog/50ac08de.html/1630324699314-e7922476-6b00-4315-894e-e9d794163070.png"></p><h2 id="3-2-运行时核心角色组成"><a href="#3-2-运行时核心角色组成" class="headerlink" title="3.2 运行时核心角色组成"></a>3.2 运行时核心角色组成</h2><ul><li>由两种类型的进程组成，一个 JobManager 和一个或者多个 TaskManager。</li><li>Client 客户端不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。</li><li>提交任务完成之后，Client可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。</li><li>Client可以作为触发执行 Java&#x2F;Scala 程序的一部分运行，也可以在命令行进程.&#x2F;bin&#x2F;flink run …中运行。</li><li>可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN或Mesos等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。</li><li>Actor System<ul><li>各个角色组件互相通信的消息传递系统中间件。</li><li>Actor是一种并发编程模型，与另一种模型共享内存完全相反，Actor模型share nothing，即没有任何共享。</li><li>所有的线程(或进程)通过消息传递的方式进行合作(通信)，这些线程(或进程)称为Actor。</li><li>以其简单、高效著称</li><li>缺点<ul><li>唯一的缺点是不能实现真正意义上的并行， 而是通过并发实现的并行效果，会存在一定的不确定性。</li><li>纯消息通信，实时性和粒度控制上会略弱于共享内存的方式。</li></ul></li></ul></li></ul><h2 id="3-3-核心组成角色剖析"><a href="#3-3-核心组成角色剖析" class="headerlink" title="3.3 核心组成角色剖析"></a>3.3 核心组成角色剖析</h2><ul><li>JobManager<ul><li>JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：<ul><li>它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：<ul><li>ResourceManager<ul><li>ResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的最小单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。</li></ul></li><li>Dispatcher<ul><li>Dispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。</li></ul></li><li>JobMaster<ul><li>JobMaster 负责管理单个JobGraph的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。</li><li>始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。</li></ul></li></ul></li></ul></li></ul></li><li>TaskManager<ul><li>TaskManager（也称为 worker）执行作业流的 task，并且缓存和交换数据流。</li><li>必须始终至少有一个 TaskManager。在 TaskManager 中资源调度的最小单位是 task slot。</li><li>TaskManager 中 task slot 的数量表示并发处理 task 的数量。请注意一个 task slot 中可以执行多个算子。</li></ul></li></ul><h2 id="3-4-yarn模式提交任务的工作流程"><a href="#3-4-yarn模式提交任务的工作流程" class="headerlink" title="3.4 yarn模式提交任务的工作流程"></a>3.4 yarn模式提交任务的工作流程</h2><ul><li>工作流程图：</li></ul><p><img src="/blog/50ac08de.html/1630325042367-d709bf3d-24b0-4907-9f10-64693abdf588.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Flink-架构设计图&quot;&gt;&lt;a href=&quot;#一：Flink-架构设计图&quot; class=&quot;headerlink&quot; title=&quot;一：Flink 架构设计图&quot;&gt;&lt;/a&gt;一：Flink 架构设计图&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/50ac08de.h</summary>
      
    
    
    
    <category term="Flink" scheme="https://aiyingke.cn/categories/Flink/"/>
    
    
    <category term="Flink" scheme="https://aiyingke.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 概述</title>
    <link href="https://aiyingke.cn/blog/72cd4c87.html/"/>
    <id>https://aiyingke.cn/blog/72cd4c87.html/</id>
    <published>2023-02-19T16:45:05.000Z</published>
    <updated>2023-02-19T17:27:46.943Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：产生背景"><a href="#一：产生背景" class="headerlink" title="一：产生背景"></a>一：产生背景</h1><h2 id="1-1-历史背景"><a href="#1-1-历史背景" class="headerlink" title="1.1 历史背景"></a>1.1 历史背景</h2><ul><li>随着互联网应用的快速发展，<code>实时流数据产生日益增多和普遍化</code>。如日常生活、金融、驾驶、LBS、电商等众多领域。</li><li>实时数据的处理和挖掘能够带来离线数据处理和挖掘<code>更多的社会发展和商业价值</code>。</li><li>如何快速响应和处理这些大规模的实时数据流，成为众多互联网大厂的当务之急。</li><li>在flink之前也出现了很多<code>流数据处理引擎</code>，包括storm、sparkstreaming等知名流行框架，但各自均有较明显的不足，导致没有达到理想的流处理引擎的标准要求。</li></ul><h2 id="1-2-优秀的流处理引擎"><a href="#1-2-优秀的流处理引擎" class="headerlink" title="1.2 优秀的流处理引擎"></a>1.2 优秀的流处理引擎</h2><ul><li>优秀流处理引擎标准要求<ul><li>低延迟、高吞吐量、容错性、窗口时间语义化、编程效率高与运行效果好的用户体验；</li></ul></li><li>storm<ul><li>优点：低延迟</li><li>缺点：其它要求都较差一些</li></ul></li><li>sparkstreaming<ul><li>优点：高吞吐量、容错性高</li><li>缺点：其它要求都较差一些</li></ul></li></ul><h1 id="二：基本介绍"><a href="#二：基本介绍" class="headerlink" title="二：基本介绍"></a>二：基本介绍</h1><h2 id="2-1-概念说明"><a href="#2-1-概念说明" class="headerlink" title="2.1 概念说明"></a>2.1 概念说明</h2><ul><li>由Apache软件基金会开发的开源流处理框架</li><li>其核心是用Java和Scala编写的框架和分布式处理引擎</li><li>用于对无界和有界数据流进行有状态计算。<ul><li>无界数据流: 即为实时流数据；</li><li>有界数据流：即为离线数据，也称为批处理数据；</li></ul></li></ul><h2 id="2-2-特点特征"><a href="#2-2-特点特征" class="headerlink" title="2.2 特点特征"></a>2.2 特点特征</h2><ul><li>被设计为在所有常见的集群环境中运行，以内存速度和任何规模执行计算。</li><li>能够达到实时流处理引擎的全部标准要求。<ul><li>低延迟、高吞吐量、容错性、窗口时间语义化、编程效率高与运行效果好的用户体验；</li></ul></li></ul><h1 id="三：应用场景"><a href="#三：应用场景" class="headerlink" title="三：应用场景"></a>三：应用场景</h1><h2 id="3-1-官方说明"><a href="#3-1-官方说明" class="headerlink" title="3.1 官方说明"></a>3.1 官方说明</h2><ul><li>事件驱动型应用</li><li>数据分析型应用</li><li>数据管道 ETL</li></ul><h2 id="3-2-实际情况"><a href="#3-2-实际情况" class="headerlink" title="3.2 实际情况"></a>3.2 实际情况</h2><ul><li>要求严格的实时流处理场景</li></ul><h1 id="四：代码实现"><a href="#四：代码实现" class="headerlink" title="四：代码实现"></a>四：代码实现</h1><h2 id="4-1-实现方式"><a href="#4-1-实现方式" class="headerlink" title="4.1 实现方式"></a>4.1 实现方式</h2><ul><li>Java API</li><li>Scala API</li></ul><h2 id="4-2-统一数据处理过程抽象"><a href="#4-2-统一数据处理过程抽象" class="headerlink" title="4.2 统一数据处理过程抽象"></a>4.2 统一数据处理过程抽象</h2><ul><li>将实时和批处理的数据过程，均抽象成三个过程，即Source-&gt;Transform-&gt;Sink。<ul><li>Source为源数据读入，即Source算子。</li><li>Transform是数据转换处理过程，即Transform算子。</li><li>Sink即数据接收器，即数据落地到存储层，即Sink算子。</li></ul></li><li>代码实现复杂度<ul><li>丰富的API和算子操作；</li><li>抽象封装统一性较高，支持类SQL编程，编程复杂度并不高。</li></ul></li></ul><h1 id="五：版本发展"><a href="#五：版本发展" class="headerlink" title="五：版本发展"></a>五：版本发展</h1><table><thead><tr><th><strong>版本</strong></th><th><strong>发行日期</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>Flink 0.6-incubating</strong></td><td><strong>2014-08-26</strong></td><td><strong>初步得到团队内部认可，正在快速迭代中</strong></td></tr><tr><td><strong>Flink 0.9.0-milestone</strong></td><td><strong>2015-04-13</strong></td><td><strong>有重大进展，得正式对外发布，</strong></td></tr><tr><td><strong>0.9</strong></td><td><strong>2015-09-01</strong></td><td><strong>从此时开始引入阿里巴巴，并成为阿里系主干业务实时流处理引擎，内部改良优化后命名为blink</strong></td></tr><tr><td>0.10</td><td>2016-02-11</td><td></td></tr><tr><td><strong>1.0</strong></td><td><strong>2016-05-11</strong></td><td><strong>很有里程碑、代表性的一个版本</strong></td></tr><tr><td>1.1</td><td>2017-03-22</td><td></td></tr><tr><td>1.2</td><td>2017-04-26</td><td></td></tr><tr><td>1.3</td><td>2018-03-15</td><td></td></tr><tr><td>1.4</td><td>2018-03-08</td><td></td></tr><tr><td>1.5</td><td>2018-10-29</td><td></td></tr><tr><td>1.6</td><td>2018-10-29</td><td></td></tr><tr><td><strong>1、在2019年初，blink在阿里内部经过多年的商用实践，增加了N多新特性，并得到广泛应用和成熟化，正式对外开源，并捐赠给Apache Flink社区，并成为其下的分支方式开源并逐步融合后，依然以Flink为主进一步推进开源进程。****2、阿里系以9000万欧元收购了创业公司 Data Artisans，即Flink的开发团队所属公司。</strong></td><td></td><td></td></tr><tr><td>1.7</td><td>2019-02-15</td><td></td></tr><tr><td>1.8</td><td>2019-04-09</td><td></td></tr><tr><td><strong>1.9</strong></td><td><strong>2019-08-19</strong></td><td><strong>目前市占率较高的一个版本</strong></td></tr><tr><td>1.10</td><td>2020-02-11</td><td></td></tr><tr><td>1.11</td><td>2020-07-06</td><td><strong>从此版本开始，加入很多新特性，支持hadoop3.x版本</strong></td></tr><tr><td>1.12</td><td>2020-12-08</td><td></td></tr><tr><td>Flink 1.13.0</td><td>2021-04-30</td><td></td></tr><tr><td><strong>Flink 1.13.1</strong></td><td><strong>2021-05-28</strong></td><td><strong>版本迭代很快，社区很活跃，发展非常快****已是稳定版。</strong></td></tr></tbody></table><ul><li>Flink版本在早期就得到阿里认可，并进行集团内部孵化和二次开发、商用实践，命名为Blink。</li><li>Blink的主要贡献是在用户体验上，包括SQL、webUI等方面。</li><li>在2019年进行了开源反馈给社区，从此更多的是以Flink merge Blink新功能后，以Flink为主继续推进开源。</li><li>基于市场量、成熟度、社区丰富度等方面，通常选择1.13.1版本。</li></ul><h1 id="六：市场前景"><a href="#六：市场前景" class="headerlink" title="六：市场前景"></a>六：市场前景</h1><ul><li>现实情况<ul><li>学习成本较高、应用场景较垂直，其实际开发者在市场上是比较衡缺的。</li><li>相对于更广大的中小型公司，Flink的使用量最主要是集中在中大型互联网科技公司。</li></ul></li><li>发展趋势<ul><li>商业市场、各种大型IT企业对大规模实时数据场景需求旺盛。</li><li>Flink在实时数据处理方面的架构设计与商用实践表现较为突出。</li><li>得到阿里系的商业收购+大规模人力财力物力的支持，未来发展不可限量。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：产生背景&quot;&gt;&lt;a href=&quot;#一：产生背景&quot; class=&quot;headerlink&quot; title=&quot;一：产生背景&quot;&gt;&lt;/a&gt;一：产生背景&lt;/h1&gt;&lt;h2 id=&quot;1-1-历史背景&quot;&gt;&lt;a href=&quot;#1-1-历史背景&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="Flink" scheme="https://aiyingke.cn/categories/Flink/"/>
    
    
    <category term="Flink" scheme="https://aiyingke.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Spark 交互操作</title>
    <link href="https://aiyingke.cn/blog/75610dc7.html/"/>
    <id>https://aiyingke.cn/blog/75610dc7.html/</id>
    <published>2023-02-19T07:25:39.000Z</published>
    <updated>2023-02-19T08:01:27.442Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Spark-运行模式"><a href="#一：Spark-运行模式" class="headerlink" title="一：Spark 运行模式"></a>一：Spark 运行模式</h1><p>即作业以什么样的模式去执行，主要是单机、分布式两种方式的细节选择。</p><table><thead><tr><th><strong>序号</strong></th><th align="center"><strong>模式名称</strong></th><th align="center"><strong>特点</strong></th><th align="center"><strong>应用场景</strong></th></tr></thead><tbody><tr><td>1</td><td align="center">本地运行模式(local)</td><td align="center">单台机器多线程来模拟spark分布式计算</td><td align="center">机器资源不够测试验证程序逻辑的正确性</td></tr><tr><td>2</td><td align="center">伪分布式模式</td><td align="center">单台机器多进程来模拟spark分布式计算</td><td align="center">机器资源不够测试验证程序逻辑的正确性</td></tr><tr><td>3</td><td align="center">standalone(client)</td><td align="center">独立布署spark计算集群自带clustermanagerdriver运行在spark submit client端</td><td align="center">机器资源充分纯用spark计算框架任务提交后在spark submit   client端实时查看反馈信息数据共享性弱测试使用还可以，生产环境极少使用该种模式</td></tr><tr><td>4</td><td align="center">standalone(cluster)</td><td align="center">独立布署spark计算集群自带clustermanagerdriver运行在spark worker node端</td><td align="center">机器资源充分纯用spark计算框架任务提交后将退出spark   submit client端数据共享性弱测试和生产环境均可以自由使用，但更多用于生产环境</td></tr><tr><td>5</td><td align="center">spark on yarn(yarn-client)</td><td align="center">以yarn集群为基础只添加spark计算框架相关包driver运行在yarn client上</td><td align="center">机器资源充分多种计算框架混用数据共享性强任务提交后在yarn client端实时查看反馈信息</td></tr><tr><td>6</td><td align="center">spark on yarn(yarn-cluster)</td><td align="center">以yarn集群为基础只添加spark计算框架相关包driver运行在集群的am contianer中</td><td align="center">机器资源充分多种计算框架混用数据共享性强任务提交后将退出yarn client端</td></tr><tr><td>7</td><td align="center">spark on mesos&#x2F;ec2</td><td align="center">与spark on yarn类似</td><td align="center">与spark on yarn类似在国内应用较少</td></tr></tbody></table><h1 id="二：Spark-用户交互方式"><a href="#二：Spark-用户交互方式" class="headerlink" title="二：Spark 用户交互方式"></a>二：Spark 用户交互方式</h1><ol><li>spark-shell：spark命令行方式来操作spark作业。<ul><li>多用于简单的学习、测试、简易作业操作。</li></ul></li><li>spark-submit：通过程序脚本，提交相关的代码、依赖等来操作spark作业。<ul><li>最多见的提交任务的交互方式，简单易用、参数齐全。</li></ul></li><li>spark-sql：通过sql的方式操作spark作业。<ul><li>sql相关的学习、测试、生产环境研发均可以使用该直接操作交互方式。</li></ul></li><li>spark-class：最低层的调用方式，其它调用方式多是最终转化到该方式中去提交。<ul><li>直接使用较少</li></ul></li><li>sparkR、sparkPython：通过其它非java、非scala语言直接操作spark作业的方式。<ul><li>R、python语言使用者的交互方式。</li></ul></li></ol><h1 id="三：Spark-Shell-操作"><a href="#三：Spark-Shell-操作" class="headerlink" title="三：Spark-Shell 操作"></a>三：Spark-Shell 操作</h1><h2 id="3-1-交互方式定位"><a href="#3-1-交互方式定位" class="headerlink" title="3.1 交互方式定位"></a>3.1 交互方式定位</h2><ul><li>一个强大的交互式数据操作与分析的工具，提供一个简单的方式快速学习spark相关的API。</li></ul><h2 id="3-2-启动方式"><a href="#3-2-启动方式" class="headerlink" title="3.2 启动方式"></a>3.2 启动方式</h2><ul><li>前置环境：已将spark-shell等交互式脚本已加入系统PATH变量，可在任意位置使用。</li><li>以本地2个线程来模拟运行spark相关操作，该数量一般与本机的cpu核数相一致为最佳spark-shell –master local[2]</li></ul><h2 id="3-3-相关参数"><a href="#3-3-相关参数" class="headerlink" title="3.3 相关参数"></a>3.3 相关参数</h2><ul><li>参数列表获取方式：spark-shell –help</li></ul><h1 id="四：Spark-submit-操作"><a href="#四：Spark-submit-操作" class="headerlink" title="四：Spark-submit 操作"></a>四：Spark-submit 操作</h1><h2 id="4-1-交互方式定位"><a href="#4-1-交互方式定位" class="headerlink" title="4.1 交互方式定位"></a>4.1 交互方式定位</h2><ul><li>最常用的通过程序脚本，提交相关的代码、依赖等来操作spark作业的方式。</li></ul><h2 id="4-2-启动方式"><a href="#4-2-启动方式" class="headerlink" title="4.2 启动方式"></a>4.2 启动方式</h2><ul><li>spark-submit提交任务的模板</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --jars jar_list_by_comma \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure><h2 id="4-3-spark-submit-详细参数说明"><a href="#4-3-spark-submit-详细参数说明" class="headerlink" title="4.3 spark-submit 详细参数说明"></a>4.3 spark-submit 详细参数说明</h2><table><thead><tr><th align="center">参数名</th><th>参数说明</th></tr></thead><tbody><tr><td align="center">–master</td><td>master 的地址，提交任务到哪里执行，例如 spark:&#x2F;&#x2F;host:port,   yarn, local</td></tr><tr><td align="center">–deploy-mode</td><td>在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</td></tr><tr><td align="center">–class</td><td>应用程序的主类，仅针对 java 或 scala 应用</td></tr><tr><td align="center">–name</td><td>应用程序的名称</td></tr><tr><td align="center">–jars</td><td>用逗号分隔的本地jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</td></tr><tr><td align="center">–packages</td><td>包含在driver 和executor 的  classpath 中的 jar 的 maven 坐标</td></tr><tr><td align="center">–exclude-packages</td><td>为了避免冲突   而指定不包含的 package</td></tr><tr><td align="center">–repositories</td><td>远程 repository</td></tr><tr><td align="center">–conf PROP&#x3D;VALUE</td><td>指定 spark 配置属性的值， 例如  -conf spark.executor.extraJavaOptions&#x3D;”-XX:MaxPermSize&#x3D;256m”</td></tr><tr><td align="center">–properties-file</td><td>加载的配置文件，默认为 conf&#x2F;spark-defaults.conf</td></tr><tr><td align="center">–driver-memory</td><td>Driver内存，默认 1G</td></tr><tr><td align="center">–driver-java-options</td><td>传给 driver 的额外的 Java 选项</td></tr><tr><td align="center">–driver-library-path</td><td>传给 driver 的额外的库路径</td></tr><tr><td align="center">–driver-class-path</td><td>传给 driver 的额外的类路径</td></tr><tr><td align="center">–driver-cores</td><td>Driver 的核数，默认是1。在 yarn 或者 standalone 下使用</td></tr><tr><td align="center">–executor-memory</td><td>每个 executor 的内存，默认是1G</td></tr><tr><td align="center">–total-executor-cores</td><td>所有 executor 总共的核数。仅仅在  mesos 或者 standalone 下使用</td></tr><tr><td align="center">–num-executors</td><td>启动的 executor 数量。默认为2。在 yarn 下使用</td></tr><tr><td align="center">–executor-core</td><td>每个 executor 的核数。在yarn或者standalone下使用</td></tr></tbody></table><h2 id="4-4-关于–master取值的特别说明"><a href="#4-4-关于–master取值的特别说明" class="headerlink" title="4.4 关于–master取值的特别说明"></a>4.4 关于–master取值的特别说明</h2><table><thead><tr><th>local</th><th>本地worker线程中运行spark，完全没有并行</th></tr></thead><tbody><tr><td>local[K]</td><td>在本地work线程中启动K个线程运行spark</td></tr><tr><td>local[*]</td><td>启动与本地work机器的core个数相同的线程数来运行spark</td></tr><tr><td>spark:&#x2F;&#x2F;HOST:PORT</td><td>连接指定的standalone集群的master，默认7077端口</td></tr><tr><td>mesos:&#x2F;&#x2F;HOST:PORT</td><td>连接到mesos集群，国内用的极少</td></tr><tr><td>yarn</td><td>使用yarn的cluster或者yarn的client模式连接。取决于–deploy-mode参数，由deploy-mode的取值为client或是cluster来最终决定。也可以用yarn-client或是yarn-cluster进行二合一参数使用，保留–master去掉—deploy-mode参数亦可。–master   yarn-client，相当于—master   yarn –deploy-mode client的二合一</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Spark-运行模式&quot;&gt;&lt;a href=&quot;#一：Spark-运行模式&quot; class=&quot;headerlink&quot; title=&quot;一：Spark 运行模式&quot;&gt;&lt;/a&gt;一：Spark 运行模式&lt;/h1&gt;&lt;p&gt;即作业以什么样的模式去执行，主要是单机、分布式两种方式的细节</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 相关术语</title>
    <link href="https://aiyingke.cn/blog/f278f015.html/"/>
    <id>https://aiyingke.cn/blog/f278f015.html/</id>
    <published>2023-02-19T07:00:48.000Z</published>
    <updated>2023-02-19T07:19:37.117Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：RDD-Resilient-Distributed-DataSet"><a href="#一：RDD-Resilient-Distributed-DataSet" class="headerlink" title="一：RDD (Resilient Distributed DataSet)"></a>一：RDD (Resilient Distributed DataSet)</h1><ul><li>弹性分布式数据集，是对数据集在spark存储和计算过程中的一种抽象。</li><li>是一组只读、可分区的分布式数据集合。</li><li>一个RDD 包含多个分区Partition(类似于MapReduce中的InputSplit中的block)，分区是依照一定的规则的，将具有相同规则的属性的数据记录放在一起。</li><li>横向上可切分并行计算，以分区Partition为切分后的最小存储和计算单元。</li><li>纵向上可进行内外存切换使用，即当数据在内存不足时，可以用外存磁盘来补充。</li></ul><h1 id="二：Partition-（分区）"><a href="#二：Partition-（分区）" class="headerlink" title="二：Partition （分区）"></a>二：Partition （分区）</h1><ul><li>Partition类似hadoop的Split中的block，计算是以partition为单位进行的，提供了一种划分数据的方式。</li><li>Partition的划分依据有很多，常见的有Hash分区、范围分区等，也可以自己定义的，像HDFS文件，划分的方式就和MapReduce一样，以文件的block来划分不同的partition。</li><li>一个Partition交给一个Task去计算处理。</li></ul><h1 id="三：算子"><a href="#三：算子" class="headerlink" title="三：算子"></a>三：算子</h1><ul><li>英文简称：Operator，简称op</li><li>广义上讲，对任何函数进行某一项操作都可以认为是一个算子</li><li>通俗上讲，算子即为映射、关系、变换。</li><li>MapReduce算子，主要分为两个，即为Map和Reduce两个主要操作的算子，导致灵活可用性比较差。</li><li>Spark算子，分为两大类，即为Transformation和Action类，合计有80多个。</li></ul><h1 id="四：Transformation类算子"><a href="#四：Transformation类算子" class="headerlink" title="四：Transformation类算子"></a>四：Transformation类算子</h1><ul><li>操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。</li><li>细分类<ul><li>Value数据类型的Transformation算子</li><li>Key-Value数据类型的Transfromation算子</li></ul></li></ul><h1 id="五：Action类算子"><a href="#五：Action类算子" class="headerlink" title="五：Action类算子"></a>五：Action类算子</h1><ul><li>会触发 Spark 提交作业（Job），并将数据输出 Spark 系统。</li></ul><h1 id="六：窄依赖"><a href="#六：窄依赖" class="headerlink" title="六：窄依赖"></a>六：窄依赖</h1><ul><li>如果一个父RDD的每个分区只被子RDD的一个分区使用 —-&gt; 一对一关系</li></ul><h1 id="七：宽依赖"><a href="#七：宽依赖" class="headerlink" title="七：宽依赖"></a>七：宽依赖</h1><ul><li>如果一个父RDD的每个分区要被子RDD 的多个分区使用 —-&gt; 一对多关系</li></ul><h1 id="八：Application"><a href="#八：Application" class="headerlink" title="八：Application"></a>八：Application</h1><ul><li>Spark Application的概念和MapReduce中的job或者yarn中的application类似，指的是用户编写的Spark应用程序，包含了一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码</li><li>一般是指整个Spark项目从开发、测试、布署、运行的全部。</li></ul><h1 id="九：Driver"><a href="#九：Driver" class="headerlink" title="九：Driver"></a>九：Driver</h1><ul><li>运行main函数并且创建SparkContext的程序。</li><li>称为驱动程序，Driver Program类似于hadoop的wordcount程序中的driver类的main函数。</li></ul><h1 id="十：Cluster-Manager"><a href="#十：Cluster-Manager" class="headerlink" title="十：Cluster Manager"></a>十：Cluster Manager</h1><ul><li>集群的资源管理器，在集群上获取资源的服务。如Yarn、Mesos、Spark Standalone等。</li><li>以Yarn为例，驱动程序会向Yarn申请计算我这个任务需要多少的内存，多少CPU等，后由Cluster Manager会通过调度告诉驱动程序可以使用，然后驱动程序将任务分配到既定的Worker Node上面执行。</li></ul><h1 id="十一：WorkerNode"><a href="#十一：WorkerNode" class="headerlink" title="十一：WorkerNode"></a>十一：WorkerNode</h1><ul><li>集群中任何一个可以运行spark应用代码的节点。</li><li>Worker Node就是物理机器节点，可以在上面启动Executor进程。</li></ul><h1 id="十二：Executor"><a href="#十二：Executor" class="headerlink" title="十二：Executor"></a>十二：Executor</h1><ul><li>Application运行在Worker节点上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上，每个Application都有各自独立专享的一批Executor。</li><li>Executor即为spark概念的资源容器，类比于yarn的container容器，真正承载Task的运行与管理，以多线程的方式运行Task，更加高效快速。</li></ul><h1 id="十三：Task"><a href="#十三：Task" class="headerlink" title="十三：Task"></a>十三：Task</h1><ul><li>与Hadoop中的Map Task或者Reduce Task是类同的。</li><li>分配到executor上的基本工作单元，执行实际的计算任务。</li><li>Task分为两类，即为ShuffleMapTask和ResultTask。<ul><li>ShuffleMapTask：即为Map任务和发生Shuffle的任务的操作，由Transformation操作组成，其输出结果是为下个阶段任务(ResultTask)进行做准备，不是最终要输出的结果。</li><li>ResultTask：即为Action操作触发的Job作业的最后一个阶段任务，其输出结果即为Application最终的输出或存储结果。</li></ul></li></ul><h1 id="十四：Job（作业）"><a href="#十四：Job（作业）" class="headerlink" title="十四：Job（作业）"></a>十四：Job（作业）</h1><ul><li>Spark RDD里的每个action的计算会生成一个job。</li><li>用户提交的Job会提交给DAGScheduler（Job调度器），Job会被分解成Stage去执行，每个Stage由一组相同计算规则的Task组成，该组Task也称为TaskSet，实际交由TaskScheduler去调度Task的机器执行节点，最终完成作业的执行。</li></ul><h1 id="十五：Stage（阶段）"><a href="#十五：Stage（阶段）" class="headerlink" title="十五：Stage（阶段）"></a>十五：Stage（阶段）</h1><ul><li>Stage是Job的组成部分，每个Job可以包含1个或者多个Stage。</li><li>Job切分成Stage是以Shuffle作为分隔依据，Shuffle前是一个Stage，Shuffle后是一个Stage。即为按RDD宽窄依赖来划分Stage。  </li><li>每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业可以被分为一个或多个阶段。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：RDD-Resilient-Distributed-DataSet&quot;&gt;&lt;a href=&quot;#一：RDD-Resilient-Distributed-DataSet&quot; class=&quot;headerlink&quot; title=&quot;一：RDD (Resilient Distr</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 架构设计</title>
    <link href="https://aiyingke.cn/blog/6a386de9.html/"/>
    <id>https://aiyingke.cn/blog/6a386de9.html/</id>
    <published>2023-02-19T06:36:20.000Z</published>
    <updated>2023-02-19T07:06:56.914Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：架构总览"><a href="#一：架构总览" class="headerlink" title="一：架构总览"></a>一：架构总览</h1><p><img src="/blog/6a386de9.html/1629182444027-d40b52f7-91de-4983-9fe1-c9c7e82c2a3d.png"></p><h1 id="二：角色作用"><a href="#二：角色作用" class="headerlink" title="二：角色作用"></a>二：角色作用</h1><ul><li>Client：面向用户，对外提供接口，提交代码的入口。</li><li>Driver Program：驱动器程序，用于解耦客户端和内部实际操作，将用户程序转化为任务。</li><li>SparkContent：Spark 上下文，承接作用，用于配置上下文环境。</li><li>Cluster Manager（Resource Manager）：集群资源管理器，统一资源管理与任务调度。</li><li>Application Master：任务的执行，调度指挥者。</li><li>Worker Node：工作节点，任务的实际执行者。</li></ul><h1 id="三：角色间关系"><a href="#三：角色间关系" class="headerlink" title="三：角色间关系"></a>三：角色间关系</h1><ol><li>客户端接收到用户指令、代码；</li><li>驱动器服务于客户端，承接指令传达给集群资源管理器；</li><li>集群资源管理器根据当前情况，进行资源调度，生成一个任务调度者 AM（Application Master）</li><li>AM 给相应的工作节点分配任务；</li><li>工作节点执行任务，执行完毕，结果返回给 AM，并向资源管理器汇报自身资源情况，任务已完成当前空闲状态。</li><li>AM 接收到计算结果进行汇总，返回给客户端。</li></ol><h1 id="四：工作特性"><a href="#四：工作特性" class="headerlink" title="四：工作特性"></a>四：工作特性</h1><ul><li>内存计算</li><li>多线程</li><li>缓存</li><li>每一个 AM 都有一批专享的 Executor，以多线程方式启动多个 Task 任务，并行的线程计算任务缓存 RDD数据缓存块，存储复用的数据模块。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：架构总览&quot;&gt;&lt;a href=&quot;#一：架构总览&quot; class=&quot;headerlink&quot; title=&quot;一：架构总览&quot;&gt;&lt;/a&gt;一：架构总览&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/6a386de9.html/1629182444027-d40b52f7-9</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 数据压缩</title>
    <link href="https://aiyingke.cn/blog/40919b9a.html/"/>
    <id>https://aiyingke.cn/blog/40919b9a.html/</id>
    <published>2023-02-10T07:37:26.000Z</published>
    <updated>2023-02-10T08:42:28.062Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：概述"><a href="#一：概述" class="headerlink" title="一：概述"></a>一：概述</h1><h2 id="1-1-优缺点"><a href="#1-1-优缺点" class="headerlink" title="1.1 优缺点"></a>1.1 优缺点</h2><ul><li>压缩的优点：以减少磁盘 IO、减少磁盘存储空间。</li><li>压缩的缺点：增加 CPU 开销。</li></ul><h2 id="1-2-压缩原则"><a href="#1-2-压缩原则" class="headerlink" title="1.2 压缩原则"></a>1.2 压缩原则</h2><ul><li>运算密集型的 Job，少用压缩</li><li>IO 密集型的 Job，多用压缩</li></ul><h1 id="二：MR-支持的压缩编码"><a href="#二：MR-支持的压缩编码" class="headerlink" title="二：MR 支持的压缩编码"></a>二：MR 支持的压缩编码</h1><h2 id="2-1-压缩算法对比介绍"><a href="#2-1-压缩算法对比介绍" class="headerlink" title="2.1 压缩算法对比介绍"></a>2.1 压缩算法对比介绍</h2><table><thead><tr><th>压缩格式</th><th>Hadoop 是否自带</th><th>算法</th><th>文件扩展名</th><th>是否可切片</th><th>换成压缩格式后，原来的程序是否需要修改</th></tr></thead><tbody><tr><td>DEFLATE</td><td>是，直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>Gzip</td><td>是，直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>bzip2</td><td>是，直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样，不需要修改</td></tr><tr><td>LZO</td><td>否，需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引，还需要指定 输入格式</td></tr><tr><td>Snappy</td><td>是，直接使用</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样，不需要修改</td></tr></tbody></table><h2 id="2-2-压缩性能的比较"><a href="#2-2-压缩性能的比较" class="headerlink" title="2.2 压缩性能的比较"></a>2.2 压缩性能的比较</h2><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB&#x2F;s</td><td>58MB&#x2F;s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB&#x2F;s</td><td>9.5MB&#x2F;s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB&#x2F;s</td><td>74.6MB&#x2F;s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p><p>Snappy compresses at about 250 MB&#x2F;sec or more and decompresses at about 500 MB&#x2F;sec or more.</p><h1 id="三：压缩方式选择"><a href="#三：压缩方式选择" class="headerlink" title="三：压缩方式选择"></a>三：压缩方式选择</h1><p>压缩方式选择时重点考虑：</p><ul><li>压缩&#x2F;解压缩速度</li><li>压缩率（压缩后存储大小）</li><li>压缩后是否 可以支持切片。</li></ul><h2 id="3-1-Gzip-压缩"><a href="#3-1-Gzip-压缩" class="headerlink" title="3.1 Gzip 压缩"></a>3.1 Gzip 压缩</h2><ul><li>优点：压缩率比较高；</li><li>缺点：不支持 Split；压缩&#x2F;解压速度一般；</li></ul><h2 id="3-2-Bzip2-压缩"><a href="#3-2-Bzip2-压缩" class="headerlink" title="3.2 Bzip2 压缩"></a>3.2 Bzip2 压缩</h2><ul><li>优点：压缩率高；支持 Split；</li><li>缺点：压缩&#x2F;解压速度慢。</li></ul><h2 id="3-3-Lzo-压缩"><a href="#3-3-Lzo-压缩" class="headerlink" title="3.3 Lzo 压缩"></a>3.3 Lzo 压缩</h2><ul><li>优点：压缩&#x2F;解压速度比较快；支持 Split；</li><li>缺点：压缩率一般；想支持切片需要额外创建索引。</li></ul><h2 id="3-4-Snappy-压缩"><a href="#3-4-Snappy-压缩" class="headerlink" title="3.4 Snappy 压缩"></a>3.4 Snappy 压缩</h2><ul><li>优点：压缩和解压缩速度快；</li><li>缺点：不支持 Split；压缩率一般；</li></ul><h1 id="四：压缩位置选择"><a href="#四：压缩位置选择" class="headerlink" title="四：压缩位置选择"></a>四：压缩位置选择</h1><p>压缩可以在 MapReduce 作用的任意阶段启用。</p><p><img src="/blog/40919b9a.html/image-20230210155254105.png"></p><h1 id="五：压缩参数配置"><a href="#五：压缩参数配置" class="headerlink" title="五：压缩参数配置"></a>五：压缩参数配置</h1><h2 id="5-1-为了支持多种压缩-x2F-解压缩算法，Hadoop-引入了编码-x2F-解码器"><a href="#5-1-为了支持多种压缩-x2F-解压缩算法，Hadoop-引入了编码-x2F-解码器" class="headerlink" title="5.1 为了支持多种压缩&#x2F;解压缩算法，Hadoop 引入了编码&#x2F;解码器"></a>5.1 为了支持多种压缩&#x2F;解压缩算法，Hadoop 引入了编码&#x2F;解码器</h2><table><thead><tr><th>压缩格式</th><th>对应的编码&#x2F;解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><h2 id="5-2-要在-Hadoop-中启用压缩，可以配置如下参数"><a href="#5-2-要在-Hadoop-中启用压缩，可以配置如下参数" class="headerlink" title="5.2 要在 Hadoop 中启用压缩，可以配置如下参数"></a>5.2 要在 Hadoop 中启用压缩，可以配置如下参数</h2><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在 core-site.xml 中配置）</td><td>无，这个需要在命令行输入 hadoop checknative 查看</td><td>输入压缩</td><td>Hadoop 使用文件扩展 名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compr ess（在 mapred-site.xml 中 配置）</td><td>false</td><td>mapper 输出</td><td>这个参数设为 true 启 用压缩</td></tr><tr><td>mapreduce.map.output.compr ess.codec（在 mapredsite.xml 中配置）</td><td>org.apache.hadoop.io.com press.DefaultCodec</td><td>mapper 输出</td><td>企业多使用 LZO 或 Snappy 编解码器在此 阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutpu tformat.compress（在 mapred-site.xml 中配置）</td><td>false</td><td>reducer 输出</td><td>这个参数设为 true 启 用压缩</td></tr><tr><td>mapreduce.output.fileoutpu tformat.compress.codec（在 mapred-site.xml 中配置）</td><td>org.apache.hadoop.io.com press.DefaultCodec</td><td>reducer 输出</td><td>使用标准工具或者编 解码器，如 gzip 和 bzip2</td></tr></tbody></table><h1 id="六：压缩实操案例"><a href="#六：压缩实操案例" class="headerlink" title="六：压缩实操案例"></a>六：压缩实操案例</h1><h2 id="6-1-Map-输出端采用压缩"><a href="#6-1-Map-输出端采用压缩" class="headerlink" title="6.1 Map 输出端采用压缩"></a>6.1 Map 输出端采用压缩</h2><p>即使你的 MapReduce 的输入输出文件都是未压缩的文件，你仍然可以对 Map 任务的中 间结果输出做压缩，因为它要写在硬盘并且通过网络传输到 Reduce 节点，对其压缩可以提高很多性能。</p><p>Hadoop 源码支持的压缩格式有：BZip2Codec、DefaultCodec</p><h3 id="（1）驱动器-Driver"><a href="#（1）驱动器-Driver" class="headerlink" title="（1）驱动器 Driver"></a>（1）驱动器 Driver</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.mapCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 16:28 2023/2/10</span></span><br><span class="line"><span class="comment">         * Description: 开启map端输出压缩</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">        configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output888&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-保持不变"><a href="#（2）Mapper-保持不变" class="headerlink" title="（2）Mapper 保持不变"></a>（2）Mapper 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.mapCompress;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-保持不变"><a href="#（3）Reducer-保持不变" class="headerlink" title="（3）Reducer 保持不变"></a>（3）Reducer 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.mapCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:41 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-2-Reduce-输出端采用压缩"><a href="#6-2-Reduce-输出端采用压缩" class="headerlink" title="6.2 Reduce 输出端采用压缩"></a>6.2 Reduce 输出端采用压缩</h2><h3 id="（1）驱动器-Driver-1"><a href="#（1）驱动器-Driver-1" class="headerlink" title="（1）驱动器 Driver"></a>（1）驱动器 Driver</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.reduceCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.DefaultCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 16:28 2023/2/10</span></span><br><span class="line"><span class="comment">         * Description: 开启map端输出压缩</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">        configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 16:33 2023/2/10</span></span><br><span class="line"><span class="comment">         * Description: 设置reduce端输出压缩开启</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileOutputFormat.setCompressOutput(job, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置压缩的方式</span></span><br><span class="line"><span class="comment">//        FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span></span><br><span class="line"><span class="comment">//        FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</span></span><br><span class="line">        FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output666&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-保持不变-1"><a href="#（2）Mapper-保持不变-1" class="headerlink" title="（2）Mapper 保持不变"></a>（2）Mapper 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.reduceCompress;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-保持不变-1"><a href="#（3）Reducer-保持不变-1" class="headerlink" title="（3）Reducer 保持不变"></a>（3）Reducer 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.reduceCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:41 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：概述&quot;&gt;&lt;a href=&quot;#一：概述&quot; class=&quot;headerlink&quot; title=&quot;一：概述&quot;&gt;&lt;/a&gt;一：概述&lt;/h1&gt;&lt;h2 id=&quot;1-1-优缺点&quot;&gt;&lt;a href=&quot;#1-1-优缺点&quot; class=&quot;headerlink&quot; title=&quot;1.1</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce 开发总结</title>
    <link href="https://aiyingke.cn/blog/6e775d30.html/"/>
    <id>https://aiyingke.cn/blog/6e775d30.html/</id>
    <published>2023-02-10T06:58:36.000Z</published>
    <updated>2023-02-10T07:37:33.108Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：输入数据接口：InputFormat"><a href="#一：输入数据接口：InputFormat" class="headerlink" title="一：输入数据接口：InputFormat"></a>一：输入数据接口：InputFormat</h1><ul><li>默认使用的实现类是：TextInputFormat</li><li>TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为 key，行内容作为 value 返回。</li><li>CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</li></ul><h1 id="二：逻辑处理接口：Mapper"><a href="#二：逻辑处理接口：Mapper" class="headerlink" title="二：逻辑处理接口：Mapper"></a>二：逻辑处理接口：Mapper</h1><ul><li>用户根据业务需求实现其中三个方法：map() setup() cleanup ()</li></ul><h1 id="三：Partitioner-分区"><a href="#三：Partitioner-分区" class="headerlink" title="三：Partitioner 分区"></a>三：Partitioner 分区</h1><ul><li>有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个 分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces</li><li>如果业务上有特别的需求，可以自定义分区。</li></ul><h1 id="四：Comparable-排序"><a href="#四：Comparable-排序" class="headerlink" title="四：Comparable 排序"></a>四：Comparable 排序</h1><ul><li>当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接 口，重写其中的 compareTo()方法。</li><li>部分排序：对最终输出的每一个文件进行内部排序。</li><li>全排序：对所有数据进行排序，通常只有一个 Reduce。</li><li>二次排序：排序的条件有两个。</li></ul><h1 id="五：Combiner-合并"><a href="#五：Combiner-合并" class="headerlink" title="五：Combiner 合并"></a>五：Combiner 合并</h1><ul><li>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的 业务处理结果。</li></ul><h1 id="六：逻辑处理接口：Reducer"><a href="#六：逻辑处理接口：Reducer" class="headerlink" title="六：逻辑处理接口：Reducer"></a>六：逻辑处理接口：Reducer</h1><ul><li>用户根据业务需求实现其中三个方法：reduce() setup() cleanup ()</li></ul><h1 id="七：输出数据接口：OutputFormat"><a href="#七：输出数据接口：OutputFormat" class="headerlink" title="七：输出数据接口：OutputFormat"></a>七：输出数据接口：OutputFormat</h1><ul><li>用户根据业务需求实现其中三个方法：reduce() setup() cleanup ()</li><li>用户还可以自定义 OutputFormat。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：输入数据接口：InputFormat&quot;&gt;&lt;a href=&quot;#一：输入数据接口：InputFormat&quot; class=&quot;headerlink&quot; title=&quot;一：输入数据接口：InputFormat&quot;&gt;&lt;/a&gt;一：输入数据接口：InputFormat&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-ETL数据清洗</title>
    <link href="https://aiyingke.cn/blog/e164e43a.html/"/>
    <id>https://aiyingke.cn/blog/e164e43a.html/</id>
    <published>2023-02-10T04:41:19.000Z</published>
    <updated>2023-02-10T05:49:10.717Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：数据清洗（ETL）"><a href="#一：数据清洗（ETL）" class="headerlink" title="一：数据清洗（ETL）"></a>一：数据清洗（ETL）</h1><p>“ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取 （Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据仓库。</p><p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户 要求的数据。<code>清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序。</code></p><h1 id="二：案例分析"><a href="#二：案例分析" class="headerlink" title="二：案例分析"></a>二：案例分析</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>去除日志中字段个数小于等于 11 的日志。</p><p>（1）输入数据：web.log</p><p>（2）期望输出数据：每行字段长度都大于 11。</p><h2 id="2-2-需求分析"><a href="#2-2-需求分析" class="headerlink" title="2.2 需求分析"></a>2.2 需求分析</h2><p>在 Map 阶段对输入的数据根据规则进行过滤清洗。</p><h2 id="2-3-代码实现"><a href="#2-3-代码实现" class="headerlink" title="2.3 代码实现"></a>2.3 代码实现</h2><h3 id="（1）编写-WebLogMapper-类"><a href="#（1）编写-WebLogMapper-类" class="headerlink" title="（1）编写 WebLogMapper 类"></a>（1）编写 WebLogMapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.ETL;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 12:52 2023/2/10</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.解析日志</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">res</span> <span class="operator">=</span> parseLog(line, context);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.日志不合法退出</span></span><br><span class="line">        <span class="keyword">if</span> (!res) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.日志合法直接写出</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 12:57 2023/2/10</span></span><br><span class="line"><span class="comment">     * Description: 日志清洗规则类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">parseLog</span><span class="params">(String line, Context context)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.按空格分割</span></span><br><span class="line">        <span class="keyword">final</span> String[] s = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.过滤日志长度大于11的数据</span></span><br><span class="line">        <span class="keyword">if</span> (s.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-WebLogDriver-类"><a href="#（2）编写-WebLogDriver-类" class="headerlink" title="（2）编写 WebLogDriver 类"></a>（2）编写 WebLogDriver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.ETL;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 13:01 2023/2/10</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="comment">// 参数：输入输出路径</span></span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;Y:\\Temp\\input&quot;</span>, <span class="string">&quot;Y:\\Temp\\output2&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取job信息</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载jar包</span></span><br><span class="line">        job.setJarByClass(WebLogMapper.class);</span><br><span class="line">        <span class="comment">// 关联map</span></span><br><span class="line">        job.setMapperClass(WebLogMapper.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 ReduceTask 个数为0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：数据清洗（ETL）&quot;&gt;&lt;a href=&quot;#一：数据清洗（ETL）&quot; class=&quot;headerlink&quot; title=&quot;一：数据清洗（ETL）&quot;&gt;&lt;/a&gt;一：数据清洗（ETL）&lt;/h1&gt;&lt;p&gt;“ETL，是英文 Extract-Transform-Load 的缩</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce Join 应用</title>
    <link href="https://aiyingke.cn/blog/bb94c1b7.html/"/>
    <id>https://aiyingke.cn/blog/bb94c1b7.html/</id>
    <published>2023-02-01T08:10:56.000Z</published>
    <updated>2023-02-03T07:01:03.329Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Reduce-Join"><a href="#一：Reduce-Join" class="headerlink" title="一：Reduce Join"></a>一：Reduce Join</h1><p>​Map 端的主要工作：为来自不同表或文件的 key&#x2F;value 对，打标签以区别不同来源的记 录。然后用连接字段作为 key，其余部分和新加的标志作为 value，最后进行输出。</p><p>​Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，我们只需要 在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进 行合并就 ok 了。</p><h1 id="二：Reduce-Join-案例实操"><a href="#二：Reduce-Join-案例实操" class="headerlink" title="二：Reduce Join 案例实操"></a>二：Reduce Join 案例实操</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>订单数据表</p><img src="/blog/bb94c1b7.html/image-20230201175235245.png" style="zoom:50%;"><p>商品信息表</p><img src="/blog/bb94c1b7.html/image-20230201175323217.png" alt="image-20230201175323217" style="zoom:50%;"><img src="/blog/bb94c1b7.html/image-20230201175344682.png" alt="image-20230201175344682" style="zoom:50%;"><p>将商品信息表中数据根据商品 pid 合并到订单数据表中。</p><p>最终数据形式：</p><img src="/blog/bb94c1b7.html/image-20230201175431751.png" style="zoom:50%;"><h2 id="2-2-需求分析"><a href="#2-2-需求分析" class="headerlink" title="2.2 需求分析"></a>2.2 需求分析</h2><p>​通过将关联条件作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源 的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。</p><h2 id="2-3-Reduce端表合并（数据倾斜）"><a href="#2-3-Reduce端表合并（数据倾斜）" class="headerlink" title="2.3 Reduce端表合并（数据倾斜）"></a>2.3 Reduce端表合并（数据倾斜）</h2><p><img src="/blog/bb94c1b7.html/image-20230201175657723.png"></p><h2 id="2-4-代码实现"><a href="#2-4-代码实现" class="headerlink" title="2.4 代码实现"></a>2.4 代码实现</h2><h3 id="（1）创建商品和订单合并后的-TableBean-类"><a href="#（1）创建商品和订单合并后的-TableBean-类" class="headerlink" title="（1）创建商品和订单合并后的 TableBean 类"></a>（1）创建商品和订单合并后的 TableBean 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:01 2023/2/1</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id;  <span class="comment">//订单ID</span></span><br><span class="line">    <span class="keyword">private</span> String pid;  <span class="comment">//产品ID</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> amount;  <span class="comment">//产品数量</span></span><br><span class="line">    <span class="keyword">private</span> String pName;  <span class="comment">//产品名称</span></span><br><span class="line">    <span class="keyword">private</span> String flag;  <span class="comment">//判断订单表（order）或者产品表（pd）的标志字段</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 最终输出形态</span></span><br><span class="line">        <span class="keyword">return</span> id + <span class="string">&quot;\t&quot;</span> + pName + <span class="string">&quot;\t&quot;</span> + amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeUTF(id);</span><br><span class="line">        dataOutput.writeUTF(pid);</span><br><span class="line">        dataOutput.writeInt(amount);</span><br><span class="line">        dataOutput.writeUTF(pName);</span><br><span class="line">        dataOutput.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.pid = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.amount = dataInput.readInt();</span><br><span class="line">        <span class="built_in">this</span>.pName = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.flag = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-TableMapper-类"><a href="#（2）编写-TableMapper-类" class="headerlink" title="（2）编写 TableMapper 类"></a>（2）编写 TableMapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:38 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, TableBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String filename;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">TableBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取对应文件名称</span></span><br><span class="line">        <span class="type">InputSplit</span> <span class="variable">split</span> <span class="operator">=</span> context.getInputSplit();</span><br><span class="line">        <span class="type">FileSplit</span> <span class="variable">fileSplit</span> <span class="operator">=</span> (FileSplit) split;</span><br><span class="line">        filename = fileSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是哪个文件，然后针对文件进行不同的操作</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (filename.contains(<span class="string">&quot;order&quot;</span>)) &#123;</span><br><span class="line">            <span class="comment">// 订单表的处理</span></span><br><span class="line">            <span class="comment">// 封装 outK</span></span><br><span class="line">            outK.set(split[<span class="number">1</span>]); <span class="comment">//两表相同的字段 用于进入同一个reduce     pid</span></span><br><span class="line">            <span class="comment">// 封装 outV</span></span><br><span class="line">            outV.setId(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPid(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setAmount(Integer.parseInt(split[<span class="number">2</span>])); <span class="comment">// int 类型</span></span><br><span class="line">            outV.setPName(<span class="string">&quot; &quot;</span>);  <span class="comment">// 该表中未含有这个字段，设置为空</span></span><br><span class="line">            outV.setFlag(<span class="string">&quot;order&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 商品表的处理</span></span><br><span class="line">            <span class="comment">// 封装 outK</span></span><br><span class="line">            outK.set(split[<span class="number">0</span>]);</span><br><span class="line">            <span class="comment">// 封装 outV</span></span><br><span class="line">            outV.setId(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            outV.setPid(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setAmount(<span class="number">0</span>); <span class="comment">// int 类型</span></span><br><span class="line">            outV.setPName(split[<span class="number">1</span>]);  <span class="comment">// 该表中未含有这个字段，设置为空</span></span><br><span class="line">            outV.setFlag(<span class="string">&quot;pd&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 写出 KV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）编写-TableReducer-类"><a href="#（3）编写-TableReducer-类" class="headerlink" title="（3）编写 TableReducer 类"></a>（3）编写 TableReducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationTargetException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:59 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, TableBean, TableBean, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Reducer&lt;Text, TableBean, TableBean, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">TableBean</span> <span class="variable">pdBean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line">            <span class="comment">// 判断数据来自哪个表</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&quot;order&quot;</span>.equals(value.getFlag())) &#123;</span><br><span class="line">                <span class="comment">// 订单表</span></span><br><span class="line">                <span class="comment">// 创建一个临时TableBean对象接收value,不可以直接进行赋值，hadoop内部进行了优化 传递过来的对象仅有地址，因此直接赋值 只会保留最后一个对象的信息</span></span><br><span class="line">                <span class="type">TableBean</span> <span class="variable">tmpOrderBean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(tmpOrderBean, value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException | IllegalAccessException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 将临时创建的对象 添加进入集合中</span></span><br><span class="line">                orderBeans.add(tmpOrderBean);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(pdBean,value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException | IllegalAccessException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//遍历集合 orderBeans,替换掉每个 orderBean 的 pid 为 pName,然后写出</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean orderBean : orderBeans) &#123;</span><br><span class="line">            <span class="comment">// 同pid 进同一个reduce 故而pName 唯一</span></span><br><span class="line">            orderBean.setPName(pdBean.getPName());</span><br><span class="line">            <span class="comment">// 写出修改后的对象</span></span><br><span class="line">            context.write(orderBean,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）编写-TableDriver-类"><a href="#（4）编写-TableDriver-类" class="headerlink" title="（4）编写 TableDriver 类"></a>（4）编写 TableDriver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:16 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(TableDriver.class);</span><br><span class="line">        job.setMapperClass(TableMapper.class);</span><br><span class="line">        job.setReducerClass(TableReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(TableBean.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(TableBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input\\&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output5\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）运行结果"><a href="#（5）运行结果" class="headerlink" title="（5）运行结果"></a>（5）运行结果</h3><img src="/blog/bb94c1b7.html/image-20230203142810183.png" alt="image-20230203142810183" style="zoom: 67%;"><h3 id="（6）总结"><a href="#（6）总结" class="headerlink" title="（6）总结"></a>（6）总结</h3><p>​缺点：这种方式中，合并的操作是在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜。</p><p>​解决方案：Map 端实现数据合并。</p><h1 id="三：Map-Join"><a href="#三：Map-Join" class="headerlink" title="三：Map Join"></a>三：Map Join</h1><h2 id="3-1-应用场景"><a href="#3-1-应用场景" class="headerlink" title="3.1 应用场景"></a>3.1 应用场景</h2><p>Map Join 适用于一张表十分小、一张表很大的场景。</p><h2 id="3-2-优点"><a href="#3-2-优点" class="headerlink" title="3.2 优点"></a>3.2 优点</h2><p>在 Reduce 端处理过多的表，非常容易产生数据倾斜；</p><p>在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数 据的压力，尽可能的减少数据倾斜。</p><h2 id="3-3-实现手段——采用-DistributedCache"><a href="#3-3-实现手段——采用-DistributedCache" class="headerlink" title="3.3 实现手段——采用 DistributedCache"></a>3.3 实现手段——采用 DistributedCache</h2><p>（1）在 Mapper 的 setup 阶段，将文件读取到缓存集合中。</p><p>（2）在 Driver 驱动类中加载缓存。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到 Task 运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///y:/Temp/input/pd.txt&quot;</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置 HDFS 路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop101:8020/tmp/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure><h1 id="四：Map-Join-案例实操"><a href="#四：Map-Join-案例实操" class="headerlink" title="四：Map Join 案例实操"></a>四：Map Join 案例实操</h1><h2 id="4-1-需求"><a href="#4-1-需求" class="headerlink" title="4.1 需求"></a>4.1 需求</h2><p><img src="/blog/bb94c1b7.html/image-20230203143257142.png"></p><h2 id="4-2-需求分析"><a href="#4-2-需求分析" class="headerlink" title="4.2 需求分析"></a>4.2 需求分析</h2><p>MapJoin 适用于关联表中有小表的情形；</p><p><img src="/blog/bb94c1b7.html/image-20230203143357885.png"></p><h2 id="4-3-代码实现"><a href="#4-3-代码实现" class="headerlink" title="4.3 代码实现"></a>4.3 代码实现</h2><h3 id="（1）先在-MapJoinDriver-驱动类中添加缓存文件"><a href="#（1）先在-MapJoinDriver-驱动类中添加缓存文件" class="headerlink" title="（1）先在 MapJoinDriver 驱动类中添加缓存文件"></a>（1）先在 MapJoinDriver 驱动类中添加缓存文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.mapJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:16 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException, URISyntaxException &#123;</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(TableDriver.class);</span><br><span class="line">        job.setMapperClass(TableMapper.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 14:37 2023/2/3</span></span><br><span class="line"><span class="comment">         * Description: 加载缓存数据</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///Y:/Temp/pd.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input\\&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output2\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）在-MapJoinMapper-类中的-setup-方法中读取缓存文件"><a href="#（2）在-MapJoinMapper-类中的-setup-方法中读取缓存文件" class="headerlink" title="（2）在 MapJoinMapper 类中的 setup 方法中读取缓存文件"></a>（2）在 MapJoinMapper 类中的 setup 方法中读取缓存文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.mapJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:38 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 14:43 2023/2/3</span></span><br><span class="line"><span class="comment">     * Description: 任务开始前 先将pd数据缓存进入 pdMap</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 通过缓存文件得到小表数据 pd.txt</span></span><br><span class="line">        <span class="keyword">final</span> URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(cacheFiles[<span class="number">0</span>]);</span><br><span class="line">        <span class="comment">// 获取文件系统对象 并开启流</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(context.getConfiguration());</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FSDataInputStream</span> <span class="variable">open</span> <span class="operator">=</span> fileSystem.open(path);</span><br><span class="line">        <span class="comment">// 通过包装流转换为reader,方便按行读取</span></span><br><span class="line">        <span class="type">BufferedReader</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(open, StandardCharsets.UTF_8));</span><br><span class="line">        <span class="comment">// 逐行读取、按行处理</span></span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotEmpty(line = reader.readLine())) &#123;</span><br><span class="line">            <span class="comment">// 切割一行     11 小米</span></span><br><span class="line">            <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            pdMap.put(split[<span class="number">0</span>], split[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关闭流</span></span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 读取大表数据</span></span><br><span class="line">        <span class="comment">// 1001 11 2</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">// 通过大表的每行数据的pid 去取出pdMap 中的pName</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">pName</span> <span class="operator">=</span> pdMap.get(split[<span class="number">1</span>]);</span><br><span class="line">        <span class="comment">// 将大表每行数据的pid替换为pName</span></span><br><span class="line">        text.set(split[<span class="number">0</span>] + <span class="string">&quot;\t&quot;</span> + pName + <span class="string">&quot;\t&quot;</span> + split[<span class="number">2</span>]);</span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(text, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Reduce-Join&quot;&gt;&lt;a href=&quot;#一：Reduce-Join&quot; class=&quot;headerlink&quot; title=&quot;一：Reduce Join&quot;&gt;&lt;/a&gt;一：Reduce Join&lt;/h1&gt;&lt;p&gt;​		Map 端的主要工作：为来自不同表或文件的 k</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce 内核源码解析</title>
    <link href="https://aiyingke.cn/blog/77ff229c.html/"/>
    <id>https://aiyingke.cn/blog/77ff229c.html/</id>
    <published>2022-12-19T04:04:08.000Z</published>
    <updated>2023-02-01T07:06:40.694Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：MapTask-工作机制"><a href="#一：MapTask-工作机制" class="headerlink" title="一：MapTask 工作机制"></a>一：MapTask 工作机制</h1><p><img src="/blog/77ff229c.html/image-20221219121005329.png"></p><p>（1）Read 阶段：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中 解析出一个个 key&#x2F;value。</p><p>（2）Map 阶段：该节点主要是将解析出的 key&#x2F;value 交给用户编写 map()函数处理，并 产生一系列新的 key&#x2F;value。</p><p>（3）Collect 收集阶段：在用户编写 map()函数中，当数据处理完成后，一般会调用 OutputCollector.collect()输出结果。在该函数内部，它会将生成的 key&#x2F;value 分区（调用 Partitioner），并写入一个环形内存缓冲区中。</p><p>（4）Spill 阶段：即“溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上， 生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p><p><strong>溢写阶段详情：</strong></p><p>步骤 1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号 Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在 一起，且同一分区内所有数据按照 key 有序。</p><p>步骤 2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文 件 output&#x2F;spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之 前，对每个分区中的数据进行一次聚集操作。</p><p>步骤 3：将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元 信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大 小超过 1MB，则将内存索引写到文件 output&#x2F;spillN.out.index 中。</p><p>（5）Merge 阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并， 以确保最终只会生成一个数据文件。</p><p>当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output&#x2F;file.out 中，同时生成相应的索引文件 output&#x2F;file.out.index。 </p><p>在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多 轮递归合并的方式。每轮合并 mapreduce.task.io.sort.factor（默认 10）个文件，并将产生的文 件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 </p><p>让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量 小文件产生的随机读取带来的开销。</p><h1 id="二：ReduceTask-工作机制"><a href="#二：ReduceTask-工作机制" class="headerlink" title="二：ReduceTask 工作机制"></a>二：ReduceTask 工作机制</h1><p><img src="/blog/77ff229c.html/image-20221219121913332.png"></p><p>（1）Copy 阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数 据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p><p>（2）Sort 阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁 盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照 MapReduce 语义，用 户编写 reduce()函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一 起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了 局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。</p><p>（3）Reduce 阶段：reduce()函数将计算结果写到 HDFS 上。</p><h1 id="三：ReduceTask-并行度决定机制"><a href="#三：ReduceTask-并行度决定机制" class="headerlink" title="三：ReduceTask 并行度决定机制"></a>三：ReduceTask 并行度决定机制</h1><p>MapTask 并行度由切片个数决定，切片个数由输入文件和切片规则决定。</p><h2 id="3-1-设置-ReduceTask-并行度（个数）"><a href="#3-1-设置-ReduceTask-并行度（个数）" class="headerlink" title="3.1 设置 ReduceTask 并行度（个数）"></a>3.1 设置 ReduceTask 并行度（个数）</h2><p>ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认值是 1，手动设置为 4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure><h2 id="3-2-实验：测试-ReduceTask-多少合适"><a href="#3-2-实验：测试-ReduceTask-多少合适" class="headerlink" title="3.2 实验：测试 ReduceTask 多少合适?"></a>3.2 实验：测试 ReduceTask 多少合适?</h2><p>（1）实验环境：1 个 Master 节点，16 个 Slave 节点：CPU:8GHZ，内存: 2G</p><p>（2）实验结论：</p><p>观察实验数据，可知满足正态分布，故而可以得出最优解。</p><p><img src="/blog/77ff229c.html/image-20221219122251264.png"></p><h2 id="3-2-注意事项"><a href="#3-2-注意事项" class="headerlink" title="3.2 注意事项"></a>3.2 注意事项</h2><ul><li>ReduceTask&#x3D;0，表示没有Reduce阶段，输出文件个数和Map个数一致。</li><li>ReduceTask默认值就是1，所以输出文件个数为一个。</li><li>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜。</li><li>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全 局汇总结果，就只能有1个ReduceTask。</li><li>具体多少个ReduceTask，需要根据集群性能而定。</li><li>如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1 肯定不执行。</li></ul><h1 id="四：MapTask-amp-ReduceTask-源码解析"><a href="#四：MapTask-amp-ReduceTask-源码解析" class="headerlink" title="四：MapTask &amp; ReduceTask 源码解析"></a>四：MapTask &amp; ReduceTask 源码解析</h1><h2 id="4-1-MapTask-源码解析流程"><a href="#4-1-MapTask-源码解析流程" class="headerlink" title="4.1 MapTask 源码解析流程"></a>4.1 MapTask 源码解析流程</h2><p><img src="/blog/77ff229c.html/image-20230201145206556.png"></p><h2 id="4-2-ReduceTask-源码解析流程"><a href="#4-2-ReduceTask-源码解析流程" class="headerlink" title="4.2 ReduceTask 源码解析流程"></a>4.2 ReduceTask 源码解析流程</h2><p><img src="/blog/77ff229c.html/image-20230201150523559.png"></p><p><img src="/blog/77ff229c.html/image-20230201150555973.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：MapTask-工作机制&quot;&gt;&lt;a href=&quot;#一：MapTask-工作机制&quot; class=&quot;headerlink&quot; title=&quot;一：MapTask 工作机制&quot;&gt;&lt;/a&gt;一：MapTask 工作机制&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/77ff22</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce OutputFormat数据输出</title>
    <link href="https://aiyingke.cn/blog/83c17171.html/"/>
    <id>https://aiyingke.cn/blog/83c17171.html/</id>
    <published>2022-12-19T01:55:35.000Z</published>
    <updated>2022-12-19T03:20:56.570Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：OutputFormat-接口实现类"><a href="#一：OutputFormat-接口实现类" class="headerlink" title="一：OutputFormat 接口实现类"></a>一：OutputFormat 接口实现类</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat 接口。下面我们介绍几种常见的OutputFormat实现类。</p><h2 id="1-2-OutputFormat实现类"><a href="#1-2-OutputFormat实现类" class="headerlink" title="1.2 OutputFormat实现类"></a>1.2 OutputFormat实现类</h2><p><img src="/blog/83c17171.html/image-20221219095707511.png"></p><h2 id="1-3-默认输出格式TextOutputFormat"><a href="#1-3-默认输出格式TextOutputFormat" class="headerlink" title="1.3 默认输出格式TextOutputFormat"></a>1.3 默认输出格式TextOutputFormat</h2><h2 id="1-4-自定义OutputFormat"><a href="#1-4-自定义OutputFormat" class="headerlink" title="1.4 自定义OutputFormat"></a>1.4 自定义OutputFormat</h2><h3 id="（1）-应用场景"><a href="#（1）-应用场景" class="headerlink" title="（1） 应用场景"></a>（1） 应用场景</h3><p>例如：输出数据到MySQL&#x2F;HBase&#x2F;Elasticsearch等存储框架中。</p><h3 id="（2）自定义OutputFormat步骤"><a href="#（2）自定义OutputFormat步骤" class="headerlink" title="（2）自定义OutputFormat步骤"></a>（2）自定义OutputFormat步骤</h3><ul><li>自定义一个类继承FileOutputFormat</li><li>改写RecordWriter，具体改写输出数据的方法write()</li></ul><h1 id="二：自定义-OutputFormat-案例实操"><a href="#二：自定义-OutputFormat-案例实操" class="headerlink" title="二：自定义 OutputFormat 案例实操"></a>二：自定义 OutputFormat 案例实操</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>过滤输入的 log 日志，包含 aiyingke 的网站输出到 e:&#x2F;aiyingke.log，不包含 aiyingke 的网站输出到 e:&#x2F;other.log；</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>log.txt</li></ul><h3 id="（2）期望输出数据"><a href="#（2）期望输出数据" class="headerlink" title="（2）期望输出数据"></a>（2）期望输出数据</h3><ul><li>aiyingke.log</li><li>other.log</li></ul><h2 id="2-2-需求分析"><a href="#2-2-需求分析" class="headerlink" title="2.2 需求分析"></a>2.2 需求分析</h2><ul><li>需求</li><li>输入数据</li><li>输出数据</li><li>自定义一个 OutputFormat 类<ul><li>创建一个类LogRecordWriter继承RecordWriter</li><li>创建两个文件的输出流：aiyingke，other</li><li>如果输入数据包含aiyingke，输出到aiyingkeOut流 如果不包含aiyingke，输出到otherOut流</li></ul></li><li>驱动类 Driver<ul><li>要将自定义的输出格式组件设置到job中</li><li>job.setOutputFormatClass(LogOutputFormat.class) ;</li></ul></li></ul><h2 id="2-3-代码实现"><a href="#2-3-代码实现" class="headerlink" title="2.3 代码实现"></a>2.3 代码实现</h2><h3 id="（1）编写-LogMapper-类"><a href="#（1）编写-LogMapper-类" class="headerlink" title="（1）编写 LogMapper 类"></a>（1）编写 LogMapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:08 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// https://aiyingke.cn</span></span><br><span class="line">        <span class="comment">// 互换 KV 位置</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-LogReducer-类"><a href="#（2）编写-LogReducer-类" class="headerlink" title="（2）编写 LogReducer 类"></a>（2）编写 LogReducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:11 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, NullWritable, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Reducer&lt;Text, NullWritable, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key 相同情况,进入同一个reduce,遍历结果</span></span><br><span class="line">        <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">            <span class="comment">// 写出</span></span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）自定义一个-LogOutputFormat-类"><a href="#（3）自定义一个-LogOutputFormat-类" class="headerlink" title="（3）自定义一个 LogOutputFormat 类"></a>（3）自定义一个 LogOutputFormat 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:15 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogOutputFormat</span> <span class="keyword">extends</span> <span class="title class_">FileOutputFormat</span>&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title function_">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 创建自定义的 LogRecordWriter</span></span><br><span class="line">        <span class="type">LogRecordWriter</span> <span class="variable">lrw</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LogRecordWriter</span>(job);</span><br><span class="line">        <span class="keyword">return</span> lrw;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）编写-LogRecordWriter-类"><a href="#（4）编写-LogRecordWriter-类" class="headerlink" title="（4）编写 LogRecordWriter 类"></a>（4）编写 LogRecordWriter 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:44 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogRecordWriter</span> <span class="keyword">extends</span> <span class="title class_">RecordWriter</span>&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> FSDataOutputStream otherOut;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> FSDataOutputStream aiyingkeOut;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">LogRecordWriter</span><span class="params">(TaskAttemptContext job)</span> &#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(job.getConfiguration());</span><br><span class="line">            <span class="comment">// 用文件系统对象创建两个输出流对应不同的目录</span></span><br><span class="line">            aiyingkeOut = fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\logOut\\aiyingke\\aiyingke.log&quot;</span>));</span><br><span class="line">            otherOut = fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\logOut\\other\\other.log&quot;</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(Text text, NullWritable nullWritable)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="comment">// 根据当前行是否包含 aiyingke 决定采用哪个流输出</span></span><br><span class="line">        <span class="keyword">if</span> (line.contains(<span class="string">&quot;aiyingke&quot;</span>)) &#123;</span><br><span class="line">            aiyingkeOut.writeBytes(line + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            otherOut.writeBytes(line + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        IOUtils.closeStream(aiyingkeOut);</span><br><span class="line">        IOUtils.closeStream(otherOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）编写-LogDriver-类"><a href="#（5）编写-LogDriver-类" class="headerlink" title="（5）编写 LogDriver 类"></a>（5）编写 LogDriver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 11:04 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;Log&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper  reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(LogMapper.class);</span><br><span class="line">        job.setReducerClass(LogReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置自定义的 OutputFormat</span></span><br><span class="line">        job.setOutputFormatClass(LogOutputFormat.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         *         虽然我们定义了 OutputFormat ,但是因为我们的 OutputFormat 继承自 FileOutputFormat</span></span><br><span class="line"><span class="comment">         *         而 FileOutputFormat 要输出一个 _SUCCESS 的标志文件,所以这里换得指定一个输出目录</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\log\\&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\logOut\\&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：OutputFormat-接口实现类&quot;&gt;&lt;a href=&quot;#一：OutputFormat-接口实现类&quot; class=&quot;headerlink&quot; title=&quot;一：OutputFormat 接口实现类&quot;&gt;&lt;/a&gt;一：OutputFormat 接口实现类&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mapreduce Combiner合并</title>
    <link href="https://aiyingke.cn/blog/b3e50fdd.html/"/>
    <id>https://aiyingke.cn/blog/b3e50fdd.html/</id>
    <published>2022-12-18T10:32:39.000Z</published>
    <updated>2022-12-18T11:28:28.458Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Combiner合并概述"><a href="#一：Combiner合并概述" class="headerlink" title="一：Combiner合并概述"></a>一：Combiner合并概述</h1><p>（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。</p><p>（2）Combiner组件的父类就是Reducer。</p><p>（3）Combiner和Reducer的区别在于运行的位置</p><ul><li>Combiner是在每一个MapTask所在的节点运行</li><li>Reducer是接收全局所有Mapper的输出结果</li></ul><p>（4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。</p><p>（5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv 应该跟Reducer的输入kv类型要对应起来。以下求平均值就不能够使用 Combiner；</p><p><img src="/blog/b3e50fdd.html/image-20221218183450549.png"></p><h1 id="二：自定义-Combiner-实现步骤"><a href="#二：自定义-Combiner-实现步骤" class="headerlink" title="二：自定义 Combiner 实现步骤"></a>二：自定义 Combiner 实现步骤</h1><h2 id="2-1-自定义一个-Combiner-继承-Reducer，重写-Reduce-方法"><a href="#2-1-自定义一个-Combiner-继承-Reducer，重写-Reduce-方法" class="headerlink" title="2.1 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法"></a>2.1 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line"></span><br><span class="line">## 2.2 在 Job 驱动类中设置</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 三：Combiner 合并案例实操</span><br><span class="line"></span><br><span class="line">## 3.1 需求</span><br><span class="line"></span><br><span class="line">统计过程中对每一个 MapTask 的输出进行局部汇总，以减小网络传输量即采用 Combiner 功能。</span><br><span class="line"></span><br><span class="line">### （1）数据输入</span><br><span class="line"></span><br><span class="line">- data.txt</span><br><span class="line"></span><br><span class="line">### （2）期望输出数据</span><br><span class="line"></span><br><span class="line">- Combine 输入数据多，输出时经过合并，输出数据降低。</span><br><span class="line"></span><br><span class="line">## 3.2 需求分析</span><br><span class="line"></span><br><span class="line">对每一个MapTask的输出局部汇总（Combiner）；</span><br><span class="line"></span><br><span class="line">![](./Mapreduce-Combiner合并/image-20221218183845643.png)</span><br><span class="line"></span><br><span class="line">## 3.3 代码实现（方案一）</span><br><span class="line"></span><br><span class="line">### （1）增加一个 WordCountCombiner 类继承 Reducer</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">package cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Author: Rupert Tears</span><br><span class="line"> * Date: Created in 19:18 2022/12/18</span><br><span class="line"> * Description: Thought is already is late, exactly is the earliest time.</span><br><span class="line"> */</span><br><span class="line">public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private final IntWritable outV = new IntWritable();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        // 定义聚合变量</span><br><span class="line">        int sum = 0;</span><br><span class="line"></span><br><span class="line">        // 遍历求和</span><br><span class="line">        for (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 封装 outKV</span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        // 写出</span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）在-WordcountDriver-驱动类中指定-Combiner"><a href="#（2）在-WordcountDriver-驱动类中指定-Combiner" class="headerlink" title="（2）在 WordcountDriver 驱动类中指定 Combiner"></a>（2）在 WordcountDriver 驱动类中指定 Combiner</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置自定义的 Combiner</span></span><br><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure><h2 id="3-4-代码实现（方案二）"><a href="#3-4-代码实现（方案二）" class="headerlink" title="3.4 代码实现（方案二）"></a>3.4 代码实现（方案二）</h2><h3 id="（1）将-WordcountReducer-作为-Combiner-在-WordcountDriver-驱动类中指定"><a href="#（1）将-WordcountReducer-作为-Combiner-在-WordcountDriver-驱动类中指定" class="headerlink" title="（1）将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定"></a>（1）将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountReducer.class);</span><br></pre></td></tr></table></figure><h2 id="3-5-代码汇总"><a href="#3-5-代码汇总" class="headerlink" title="3.5 代码汇总"></a>3.5 代码汇总</h2><h3 id="（1）驱动器类"><a href="#（1）驱动器类" class="headerlink" title="（1）驱动器类"></a>（1）驱动器类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        <span class="comment">// 设置自定义的 Combiner</span></span><br><span class="line">        job.setCombinerClass(WordCountCombiner.class);</span><br><span class="line"><span class="comment">//        job.setCombinerClass(WordCountReducer.class);</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\asdsad&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-类"><a href="#（2）Mapper-类" class="headerlink" title="（2）Mapper 类"></a>（2）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-类"><a href="#（3）Reducer-类" class="headerlink" title="（3）Reducer 类"></a>（3）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:41 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）Combiner-类"><a href="#（4）Combiner-类" class="headerlink" title="（4）Combiner 类"></a>（4）Combiner 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 19:18 2022/12/18</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义聚合变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 封装 outKV</span></span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 日志输出</span><br><span class="line">Combine input records=6</span><br><span class="line">Combine output records=3</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Combiner合并概述&quot;&gt;&lt;a href=&quot;#一：Combiner合并概述&quot; class=&quot;headerlink&quot; title=&quot;一：Combiner合并概述&quot;&gt;&lt;/a&gt;一：Combiner合并概述&lt;/h1&gt;&lt;p&gt;（1）Combiner是MR程序中Mappe</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mapreduce WritableComparable排序</title>
    <link href="https://aiyingke.cn/blog/59213c23.html/"/>
    <id>https://aiyingke.cn/blog/59213c23.html/</id>
    <published>2022-12-18T09:34:38.000Z</published>
    <updated>2022-12-18T10:31:24.465Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：排序概述"><a href="#一：排序概述" class="headerlink" title="一：排序概述"></a>一：排序概述</h1><ul><li>排序是MapReduce框架中最重要的操作之一。</li><li>MapTask和ReduceTask均会对数据按 照key进行排序。该操作属于 Hadoop的默认行为。<code>任何应用程序中的数据均会被排序，而不管逻辑上是否需要</code>。</li><li>默认排序是按照<code>字典顺序排序</code>，且实现该排序的方法是<code>快速排序</code>。</li><li>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，<code>当环形缓冲区使 用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序</code>，并将这些有序数据溢写到磁盘上，而<code>当数据处理完毕后，它会对磁盘上所有文件进行归并排序</code>。</li><li>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大 小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到 一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者 数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完 毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</li></ul><h1 id="二：排序分类"><a href="#二：排序分类" class="headerlink" title="二：排序分类"></a>二：排序分类</h1><h2 id="（1）部分排序"><a href="#（1）部分排序" class="headerlink" title="（1）部分排序"></a>（1）部分排序</h2><p>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。</p><h2 id="（2）全排序"><a href="#（2）全排序" class="headerlink" title="（2）全排序"></a>（2）全排序</h2><p>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在 处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。</p><h2 id="（3）辅助排序：（GroupingComparator分组）"><a href="#（3）辅助排序：（GroupingComparator分组）" class="headerlink" title="（3）辅助排序：（GroupingComparator分组）"></a>（3）辅助排序：（GroupingComparator分组）</h2><p>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部 字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</p><h2 id="（4）二次排序"><a href="#（4）二次排序" class="headerlink" title="（4）二次排序"></a>（4）二次排序</h2><p>在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</p><h1 id="三：自定义排序-WritableComparable-原理分析"><a href="#三：自定义排序-WritableComparable-原理分析" class="headerlink" title="三：自定义排序 WritableComparable 原理分析"></a>三：自定义排序 WritableComparable 原理分析</h1><p>bean 对象做为 key 传输，需要<code>实现 WritableComparable 接口重写 compareTo 方法</code>，就可 以实现排序。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean bean)</span> &#123;</span><br><span class="line"><span class="type">int</span> result;</span><br><span class="line"><span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">result = -<span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">result = <span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">result = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="四：WritableComparable-排序案例实操（全排序）"><a href="#四：WritableComparable-排序案例实操（全排序）" class="headerlink" title="四：WritableComparable 排序案例实操（全排序）"></a>四：WritableComparable 排序案例实操（全排序）</h1><h2 id="4-1-需求"><a href="#4-1-需求" class="headerlink" title="4.1 需求"></a>4.1 需求</h2><p>对 phone_data .txt 中的数据，按照总流量进行倒叙排序；</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>phone_data .txt</li></ul><h3 id="（2）期望输出数据"><a href="#（2）期望输出数据" class="headerlink" title="（2）期望输出数据"></a>（2）期望输出数据</h3><p><img src="/blog/59213c23.html/image-20221218174341483.png"></p><h2 id="4-2-需求分析"><a href="#4-2-需求分析" class="headerlink" title="4.2 需求分析"></a>4.2 需求分析</h2><p><img src="/blog/59213c23.html/image-20221218174423794.png"></p><h2 id="4-3-代码实现"><a href="#4-3-代码实现" class="headerlink" title="4.3 代码实现"></a>4.3 代码实现</h2><h3 id="（1）FlowBean-对象实现-WritableComparable，增加比较功能"><a href="#（1）FlowBean-对象实现-WritableComparable，增加比较功能" class="headerlink" title="（1）FlowBean 对象实现 WritableComparable，增加比较功能"></a>（1）FlowBean 对象实现 WritableComparable，增加比较功能</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 实现WritableComparable接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 覆写compareTo方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 按照总流量比较,倒叙排列</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="（2）实体类"><a href="#（2）实体类" class="headerlink" title="（2）实体类"></a>（2）实体类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:40 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 按照总流量比较,倒叙排列</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Mapper-类"><a href="#（3）Mapper-类" class="headerlink" title="（3）Mapper 类"></a>（3）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:59 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, FlowBean, Text&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Text</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, FlowBean,Text &gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="comment">// 13736230513 2481 24681 27162</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割数据</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.封装 outK outV</span></span><br><span class="line">        outK.setUpFlow(Long.parseLong(split[<span class="number">1</span>]));</span><br><span class="line">        outK.setDownFlow(Long.parseLong(split[<span class="number">2</span>]));</span><br><span class="line">        outK.setSumFlow();</span><br><span class="line">        outV.set(split[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.写出 outK outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）Reducer-类"><a href="#（4）Reducer-类" class="headerlink" title="（4）Reducer 类"></a>（4）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:07 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;FlowBean, Text, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历values集合,循环写出,避免总流量相同的情况</span></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            <span class="comment">// 调换 KV 位置,反向写出</span></span><br><span class="line">            context.write(value, key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）驱动器类"><a href="#（5）驱动器类" class="headerlink" title="（5）驱动器类"></a>（5）驱动器类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:14 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取job对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.关联 driver mapper reducer</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.设置 map 端输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.设置程序最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.设置程序输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\phone_data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6.提交job</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="五：WritableComparable-排序案例实操（区内排序）"><a href="#五：WritableComparable-排序案例实操（区内排序）" class="headerlink" title="五：WritableComparable 排序案例实操（区内排序）"></a>五：WritableComparable 排序案例实操（区内排序）</h1><h2 id="5-1-需求"><a href="#5-1-需求" class="headerlink" title="5.1 需求"></a>5.1 需求</h2><p>要求每个省份手机号输出的文件中按照总流量内部排序。</p><h2 id="5-2-需求分析"><a href="#5-2-需求分析" class="headerlink" title="5.2 需求分析"></a>5.2 需求分析</h2><p>基于前一个需求，增加自定义分区类，分区按照省份手机号设置。</p><p><img src="/blog/59213c23.html/image-20221218181040515.png"></p><h2 id="5-3-代码实现"><a href="#5-3-代码实现" class="headerlink" title="5.3 代码实现"></a>5.3 代码实现</h2><h3 id="（1）增加自定义分区类"><a href="#（1）增加自定义分区类" class="headerlink" title="（1）增加自定义分区类"></a>（1）增加自定义分区类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparableWithPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:13 2022/12/18</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner2</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;FlowBean, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(FlowBean flowBean, Text text, <span class="type">int</span> i)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取手机号前三位</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">prePhone</span> <span class="operator">=</span> phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义一个分区号变量 partition,根据 perPhone 设置分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回分区号</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）FlowDriver-设置分区类和-Reduce-数量"><a href="#（2）FlowDriver-设置分区类和-Reduce-数量" class="headerlink" title="（2）FlowDriver  设置分区类和 Reduce 数量"></a>（2）FlowDriver  设置分区类和 Reduce 数量</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置自定义分区器</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置对应的 ReduceTask 的个数</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h2 id="5-4-二次排序"><a href="#5-4-二次排序" class="headerlink" title="5.4 二次排序"></a>5.4 二次排序</h2><ul><li><p>若想要进行区内排序的二次排序，比如对某省份手机号的总流量进行降序；若总流量相同，按照上行流量降序排列；</p></li><li><p>仅需要在FlowBean中，对 compareTo 方法进行业务逻辑的增加。</p></li><li><pre><code class="java">    @Override    public int compareTo(FlowBean o) &#123;        // 按照总流量比较,倒叙排列（降序）        if (this.sumFlow &gt; o.sumFlow) &#123;            return -1;        &#125; else if (this.sumFlow &lt; o.sumFlow) &#123;            return 1;        &#125; else &#123;            /*             * Author: Rupert-Tears             * CreateTime: 18:29 2022/12/18             * Description: 二次排序,对上行流量升序排列             */            if (this.upFlow &gt; o.upFlow) &#123;                return 1;            &#125; else if (this.upFlow &lt; o.upFlow) &#123;                return -1;            &#125; else &#123;                return 0;            &#125;        &#125;    &#125;</code></pre></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：排序概述&quot;&gt;&lt;a href=&quot;#一：排序概述&quot; class=&quot;headerlink&quot; title=&quot;一：排序概述&quot;&gt;&lt;/a&gt;一：排序概述&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;排序是MapReduce框架中最重要的操作之一。&lt;/li&gt;
&lt;li&gt;MapTask和Reduce</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce Partition分区</title>
    <link href="https://aiyingke.cn/blog/ee4a420a.html/"/>
    <id>https://aiyingke.cn/blog/ee4a420a.html/</id>
    <published>2022-12-18T08:59:47.000Z</published>
    <updated>2022-12-18T09:31:50.705Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：问题引出"><a href="#一：问题引出" class="headerlink" title="一：问题引出"></a>一：问题引出</h1><p>要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机 归属地不同省份输出到不同文件中（分区）</p><h1 id="二：默认Partitioner分区"><a href="#二：默认Partitioner分区" class="headerlink" title="二：默认Partitioner分区"></a>二：默认Partitioner分区</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashPartitioner</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;K, V&gt; &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(K key, V value, <span class="type">int</span> numReduceTasks)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个 key存储到哪个分区。</li></ul><h1 id="三：自定义Partitioner-步骤"><a href="#三：自定义Partitioner-步骤" class="headerlink" title="三：自定义Partitioner 步骤"></a>三：自定义Partitioner 步骤</h1><h2 id="3-1-自定义类继承Partitioner，重写getPartition-方法"><a href="#3-1-自定义类继承Partitioner，重写getPartition-方法" class="headerlink" title="3.1 自定义类继承Partitioner，重写getPartition()方法"></a>3.1 自定义类继承Partitioner，重写getPartition()方法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text key, FlowBean value, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line"><span class="comment">// 控制分区代码逻辑</span></span><br><span class="line">… …</span><br><span class="line"><span class="keyword">return</span> partition;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-在Job驱动中，设置自定义Partitioner"><a href="#3-2-在Job驱动中，设置自定义Partitioner" class="headerlink" title="3.2 在Job驱动中，设置自定义Partitioner"></a>3.2 在Job驱动中，设置自定义Partitioner</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure><h2 id="3-3-自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask"><a href="#3-3-自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask" class="headerlink" title="3.3 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask"></a>3.3 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h1 id="四：分区总结"><a href="#四：分区总结" class="headerlink" title="四：分区总结"></a>四：分区总结</h1><p>（1）如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；</p><p>（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；</p><p>（3）如 果ReduceTask的数量&#x3D;1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个 ReduceTask，最终也就只会产生一个结果文件 part-r-00000；</p><p>（4）分区号必须从零开始，逐一累加。</p><h1 id="五：案例分析"><a href="#五：案例分析" class="headerlink" title="五：案例分析"></a>五：案例分析</h1><ul><li>例如：假设自定义分区数为5，则 <ul><li>（1）job.setNumReduceTasks(1); </li><li>（2）job.setNumReduceTasks(2); </li><li>（3）job.setNumReduceTasks(6); 会正常运行，只不过会产生一个输出文件 会报错 大于5，程序会正常运行，会产生空文件</li></ul></li></ul><h1 id="六：Partition-分区案例实操"><a href="#六：Partition-分区案例实操" class="headerlink" title="六：Partition 分区案例实操"></a>六：Partition 分区案例实操</h1><h2 id="6-1-需求"><a href="#6-1-需求" class="headerlink" title="6.1 需求"></a>6.1 需求</h2><p>将统计结果按照手机归属地不同省份输出到不同文件中（分区）；</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>phone_data .txt</li></ul><h3 id="（2）期望输出数据"><a href="#（2）期望输出数据" class="headerlink" title="（2）期望输出数据"></a>（2）期望输出数据</h3><ul><li>手机号 136、137、138、139 开头都分别放到一个独立的 4 个文件中，其他开头的放到 一个文件中。</li></ul><h2 id="6-2-需求分析"><a href="#6-2-需求分析" class="headerlink" title="6.2 需求分析"></a>6.2 需求分析</h2><p><img src="/blog/ee4a420a.html/image-20221218171052800.png"></p><h2 id="6-3-代码实现"><a href="#6-3-代码实现" class="headerlink" title="6.3 代码实现"></a>6.3 代码实现</h2><h3 id="（1）增加分区类"><a href="#（1）增加分区类" class="headerlink" title="（1）增加分区类"></a>（1）增加分区类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:17 2022/12/18</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="type">int</span> i)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取手机号前三位 prePhone</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">prePhone</span> <span class="operator">=</span> phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义一个分区号变量partition,根据prePhone设置分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span> (prePhone) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;136&quot;</span>:</span><br><span class="line">                partition = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;137&quot;</span>:</span><br><span class="line">                partition = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;138&quot;</span>:</span><br><span class="line">                partition = <span class="number">2</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;139&quot;</span>:</span><br><span class="line">                partition = <span class="number">3</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                partition = <span class="number">4</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最后返回分区号 partition</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）在驱动函数中增加自定义数据分区设置和-ReduceTask-设置"><a href="#（2）在驱动函数中增加自定义数据分区设置和-ReduceTask-设置" class="headerlink" title="（2）在驱动函数中增加自定义数据分区设置和 ReduceTask 设置"></a>（2）在驱动函数中增加自定义数据分区设置和 ReduceTask 设置</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定自定义分区</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"><span class="comment">// 同时指定相应数量的 ReduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="（3）驱动器类"><a href="#（3）驱动器类" class="headerlink" title="（3）驱动器类"></a>（3）驱动器类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:14 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取job对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.关联 driver mapper reducer</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.设置 map 端输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.设置程序最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定自定义分区</span></span><br><span class="line">        job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 同时指定相应数量的 ReduceTask</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.设置程序输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\phone_data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output1234\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6.提交job</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）Mapper-类"><a href="#（4）Mapper-类" class="headerlink" title="（4）Mapper 类"></a>（4）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:59 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割数据</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.抓取手机号、上行流量、下行流量</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> split[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">upFlow</span> <span class="operator">=</span> split[split.length - <span class="number">3</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">downFlow</span> <span class="operator">=</span> split[split.length - <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.封装 outK outV</span></span><br><span class="line">        outK.set(phone);</span><br><span class="line">        outV.setUpFlow(Long.parseLong(upFlow));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(downFlow));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.写出 outK outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）Reducer-类"><a href="#（5）Reducer-类" class="headerlink" title="（5）Reducer 类"></a>（5）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:07 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, FlowBean, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">long</span> <span class="variable">totalUpFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">totalDownFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.遍历values,将其中的上行流量和下行流量分别累计</span></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">            totalUpFlow += value.getUpFlow();</span><br><span class="line">            totalDownFlow += value.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.封装 outKV</span></span><br><span class="line">        outV.setUpFlow(totalUpFlow);</span><br><span class="line">        outV.setDownFlow(totalDownFlow);</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.写出 outK outV</span></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（6）实体类"><a href="#（6）实体类" class="headerlink" title="（6）实体类"></a>（6）实体类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:40 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：问题引出&quot;&gt;&lt;a href=&quot;#一：问题引出&quot; class=&quot;headerlink&quot; title=&quot;一：问题引出&quot;&gt;&lt;/a&gt;一：问题引出&lt;/h1&gt;&lt;p&gt;要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机 归属地不同省份输出到不同文件中（分</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce Shuffle机制</title>
    <link href="https://aiyingke.cn/blog/6b6718d7.html/"/>
    <id>https://aiyingke.cn/blog/6b6718d7.html/</id>
    <published>2022-12-18T08:51:03.000Z</published>
    <updated>2022-12-18T08:59:03.171Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Shuffle-机制"><a href="#一：Shuffle-机制" class="headerlink" title="一：Shuffle 机制"></a>一：Shuffle 机制</h1><ul><li>Map 方法之后，Reduce 方法之前的数据处理过程称之为 Shuffle。</li></ul><p><img src="/blog/6b6718d7.html/image-20221218165425041.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Shuffle-机制&quot;&gt;&lt;a href=&quot;#一：Shuffle-机制&quot; class=&quot;headerlink&quot; title=&quot;一：Shuffle 机制&quot;&gt;&lt;/a&gt;一：Shuffle 机制&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Map 方法之后，Reduce 方法之前的数据处</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce 工作流程</title>
    <link href="https://aiyingke.cn/blog/d1b2c594.html/"/>
    <id>https://aiyingke.cn/blog/d1b2c594.html/</id>
    <published>2022-12-18T07:31:04.000Z</published>
    <updated>2022-12-18T07:45:45.573Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：MapReduce详细工作流程图"><a href="#一：MapReduce详细工作流程图" class="headerlink" title="一：MapReduce详细工作流程图"></a>一：MapReduce详细工作流程图</h1><p><img src="/blog/d1b2c594.html/image-20221218153651886.png"></p><p><img src="/blog/d1b2c594.html/image-20221218154214498.png"></p><p>上面的流程是整个 MapReduce 最全工作流程，但是 Shuffle 过程只是从第 7 步开始到第 16 步结束，具体 Shuffle 过程详解，如下：</p><p>（1）MapTask 收集我们的 map()方法输出的 kv 对，放到内存缓冲区中</p><p>（2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件</p><p>（3）多个溢出文件会被合并成大的溢出文件</p><p>（4）在溢出过程及合并的过程中，都要调用 Partitioner 进行分区和针对 key 进行排序</p><p>（5）ReduceTask 根据自己的分区号，去各个 MapTask 机器上取相应的结果分区数据</p><p>（6）ReduceTask 会抓取到同一个分区的来自不同 MapTask 的结果文件，ReduceTask 会将这些文件再进行合并（归并排序）</p><p>（7）合并成大文件后，Shuffle 的过程也就结束了，后面进入 ReduceTask 的逻辑运算过程（从文件中取出一个一个的键值对 Group，调用用户自定义的 reduce()方法）</p><p><strong>注意：</strong> </p><ul><li>（1）Shuffle 中的缓冲区大小会影响到 MapReduce 程序的执行效率，原则上说，缓冲区 越大，磁盘 io 的次数越少，执行速度就越快。 </li><li>（2）缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认 100M。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：MapReduce详细工作流程图&quot;&gt;&lt;a href=&quot;#一：MapReduce详细工作流程图&quot; class=&quot;headerlink&quot; title=&quot;一：MapReduce详细工作流程图&quot;&gt;&lt;/a&gt;一：MapReduce详细工作流程图&lt;/h1&gt;&lt;p&gt;&lt;img s</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce 切片原理</title>
    <link href="https://aiyingke.cn/blog/f5d61b06.html/"/>
    <id>https://aiyingke.cn/blog/f5d61b06.html/</id>
    <published>2022-12-18T05:31:55.000Z</published>
    <updated>2022-12-18T06:56:09.638Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：MapReduce-框架原理"><a href="#一：MapReduce-框架原理" class="headerlink" title="一：MapReduce 框架原理"></a>一：MapReduce 框架原理</h1><p><img src="/blog/f5d61b06.html/image-20221218140443270.png"></p><h1 id="二：切片与-MapTask-并行度决定机制"><a href="#二：切片与-MapTask-并行度决定机制" class="headerlink" title="二：切片与 MapTask 并行度决定机制"></a>二：切片与 MapTask 并行度决定机制</h1><h2 id="2-1-问题引出"><a href="#2-1-问题引出" class="headerlink" title="2.1 问题引出"></a>2.1 问题引出</h2><ul><li>MapTask 的并行度决定 Map 阶段的任务处理并发度，进而影响到整个 Job 的处理速度。</li><li>1G 的数据，启动 8 个 MapTask，可以提高集群的并发处理能力。那么 1K 的数 据，也启动 8 个 MapTask，会提高集群性能吗？MapTask 并行任务是否越多越好呢？哪些因 素影响了 MapTask 并行度？</li></ul><h2 id="2-2-MapTask-并行度决定机制"><a href="#2-2-MapTask-并行度决定机制" class="headerlink" title="2.2 MapTask 并行度决定机制"></a>2.2 MapTask 并行度决定机制</h2><ul><li>数据块：Block 是 HDFS 物理上把数据分成一块一块。数据块是 HDFS 存储数据单位。</li><li>数据切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行 存储。数据切片是 MapReduce 程序计算输入数据的单位，一个切片会对应启动一个 MapTask。</li></ul><h2 id="2-3-数据切片与MapTask并行度决定机制"><a href="#2-3-数据切片与MapTask并行度决定机制" class="headerlink" title="2.3 数据切片与MapTask并行度决定机制"></a>2.3 数据切片与MapTask并行度决定机制</h2><p><img src="/blog/f5d61b06.html/image-20221218140847097.png"></p><h1 id="三：Job-提交流程源码和切片源码详解"><a href="#三：Job-提交流程源码和切片源码详解" class="headerlink" title="三：Job 提交流程源码和切片源码详解"></a>三：Job 提交流程源码和切片源码详解</h1><h2 id="3-1-Job-提交流程源码详解"><a href="#3-1-Job-提交流程源码详解" class="headerlink" title="3.1 Job 提交流程源码详解"></a>3.1 Job 提交流程源码详解</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">waitForCompletion()</span><br><span class="line">submit();</span><br><span class="line"><span class="comment">// 1 建立连接</span></span><br><span class="line">connect();</span><br><span class="line"><span class="comment">// 1）创建提交 Job 的代理</span></span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Cluster</span>(getConfiguration());</span><br><span class="line"><span class="comment">// （1）判断是本地运行环境还是 yarn 集群运行环境</span></span><br><span class="line">initialize(jobTrackAddr, conf); </span><br><span class="line"><span class="comment">// 2 提交 job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="built_in">this</span>, cluster)</span><br><span class="line"><span class="comment">// 1）创建给集群提交数据的 Stag 路径</span></span><br><span class="line"><span class="type">Path</span> <span class="variable">jobStagingArea</span> <span class="operator">=</span> JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"><span class="comment">// 2）获取 jobid ，并创建 Job 路径</span></span><br><span class="line"><span class="type">JobID</span> <span class="variable">jobId</span> <span class="operator">=</span> submitClient.getNewJobID();</span><br><span class="line"><span class="comment">// 3）拷贝 jar 包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);</span><br><span class="line"><span class="comment">// 5）向 Stag 路径写 XML 配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"><span class="comment">// 6）提交 Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(),</span><br><span class="line">waitForCompletion()</span><br><span class="line">submit();</span><br><span class="line"><span class="comment">// 1 建立连接</span></span><br><span class="line">connect();</span><br><span class="line"><span class="comment">// 1）创建提交 Job 的代理</span></span><br><span class="line"><span class="keyword">new</span> <span class="title class_">Cluster</span>(getConfiguration());</span><br><span class="line"><span class="comment">// （1）判断是本地运行环境还是 yarn 集群运行环境</span></span><br><span class="line">initialize(jobTrackAddr, conf); </span><br><span class="line"><span class="comment">// 2 提交 job</span></span><br><span class="line">submitter.submitJobInternal(Job.<span class="built_in">this</span>, cluster)</span><br><span class="line"><span class="comment">// 1）创建给集群提交数据的 Stag 路径</span></span><br><span class="line"><span class="type">Path</span> <span class="variable">jobStagingArea</span> <span class="operator">=</span> JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line"><span class="comment">// 2）获取 jobid ，并创建 Job 路径</span></span><br><span class="line"><span class="type">JobID</span> <span class="variable">jobId</span> <span class="operator">=</span> submitClient.getNewJobID();</span><br><span class="line"><span class="comment">// 3）拷贝 jar 包到集群</span></span><br><span class="line">copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line">rUploader.uploadFiles(job, jobSubmitDir);</span><br><span class="line"><span class="comment">// 4）计算切片，生成切片规划文件</span></span><br><span class="line">writeSplits(job, submitJobDir);</span><br><span class="line">maps = writeNewSplits(job, jobSubmitDir);</span><br><span class="line">input.getSplits(job);</span><br><span class="line"><span class="comment">// 5）向 Stag 路径写 XML 配置文件</span></span><br><span class="line">writeConf(conf, submitJobFile);</span><br><span class="line">conf.writeXml(out);</span><br><span class="line"><span class="comment">// 6）提交 Job,返回提交状态</span></span><br><span class="line">status = submitClient.submitJob(jobId, submitJobDir.toString(),</span><br></pre></td></tr></table></figure><h2 id="3-2-Job提交流程源码解析"><a href="#3-2-Job提交流程源码解析" class="headerlink" title="3.2 Job提交流程源码解析"></a>3.2 Job提交流程源码解析</h2><p><img src="/blog/f5d61b06.html/image-20221218141342346.png"></p><h2 id="3-3-FileInputFormat-切片源码解析（input-getSplits-job-）"><a href="#3-3-FileInputFormat-切片源码解析（input-getSplits-job-）" class="headerlink" title="3.3 FileInputFormat 切片源码解析（input.getSplits(job)）"></a>3.3 FileInputFormat 切片源码解析（input.getSplits(job)）</h2><ul><li>（1）程序先找到你数据存储的目录。</li><li>（2）开始遍历处理（规划切片）目录下的每一个文件 </li><li>（3）遍历第一个文件data1.txt <ul><li>a）获取文件大小fs.sizeOf(data.txt) </li><li>b）计算切片大小 computeSplitSize(Math.max(minSize,Math.min(maxSize,blocksize)))&#x3D;blocksize&#x3D;128M </li><li>c）默认情况下，切片大小&#x3D;blocksize </li><li>d）开始切，形成第1个切片：data.txt—0:128M 第2个切片data.txt—128:256M 第3个切片data.txt—256M:300M （每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） </li><li>e）将切片信息写到一个切片规划文件中 </li><li>f）整个切片的核心过程在getSplit()方法中完成 </li><li>g）InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等。</li></ul></li><li>（4）提交切片规划文件到YARN上，YARN上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数。</li></ul><h1 id="四：FileInputFormat-切片机制"><a href="#四：FileInputFormat-切片机制" class="headerlink" title="四：FileInputFormat 切片机制"></a>四：FileInputFormat 切片机制</h1><h2 id="4-1-FileInputFormat切片机制"><a href="#4-1-FileInputFormat切片机制" class="headerlink" title="4.1 FileInputFormat切片机制"></a>4.1 FileInputFormat切片机制</h2><p>（1）简单地按照文件的内容长度进行切片</p><p>（2）切片大小，默认等于Block大小</p><p>（3）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片</p><h2 id="4-2-案例分析"><a href="#4-2-案例分析" class="headerlink" title="4.2 案例分析"></a>4.2 案例分析</h2><p>（1）输入数据有两个文件：</p><ul><li>file1.txt 320M </li><li>file2.txt 10M</li></ul><p>（2）经过FileInputFormat的切片机制 运算后，形成的切片信息如下：</p><ul><li>file1.txt.split1– 0~128</li><li>file1.txt.split2– 128~256</li><li>file1.txt.split3– 256~320</li><li>file2.txt.split1– 0~10M</li></ul><h2 id="4-3-FileInputFormat切片大小的参数配置"><a href="#4-3-FileInputFormat切片大小的参数配置" class="headerlink" title="4.3 FileInputFormat切片大小的参数配置"></a>4.3 FileInputFormat切片大小的参数配置</h2><h3 id="（1）源码中计算切片大小的公式"><a href="#（1）源码中计算切片大小的公式" class="headerlink" title="（1）源码中计算切片大小的公式"></a>（1）源码中计算切片大小的公式</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Math.max(minSize, Math.min(maxSize, blockSize));</span><br><span class="line">mapreduce.input.fileinputformat.split.minsize=<span class="number">1</span> 默认值为<span class="number">1</span></span><br><span class="line">mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue</span><br><span class="line">因此，默认情况下，切片大小=blocksize。</span><br></pre></td></tr></table></figure><h3 id="（2）切片大小设置"><a href="#（2）切片大小设置" class="headerlink" title="（2）切片大小设置"></a>（2）切片大小设置</h3><ul><li>maxsize（切片最大值）：参数如果调得比blockSize小，则会让切片变小，而且就等于配置的这个参数的值。 </li><li>minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blockSize还大。</li></ul><h3 id="（3）获取切片信息API"><a href="#（3）获取切片信息API" class="headerlink" title="（3）获取切片信息API"></a>（3）获取切片信息API</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取切片的文件名称</span></span><br><span class="line"><span class="type">String</span> <span class="variable">name</span> <span class="operator">=</span> inputSplit.getPath().getName();</span><br><span class="line"><span class="comment">// 根据文件类型获取切片信息</span></span><br><span class="line"><span class="type">FileSplit</span> <span class="variable">inputSplit</span> <span class="operator">=</span> (FileSplit) context.getInputSplit();</span><br></pre></td></tr></table></figure><h1 id="五：TextInputFormat"><a href="#五：TextInputFormat" class="headerlink" title="五：TextInputFormat"></a>五：TextInputFormat</h1><h2 id="5-1-FileInputFormat-实现类"><a href="#5-1-FileInputFormat-实现类" class="headerlink" title="5.1 FileInputFormat 实现类"></a>5.1 FileInputFormat 实现类</h2><ul><li>思考：在运行 MapReduce 程序时，输入的文件格式包括：基于行的日志文件、二进制 格式文件、数据库表等。那么，针对不同的数据类型，MapReduce 是如何读取这些数据的呢？</li><li>FileInputFormat 常见的接口实现类包括：TextInputFormat、KeyValueTextInputFormat、 NLineInputFormat、CombineTextInputFormat 和自定义 InputFormat 等。</li></ul><h2 id="5-2-TextInputFormat"><a href="#5-2-TextInputFormat" class="headerlink" title="5.2 TextInputFormat"></a>5.2 TextInputFormat</h2><ul><li><p>TextInputFormat 是默认的 FileInputFormat 实现类。按行读取每条记录。键是存储该行在整个文件中的起始字节偏移量， LongWritable 类型。值是这行的内容，不包括任何行终止 符（换行符和回车符），Text 类型。</p></li><li><p>以下是一个示例，比如，一个分片包含了如下 4 条文本记录。</p></li><li><p>&#96;&#96;&#96;java<br>Rich learning form<br>Intelligent learning engine<br>Learning more convenient<br>From the real demand for more close to the enterprise</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 每条记录表示为以下键/值对：</span><br><span class="line"></span><br><span class="line">- ```java</span><br><span class="line">  (0,Rich learning form)</span><br><span class="line">  (20,Intelligent learning engine)</span><br><span class="line">  (49,Learning more convenient)</span><br><span class="line">  (74,From the real demand for more close to the enterprise)</span><br></pre></td></tr></table></figure></li></ul><h1 id="六：CombineTextInputFormat-切片机制"><a href="#六：CombineTextInputFormat-切片机制" class="headerlink" title="六：CombineTextInputFormat 切片机制"></a>六：CombineTextInputFormat 切片机制</h1><p>框架默认的 TextInputFormat 切片机制是对任务按文件规划切片，不管文件多小，都会 是一个单独的切片，都会交给一个 MapTask，这样如果有大量小文件，就会产生大量的 MapTask，处理效率极其低下。</p><h2 id="6-1-应用场景"><a href="#6-1-应用场景" class="headerlink" title="6.1 应用场景"></a>6.1 应用场景</h2><p>CombineTextInputFormat 用于小文件过多的场景，它可以将多个小文件从逻辑上规划到 一个切片中，这样，多个小文件就可以交给一个 MapTask 处理。</p><h2 id="6-2-虚拟存储切片最大值设置"><a href="#6-2-虚拟存储切片最大值设置" class="headerlink" title="6.2 虚拟存储切片最大值设置"></a>6.2 虚拟存储切片最大值设置</h2><ul><li>CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);&#x2F;&#x2F; 4m </li><li>注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。</li></ul><h2 id="6-3-切片机制"><a href="#6-3-切片机制" class="headerlink" title="6.3 切片机制"></a>6.3 切片机制</h2><p>生成切片过程包括：虚拟存储过程和切片过程二部分。</p><p>CombineTextInputFormat切片机制：</p><p><img src="/blog/f5d61b06.html/image-20221218143247678.png"></p><h3 id="（1）虚拟存储过程"><a href="#（1）虚拟存储过程" class="headerlink" title="（1）虚拟存储过程"></a>（1）虚拟存储过程</h3><ul><li>将输入目录下所有文件大小，依次和设置的 setMaxInputSplitSize 值比较，如果不大于设置的最大值，逻辑上划分一个块。</li><li>如果输入文件大于设置的最大值且大于两倍， 那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时 将文件均分成 2 个虚拟存储块（防止出现太小切片）。 <ul><li>例如 setMaxInputSplitSize 值为 4M，输入文件大小为 8.02M，则先逻辑上分成一个 4M。剩余的大小为 4.02M，如果按照 4M 逻辑划分，就会出现 0.02M 的小的虚拟存储 文件，所以将剩余的 4.02M 文件切分成（2.01M 和 2.01M）两个文件。</li></ul></li></ul><h3 id="（2）切片过程"><a href="#（2）切片过程" class="headerlink" title="（2）切片过程"></a>（2）切片过程</h3><ul><li>判断虚拟存储的文件大小是否大于 setMaxInputSplitSize 值，大于等于则单独 形成一个切片。</li><li>如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。</li><li>测试举例<ul><li>有 4 个小文件大小分别为 1.7M、5.1M、3.4M 以及 6.8M 这四个小 文件，则虚拟存储之后形成 6 个文件块，大小分别为：<ul><li>1.7M，（2.55M、2.55M）、3.4M 以及（3.4M、3.4M）</li></ul></li><li>最终会形成 3 个切片，大小分别为：<ul><li>（1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M</li></ul></li></ul></li></ul><h1 id="七：CombineTextInputFormat-案例实操"><a href="#七：CombineTextInputFormat-案例实操" class="headerlink" title="七：CombineTextInputFormat 案例实操"></a>七：CombineTextInputFormat 案例实操</h1><h2 id="7-1-需求"><a href="#7-1-需求" class="headerlink" title="7.1 需求"></a>7.1 需求</h2><p>将输入的大量小文件合并成一个切片统一处理。</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>准备 4 个小文件</li></ul><h3 id="（2）期望"><a href="#（2）期望" class="headerlink" title="（2）期望"></a>（2）期望</h3><ul><li>期望一个切片处理 4 个文件</li></ul><h2 id="7-2-需求分析"><a href="#7-2-需求分析" class="headerlink" title="7.2 需求分析"></a>7.2 需求分析</h2><p>通过设置切片方式为 CombineTextInputFormat，并设置切片大小，将多个小文件进行合并，然后处理。</p><h2 id="7-3-代码实现"><a href="#7-3-代码实现" class="headerlink" title="7.3 代码实现"></a>7.3 代码实现</h2><p>（1）不做任何处理，采用默认的 FileInputFormat切片方式，观察切片个数为 4。</p><ul><li>通过日志观察到：number of splits:4</li></ul><p>（2）设置切片方式为 CombineTextInputFormat </p><ul><li><p>在驱动器中添加如下代码</p></li><li><pre><code class="java">// 如果不设置 InputFormat，它默认用的是 TextInputFormat.classjob.setInputFormatClass(CombineTextInputFormat.class);//虚拟存储切片最大值设置 4mCombineTextInputFormat.setMaxInputSplitSize(job, 4194304);</code></pre></li><li><p>此时观察日志：number of splits:1</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：MapReduce-框架原理&quot;&gt;&lt;a href=&quot;#一：MapReduce-框架原理&quot; class=&quot;headerlink&quot; title=&quot;一：MapReduce 框架原理&quot;&gt;&lt;/a&gt;一：MapReduce 框架原理&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blo</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 序列化</title>
    <link href="https://aiyingke.cn/blog/926ade3a.html/"/>
    <id>https://aiyingke.cn/blog/926ade3a.html/</id>
    <published>2022-12-17T13:22:40.000Z</published>
    <updated>2022-12-17T14:34:36.453Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：序列化概述"><a href="#一：序列化概述" class="headerlink" title="一：序列化概述"></a>一：序列化概述</h1><h2 id="1-1-什么是序列化？"><a href="#1-1-什么是序列化？" class="headerlink" title="1.1 什么是序列化？"></a>1.1 什么是序列化？</h2><ul><li>序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储到磁 盘（持久化）和网络传输。</li><li>反序列化就是将收到字节序列（或其他数据传输协议）或者是磁盘的持久化数据，转换 成内存中的对象。</li></ul><h2 id="1-2-为什么要序列化？"><a href="#1-2-为什么要序列化？" class="headerlink" title="1.2 为什么要序列化？"></a>1.2 为什么要序列化？</h2><p>一般来说，“活的”对象只生存在内存里，关机断电就没有了。而且“活的”对象只能 由本地的进程使用，不能被发送到网络上的另外一台计算机。 然而序列化可以存储“活的” 对象，可以将“活的”对象发送到远程计算机。</p><h2 id="1-3-为什么不用-Java-的序列化？"><a href="#1-3-为什么不用-Java-的序列化？" class="headerlink" title="1.3 为什么不用 Java 的序列化？"></a>1.3 为什么不用 Java 的序列化？</h2><p>Java 的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带 很多额外的信息（各种校验信息，Header，继承体系等），不便于在网络中高效传输。所以， Hadoop 自己开发了一套序列化机制（Writable）。</p><h2 id="1-4-Hadoop-序列化特点"><a href="#1-4-Hadoop-序列化特点" class="headerlink" title="1.4 Hadoop 序列化特点"></a>1.4 Hadoop 序列化特点</h2><p>（1）紧凑 ：高效使用存储空间。</p><p>（2）快速：读写数据的额外开销小。</p><p>（3）互操作：支持多语言的交互。</p><h1 id="二：自定义-bean-对象实现序列化接口（Writable）"><a href="#二：自定义-bean-对象实现序列化接口（Writable）" class="headerlink" title="二：自定义 bean 对象实现序列化接口（Writable）"></a>二：自定义 bean 对象实现序列化接口（Writable）</h1><p>在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在 Hadoop 框架内部 传递一个 bean 对象，那么该对象就需要实现序列化接口。</p><p>具体实现 bean 对象序列化步骤如下 7 步：</p><h2 id="2-1-必须实现-Writable-接口"><a href="#2-1-必须实现-Writable-接口" class="headerlink" title="2.1 必须实现 Writable 接口"></a>2.1 必须实现 Writable 接口</h2><h2 id="2-2-反序列化时，需要反射调用空参构造函数，所以必须有空参构造"><a href="#2-2-反序列化时，需要反射调用空参构造函数，所以必须有空参构造" class="headerlink" title="2.2 反序列化时，需要反射调用空参构造函数，所以必须有空参构造"></a>2.2 反序列化时，需要反射调用空参构造函数，所以必须有空参构造</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line"><span class="built_in">super</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-3-重写序列化方法"><a href="#2-3-重写序列化方法" class="headerlink" title="2.3 重写序列化方法"></a>2.3 重写序列化方法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">out.writeLong(upFlow);</span><br><span class="line">out.writeLong(downFlow);</span><br><span class="line">out.writeLong(sumFlow);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-4-重写反序列化方法"><a href="#2-4-重写反序列化方法" class="headerlink" title="2.4 重写反序列化方法"></a>2.4 重写反序列化方法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">upFlow = in.readLong();</span><br><span class="line">downFlow = in.readLong();</span><br><span class="line">sumFlow = in.readLong();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-5-注意反序列化的顺序和序列化的顺序完全一致"><a href="#2-5-注意反序列化的顺序和序列化的顺序完全一致" class="headerlink" title="2.5 注意反序列化的顺序和序列化的顺序完全一致"></a>2.5 注意反序列化的顺序和序列化的顺序完全一致</h2><h2 id="2-6-要想把结果显示在文件中，需要重写-toString-，可用”-t”分开，方便后续用。"><a href="#2-6-要想把结果显示在文件中，需要重写-toString-，可用”-t”分开，方便后续用。" class="headerlink" title="2.6 要想把结果显示在文件中，需要重写 toString()，可用”\t”分开，方便后续用。"></a>2.6 要想把结果显示在文件中，需要重写 toString()，可用”\t”分开，方便后续用。</h2><h2 id="2-7-如果需要将自定义的-bean-放在-key-中传输，则还需要实现-Comparable-接口，因为-MapReduce-框中的-Shuffle-过程要求对-key-必须能排序。"><a href="#2-7-如果需要将自定义的-bean-放在-key-中传输，则还需要实现-Comparable-接口，因为-MapReduce-框中的-Shuffle-过程要求对-key-必须能排序。" class="headerlink" title="2.7 如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口，因为 MapReduce 框中的 Shuffle 过程要求对 key 必须能排序。"></a>2.7 如果需要将自定义的 bean 放在 key 中传输，则还需要实现 Comparable 接口，因为 MapReduce 框中的 Shuffle 过程要求对 key 必须能排序。</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line"><span class="comment">// 倒序排列，从大到小</span></span><br><span class="line"><span class="keyword">return</span> <span class="built_in">this</span>.sumFlow &gt; o.getSumFlow() ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="三：序列化案例实操"><a href="#三：序列化案例实操" class="headerlink" title="三：序列化案例实操"></a>三：序列化案例实操</h1><h2 id="3-1-需求"><a href="#3-1-需求" class="headerlink" title="3.1 需求"></a>3.1 需求</h2><p>统计每一个手机号耗费的总上行流量、总下行流量、总流量</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><p>phone_data.txt</p><h3 id="（2）输入数据格式"><a href="#（2）输入数据格式" class="headerlink" title="（2）输入数据格式"></a>（2）输入数据格式</h3><p><img src="/blog/926ade3a.html/image-20221217213353463.png"></p><h3 id="（3）期望输出数据格式"><a href="#（3）期望输出数据格式" class="headerlink" title="（3）期望输出数据格式"></a>（3）期望输出数据格式</h3><p><img src="/blog/926ade3a.html/image-20221217213418569.png"></p><h2 id="3-2-需求分析"><a href="#3-2-需求分析" class="headerlink" title="3.2 需求分析"></a>3.2 需求分析</h2><p><img src="/blog/926ade3a.html/image-20221217213615452.png"></p><h2 id="3-3-编写-MapReduce-程序"><a href="#3-3-编写-MapReduce-程序" class="headerlink" title="3.3 编写 MapReduce 程序"></a>3.3 编写 MapReduce 程序</h2><h3 id="（1）编写流量统计的-Bean-对象"><a href="#（1）编写流量统计的-Bean-对象" class="headerlink" title="（1）编写流量统计的 Bean 对象"></a>（1）编写流量统计的 Bean 对象</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:40 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-Mapper-类"><a href="#（2）编写-Mapper-类" class="headerlink" title="（2）编写 Mapper 类"></a>（2）编写 Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:59 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割数据</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.抓取手机号、上行流量、下行流量</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> split[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">upFlow</span> <span class="operator">=</span> split[split.length - <span class="number">3</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">downFlow</span> <span class="operator">=</span> split[split.length - <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.封装 outK outV</span></span><br><span class="line">        outK.set(phone);</span><br><span class="line">        outV.setUpFlow(Long.parseLong(upFlow));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(downFlow));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.写出 outK outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）编写-Reducer-类"><a href="#（3）编写-Reducer-类" class="headerlink" title="（3）编写 Reducer 类"></a>（3）编写 Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:07 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, FlowBean, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">long</span> <span class="variable">totalUpFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">totalDownFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.遍历values,将其中的上行流量和下行流量分别累计</span></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">            totalUpFlow += value.getUpFlow();</span><br><span class="line">            totalDownFlow += value.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.封装 outKV</span></span><br><span class="line">        outV.setUpFlow(totalUpFlow);</span><br><span class="line">        outV.setDownFlow(totalDownFlow);</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.写出 outK outV</span></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）编写-Driver-驱动类"><a href="#（4）编写-Driver-驱动类" class="headerlink" title="（4）编写 Driver 驱动类"></a>（4）编写 Driver 驱动类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:14 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取job对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.关联 driver mapper reducer</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.设置 map 端输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.设置程序最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.设置程序输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\phone_data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output2\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6.提交job</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：序列化概述&quot;&gt;&lt;a href=&quot;#一：序列化概述&quot; class=&quot;headerlink&quot; title=&quot;一：序列化概述&quot;&gt;&lt;/a&gt;一：序列化概述&lt;/h1&gt;&lt;h2 id=&quot;1-1-什么是序列化？&quot;&gt;&lt;a href=&quot;#1-1-什么是序列化？&quot; class=&quot;he</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce WordCount案例</title>
    <link href="https://aiyingke.cn/blog/5bb35ccf.html/"/>
    <id>https://aiyingke.cn/blog/5bb35ccf.html/</id>
    <published>2022-12-16T15:18:02.000Z</published>
    <updated>2022-12-16T16:27:47.571Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：需求"><a href="#一：需求" class="headerlink" title="一：需求"></a>一：需求</h1><p>对本地 WordCount.txt 文件进行词频统计，其内容均按 “,”分割，将统计结果输出到 outputDir 目录下；</p><h1 id="二：代码实现"><a href="#二：代码实现" class="headerlink" title="二：代码实现"></a>二：代码实现</h1><h2 id="2-1-环境引入"><a href="#2-1-环境引入" class="headerlink" title="2.1 环境引入"></a>2.1 环境引入</h2><h3 id="（1）Maven-依赖"><a href="#（1）Maven-依赖" class="headerlink" title="（1）Maven 依赖"></a>（1）Maven 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.3.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>junit<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>4.13.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.0.5<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="（2）配置日志"><a href="#（2）配置日志" class="headerlink" title="（2）配置日志"></a>（2）配置日志</h3><p>在项目的 src&#x2F;main&#x2F;resources 目录下，新建一个文件，命名为“log4j.properties”，在 文件中填入。</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log4j.rootLogger</span>=<span class="string">INFO, stdout</span></span><br><span class="line"><span class="attr">log4j.appender.stdout</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.stdout.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br><span class="line"><span class="attr">log4j.appender.logfile</span>=<span class="string">org.apache.log4j.FileAppender</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.File</span>=<span class="string">target/spring.log</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.logfile.layout.ConversionPattern</span>=<span class="string">%d %p [%c] - %m%n</span></span><br></pre></td></tr></table></figure><h2 id="2-2-编写代码"><a href="#2-2-编写代码" class="headerlink" title="2.2 编写代码"></a>2.2 编写代码</h2><p>创建包 cn.aiyingke.mapreduce.wordcount</p><h3 id="（1）Driver-类"><a href="#（1）Driver-类" class="headerlink" title="（1）Driver 类"></a>（1）Driver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> driver;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> mapper.WordCountMapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> reducer.WordCountReducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: Rupert Tears</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: Created in 18:17 2022/1/18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Function</span>: 在 hdfs 中的某一目录下，有一些列文件，内容均为 “,”号分割，</span></span><br><span class="line"><span class="comment"> * 统计出按 “,”分割的各个元素出现频次，输出到指定 hdfs目录中。</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 概述：词频统计工作</span></span><br><span class="line"><span class="comment"> * 驱动器类：</span></span><br><span class="line"><span class="comment"> * 1.拿到配置环境变量</span></span><br><span class="line"><span class="comment"> * 2.拿到job作业对象</span></span><br><span class="line"><span class="comment"> * 3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment"> * 4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment"> * 5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment"> * 6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment"> * 7.提交执行</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;S:\\Software\\IDEA 2021.1.2\\IDEA-workspace\\Rupert-Tears\\MapReduce\\WordCount\\data\\instanceData\\WordCount.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;S:\\Software\\IDEA 2021.1.2\\IDEA-workspace\\Rupert-Tears\\MapReduce\\WordCount\\data\\outputDir&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-类"><a href="#（2）Mapper-类" class="headerlink" title="（2）Mapper 类"></a>（2）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: Rupert Tears</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: Created in 18:22 2022/1/18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-类"><a href="#（3）Reducer-类" class="headerlink" title="（3）Reducer 类"></a>（3）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Author</span>: Rupert Tears</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Date</span>: Created in 18:23 2022/1/18</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span>: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-3-本地测试"><a href="#2-3-本地测试" class="headerlink" title="2.3 本地测试"></a>2.3 本地测试</h2><p>（1）需要首先配置好 HADOOP_HOME 变量以及 Windows 运行依赖</p><p>（2）在 IDEA上运行程序即可。</p><h1 id="三：集群测试"><a href="#三：集群测试" class="headerlink" title="三：集群测试"></a>三：集群测试</h1><h2 id="3-1-打包插件"><a href="#3-1-打包插件" class="headerlink" title="3.1 打包插件"></a>3.1 打包插件</h2><p>用 maven 打 jar 包，需要添加的打包插件依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.8.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="3-2-修改输入输出"><a href="#3-2-修改输入输出" class="headerlink" title="3.2 修改输入输出"></a>3.2 修改输入输出</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br></pre></td></tr></table></figure><h2 id="3-3-上传集群并执行命令"><a href="#3-3-上传集群并执行命令" class="headerlink" title="3.3 上传集群并执行命令"></a>3.3 上传集群并执行命令</h2><p>输入输出文件应为HDFS中的文件路径；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ghost@hadoop100 ~]$ hadoop jar /home/ghost/mapreduce/MapReduceDemo-1.0-SNAPSHOT.jar cn.aiyingke.mapreduce.wordcount.WordCountDriver /word.txt /output</span><br></pre></td></tr></table></figure><p>part-r-00000：其中包含 r 代表的是 reduce 的结果；</p><p><img src="/blog/5bb35ccf.html/image-20221217002553625.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：需求&quot;&gt;&lt;a href=&quot;#一：需求&quot; class=&quot;headerlink&quot; title=&quot;一：需求&quot;&gt;&lt;/a&gt;一：需求&lt;/h1&gt;&lt;p&gt;对本地 WordCount.txt 文件进行词频统计，其内容均按 “,”分割，将统计结果输出到 outputDir 目录下；</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
</feed>
