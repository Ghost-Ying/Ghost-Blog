<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>爱影客</title>
  
  
  <link href="https://aiyingke.cn/atom.xml" rel="self"/>
  
  <link href="https://aiyingke.cn/"/>
  <updated>2023-04-02T23:53:17.866Z</updated>
  <id>https://aiyingke.cn/</id>
  
  <author>
    <name>Rupert-Tears</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kafka 概述</title>
    <link href="https://aiyingke.cn/blog/daa0264.html/"/>
    <id>https://aiyingke.cn/blog/daa0264.html/</id>
    <published>2023-04-02T23:42:45.000Z</published>
    <updated>2023-04-02T23:53:17.866Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：定义"><a href="#一：定义" class="headerlink" title="一：定义"></a>一：定义</h1><p>Kafka 传统定义：Kafka 是一个分布式的基于发布&#x2F;订阅模式的消息队列，主要应用于大数据的实时处理场景。<br>发布&#x2F;订阅：消息的发布者不会将消息直接发送给消息的订阅者，而是将发送的消息分为不同的类别，订阅者只接受感兴趣的消息。<br>Kafka 愿景定义：Kafka 是一个开源的分布式事件流平台，被多数公司用于高性能数据管道、流分析、数据集成和关键任务应用。</p><h1 id="二：消息队列"><a href="#二：消息队列" class="headerlink" title="二：消息队列"></a>二：消息队列</h1><p>在大数据领域通常采用 Kafka 作为消息队列。</p><h2 id="2-1-传统消息队列的应用场景"><a href="#2-1-传统消息队列的应用场景" class="headerlink" title="2.1 传统消息队列的应用场景"></a>2.1 传统消息队列的应用场景</h2><p>传统的消息队列主要应用于：缓存&#x2F;消峰、解耦和异步通信。</p><h3 id="缓存-x2F-消峰："><a href="#缓存-x2F-消峰：" class="headerlink" title="缓存&#x2F;消峰："></a>缓存&#x2F;消峰：</h3><p>有助于控制和优化数据流系统的速度，解决生产消息和消费消息的处理速度不一致的情况。<br><img src="https://cdn.nlark.com/yuque/0/2023/png/12909154/1679881016297-cc8db537-950f-4240-9517-a85ad89396ce.png#averageHue=%23f6efec&clientId=u43690da4-c6c5-4&from=paste&height=462&id=u6a559842&name=image.png&originHeight=577&originWidth=1349&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=115175&status=done&style=none&taskId=u080e705d-4baf-4251-b1ef-b2e85789908&title=&width=1079.2" alt="image.png"></p><h3 id="解耦："><a href="#解耦：" class="headerlink" title="解耦："></a>解耦：</h3><p>允许独立的扩展或者修改两边的处理过程，只要确保他们遵循同样的数据接口约束。<br><img src="https://cdn.nlark.com/yuque/0/2023/png/12909154/1679880991435-815b293d-70f2-486c-aa25-e9bea76e39e8.png#averageHue=%23f9eee9&clientId=u43690da4-c6c5-4&from=paste&height=440&id=ud68cc76b&name=image.png&originHeight=550&originWidth=1258&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=105544&status=done&style=none&taskId=ucccf9f31-98c5-4aed-ba4f-3d8c2de2d4a&title=&width=1006.4" alt="image.png"></p><h3 id="异步通信："><a href="#异步通信：" class="headerlink" title="异步通信："></a>异步通信：</h3><p>允许用户把消息放入队列中，但不立即处理它，然后在需要的时候再处理它们。<br><img src="https://cdn.nlark.com/yuque/0/2023/png/12909154/1679880958125-8276fa66-7b49-46e8-b4e8-a00f8a6e2215.png#averageHue=%23f7eeea&clientId=u43690da4-c6c5-4&from=paste&height=473&id=u221a4081&name=image.png&originHeight=591&originWidth=1339&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=137927&status=done&style=none&taskId=ufcd580e4-2c27-454d-bb6d-b1b088f37e8&title=&width=1071.2" alt="image.png"></p><h2 id="2-2-消息队列的两种模式"><a href="#2-2-消息队列的两种模式" class="headerlink" title="2.2 消息队列的两种模式"></a>2.2 消息队列的两种模式</h2><h3 id="点对点模式："><a href="#点对点模式：" class="headerlink" title="点对点模式："></a>点对点模式：</h3><p>消费者主动拉去消息，收到消息后清除消息。<br><img src="https://cdn.nlark.com/yuque/0/2023/png/12909154/1679880880118-6d071538-715e-43e1-8833-321b8bedf223.png#averageHue=%23f8ece4&clientId=u43690da4-c6c5-4&from=paste&height=169&id=ud5f672f0&name=image.png&originHeight=211&originWidth=1164&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=26097&status=done&style=none&taskId=uce7f6009-e3f9-4dc9-bf9f-ac6faaa898f&title=&width=931.2" alt="image.png"></p><h3 id="发布-x2F-订阅模式："><a href="#发布-x2F-订阅模式：" class="headerlink" title="发布&#x2F;订阅模式："></a>发布&#x2F;订阅模式：</h3><ul><li>可以有多个topic主题；</li><li>消费者消费数据后，不删除数据；</li><li>每个消费者相互独立，都可以消费到数据。</li></ul><p><img src="https://cdn.nlark.com/yuque/0/2023/png/12909154/1679880924189-25c31db1-b9dc-454f-8af1-41fb4320ed53.png#averageHue=%23f7eae3&clientId=u43690da4-c6c5-4&from=paste&height=184&id=udfbd0a72&name=image.png&originHeight=230&originWidth=1164&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=55517&status=done&style=none&taskId=u5c9a3b28-4e8f-4064-a230-e456f994553&title=&width=931.2" alt="image.png"></p><h1 id="三：Kafka-基础架构"><a href="#三：Kafka-基础架构" class="headerlink" title="三：Kafka 基础架构"></a>三：Kafka 基础架构</h1><p><img src="https://cdn.nlark.com/yuque/0/2023/png/12909154/1679880712565-c1fe2e12-800c-489b-8fd6-34419a64adcb.png#averageHue=%23ecdbd6&clientId=u43690da4-c6c5-4&from=paste&height=525&id=u260ffdbb&name=image.png&originHeight=656&originWidth=1387&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=215380&status=done&style=none&taskId=u8358ded8-ddd8-47a0-a597-a18023014d1&title=&width=1109.6" alt="image.png"><br>（1）Producer：消息生产者，向 Kafka broker 发消息的客户端。<br>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。<br>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。<br>（4）Broker：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker 可以容纳多个 topic。<br>（5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic。<br>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker （服务器）上，一个 topic 可以分为多个 partition，每个 parition 是一个有序的队列。<br>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个 Follower。<br>（8）Leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。<br>（9）Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和 Leader 数据同步。Leader 发生故障时，某一个 Follower 会成为新的 Leader。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：定义&quot;&gt;&lt;a href=&quot;#一：定义&quot; class=&quot;headerlink&quot; title=&quot;一：定义&quot;&gt;&lt;/a&gt;一：定义&lt;/h1&gt;&lt;p&gt;Kafka 传统定义：Kafka 是一个分布式的基于发布&amp;#x2F;订阅模式的消息队列，主要应用于大数据的实时处理场景。&lt;b</summary>
      
    
    
    
    <category term="Kafka" scheme="https://aiyingke.cn/categories/Kafka/"/>
    
    
    <category term="Kafka" scheme="https://aiyingke.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>Spark 性能调优——Shuffle调优</title>
    <link href="https://aiyingke.cn/blog/60787c8.html/"/>
    <id>https://aiyingke.cn/blog/60787c8.html/</id>
    <published>2023-02-22T03:26:46.000Z</published>
    <updated>2023-02-22T03:29:40.474Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：ShuffleManager发展概述"><a href="#一：ShuffleManager发展概述" class="headerlink" title="一：ShuffleManager发展概述"></a>一：ShuffleManager发展概述</h1><p>在Spark的源码中，<strong>负责shuffle过程的执行、计算和处理的组件</strong>主要就是<strong>ShuffleManager</strong>，也即<strong>shuffle管理器</strong>。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</p><p>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会<strong>产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能</strong>。</p><p>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了<strong>SortShuffleManager</strong>。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</p><p>下面我们详细分析一下HashShuffleManager和SortShuffleManager的原理。</p><h1 id="二：HashShuffleManager运行原理"><a href="#二：HashShuffleManager运行原理" class="headerlink" title="二：HashShuffleManager运行原理"></a>二：HashShuffleManager运行原理</h1><h2 id="2-1-未经优化的HashShuffleManager"><a href="#2-1-未经优化的HashShuffleManager" class="headerlink" title="2.1 未经优化的HashShuffleManager"></a>2.1 未经优化的HashShuffleManager</h2><p>下图说明了未经优化的HashShuffleManager的原理。这里我们先明确一个假设前提：每个Executor只有1个CPU core，也就是说，无论这个Executor上分配多少个task线程，同一时间都只能执行一个task线程。</p><p>我们先从shuffle write开始说起。shuffle write阶段，主要就是在一个stage结束计算之后，为了下一个stage可以执行shuffle类的算子（比如reduceByKey），而将每个task处理的数据按key进行“分类”。所谓“分类”，就是对相同的key执行hash算法，从而将相同key都写入同一个磁盘文件中，而每一个磁盘文件都只属于下游stage的一个task。在将数据写入磁盘之前，会先将数据写入内存缓冲中，当内存缓冲填满之后，才会溢写到磁盘文件中去。</p><p>那么每个执行shuffle write的task，要为下一个stage创建多少个磁盘文件呢？很简单，下一个stage的task有多少个，当前stage的每个task就要创建多少份磁盘文件。比如下一个stage总共有100个task，那么当前stage的每个task都要创建100份磁盘文件。如果当前stage有50个task，总共有10个Executor，每个Executor执行5个Task，那么每个Executor上总共就要创建500个磁盘文件，所有Executor上会创建5000个磁盘文件。由此可见，未经优化的shuffle write操作所产生的磁盘文件的数量是极其惊人的。</p><p>接着我们来说说shuffle read。shuffle read，通常就是一个stage刚开始时要做的事情。此时该stage的每一个task就需要将上一个stage的计算结果中的所有相同key，从各个节点上通过网络都拉取到自己所在的节点上，然后进行key的聚合或连接等操作。由于shuffle write的过程中，task给下游stage的每个task都创建了一个磁盘文件，因此shuffle read的过程中，每个task只要从上游stage的所有task所在节点上，拉取属于自己的那一个磁盘文件即可。</p><p>shuffle read的拉取过程是一边拉取一边进行聚合的。每个shuffle read task都会有一个自己的buffer缓冲，每次都只能拉取与buffer缓冲相同大小的数据，然后通过内存中的一个Map进行聚合等操作。聚合完一批数据后，再拉取下一批数据，并放到buffer缓冲中进行聚合操作。以此类推，直到最后将所有数据到拉取完，并得到最终的结果。</p><p><img src="/blog/60787c8.html/1648882910142-f0b6c560-beac-475f-ae4c-669711da3b4b.png" alt="img"></p><h2 id="2-2-优化后的HashShuffleManager"><a href="#2-2-优化后的HashShuffleManager" class="headerlink" title="2.2 优化后的HashShuffleManager"></a>2.2 优化后的HashShuffleManager</h2><p>下图说明了优化后的HashShuffleManager的原理。</p><p><img src="/blog/60787c8.html/1648883523295-7bee2767-fd92-412e-94eb-c80712518570.png" alt="img"></p><p>这里说的优化，是指我们可以设置一个参数，spark.shuffle.consolidateFiles。该参数默认值为false，将其设置为true即可开启优化机制。通常来说，如果我们使用HashShuffleManager，那么都建议开启这个选项。</p><p>开启consolidate机制之后，在shuffle write过程中，task就不是为下游stage的每个task创建一个磁盘文件了。此时会出现shuffleFileGroup的概念，每个shuffleFileGroup会对应一批磁盘文件，磁盘文件的数量与下游stage的task数量是相同的。一个Executor上有多少个CPU core，就可以并行执行多少个task。而第一批并行执行的每个task都会创建一个shuffleFileGroup，并将数据写入对应的磁盘文件内。</p><p>当Executor的CPU core执行完一批task，接着执行下一批task时，下一批task就会复用之前已有的shuffleFileGroup，包括其中的磁盘文件。也就是说，此时task会将数据写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate机制允许不同的task复用同一批磁盘文件，这样就可以有效将多个task的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升shuffle write的性能。</p><p>假设第二个stage有100个task，第一个stage有50个task，总共还是有10个Executor，每个Executor执行5个task。那么原本使用未经优化的HashShuffleManager时，每个Executor会产生500个磁盘文件，所有Executor会产生5000个磁盘文件的。但是此时经过优化之后，每个Executor创建的磁盘文件的数量的计算公式为：CPU core的数量 * 下一个stage的task数量。也就是说，每个Executor此时只会创建100个磁盘文件，所有Executor只会创建1000个磁盘文件。</p><h2 id="2-3-普通运行机制"><a href="#2-3-普通运行机制" class="headerlink" title="2.3 普通运行机制"></a>2.3 普通运行机制</h2><p>下图说明了普通的SortShuffleManager的原理。在该模式下，数据会先写入一个内存数据结构中，此时根据不同的shuffle算子，可能选用不同的数据结构。如果是reduceByKey这种聚合类的shuffle算子，那么会选用Map数据结构，一边通过Map进行聚合，一边写入内存；如果是join这种普通的shuffle算子，那么会选用Array数据结构，直接写入内存。接着，每写一条数据进入内存数据结构之后，就会判断一下，是否达到了某个临界阈值。如果达到临界阈值的话，那么就会尝试将内存数据结构中的数据溢写到磁盘，然后清空内存数据结构。</p><p>在溢写到磁盘文件之前，会先根据key对内存数据结构中已有的数据进行排序。排序过后，会分批将数据写入磁盘文件。默认的batch数量是10000条，也就是说，排序好的数据，会以每批1万条数据的形式分批写入磁盘文件。写入磁盘文件是通过Java的BufferedOutputStream实现的。BufferedOutputStream是Java的缓冲输出流，首先会将数据缓冲在内存中，当内存缓冲满溢之后再一次写入磁盘文件中，这样可以减少磁盘IO次数，提升性能。</p><p>一个task将所有数据写入内存数据结构的过程中，会发生多次磁盘溢写操作，也就会产生多个临时文件。最后会将之前所有的临时磁盘文件都进行合并，这就是merge过程，此时会将之前所有临时磁盘文件中的数据读取出来，然后依次写入最终的磁盘文件之中。此外，由于一个task就只对应一个磁盘文件，也就意味着该task为下游stage的task准备的数据都在这一个文件中，因此还会单独写一份索引文件，其中标识了下游各个task的数据在文件中的start offset与end offset。</p><p>SortShuffleManager由于有一个磁盘文件merge的过程，因此大大减少了文件数量。比如第一个stage有50个task，总共有10个Executor，每个Executor执行5个task，而第二个stage有100个task。由于每个task最终只有一个磁盘文件，因此此时每个Executor上只有5个磁盘文件，所有Executor只有50个磁盘文件。</p><p><img src="/blog/60787c8.html/1648884121250-0f0397cf-5c7d-4a6e-8249-453ba11b395b.png" alt="img"></p><h2 id="2-4-bypass运行机制"><a href="#2-4-bypass运行机制" class="headerlink" title="2.4 bypass运行机制"></a>2.4 bypass运行机制</h2><p>下图说明了bypass SortShuffleManager的原理。bypass运行机制的触发条件如下：</p><p> * shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。 </p><p>* 不是聚合类的shuffle算子（比如reduceByKey）。</p><p>此时task会为每个下游task都创建一个临时磁盘文件，并将数据按key进行hash然后根据key的hash值，将key写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。</p><p>该过程的磁盘写机制其实跟未经优化的HashShuffleManager是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的HashShuffleManager来说，shuffle read的性能会更好。</p><p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，<strong>启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</strong></p><p><img src="/blog/60787c8.html/1648884245925-5db22e38-be48-42a6-8668-3dbdac9cbf1c.png" alt="img"></p><h1 id="三：shuffle相关参数调优"><a href="#三：shuffle相关参数调优" class="headerlink" title="三：shuffle相关参数调优"></a>三：shuffle相关参数调优</h1><p>以下是Shffule过程中的一些主要参数，这里详细讲解了各个参数的功能、默认值以及基于实践经验给出的调优建议。</p><h2 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h2><ul><li>默认值：32k</li><li>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会<strong>先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘</strong>。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h2 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h2><ul><li>默认值：48m</li><li>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了<strong>每次能够拉取多少数据</strong>。</li><li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li></ul><h2 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h2><ul><li>默认值：3</li><li>参数说明：shuffle read task从shuffle write task所在节点<strong>拉取属于自己的数据</strong>时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了<strong>可以重试的最大次数</strong>。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li><li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li></ul><h2 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h2><ul><li>默认值：5s</li><li>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li><li>调优建议：建议<strong>加大间隔时长</strong>（比如60s），以<strong>增加shuffle操作的稳定性</strong>。</li></ul><h2 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h2><ul><li>默认值：0.2</li><li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li><li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很<strong>少使用持久化操作，建议调高这个比例</strong>，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</li></ul><h2 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h2><ul><li>默认值：sort</li><li>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li><li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中<strong>需要该排序机制的话，则使用默认的SortShuffleManager就可以</strong>；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li></ul><h2 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h2><ul><li>默认值：200</li><li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li><li>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li></ul><h2 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h2><ul><li>默认值：false</li><li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul><h2 id="spark-shuffle-consolidateFiles-1"><a href="#spark-shuffle-consolidateFiles-1" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h2><ul><li>默认值：false</li><li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li><li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。</li></ul><p>开发过程中的优化原则、运行前的资源参数设置调优、运行中的数据倾斜的解决方案、为了精益求精的shuffle调优。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：ShuffleManager发展概述&quot;&gt;&lt;a href=&quot;#一：ShuffleManager发展概述&quot; class=&quot;headerlink&quot; title=&quot;一：ShuffleManager发展概述&quot;&gt;&lt;/a&gt;一：ShuffleManager发展概述&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 性能调优——数据倾斜调优</title>
    <link href="https://aiyingke.cn/blog/ed3d1f39.html/"/>
    <id>https://aiyingke.cn/blog/ed3d1f39.html/</id>
    <published>2023-02-22T03:22:33.000Z</published>
    <updated>2023-02-22T03:25:56.389Z</updated>
    
    <content type="html"><![CDATA[<p>数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。</p><h1 id="一：数据倾斜发生时的现象"><a href="#一：数据倾斜发生时的现象" class="headerlink" title="一：数据倾斜发生时的现象"></a>一：数据倾斜发生时的现象</h1><ul><li>绝大多数task执行得都非常快，但个别task执行极慢。比如，<strong>总共有1000个task，997个task都在1分钟之内执行完了，但是剩余两三个task却要一两个小时</strong>。这种情况很常见。</li><li>原本能够正常执行的Spark作业，某天突然报出OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li></ul><p><img src="/blog/ed3d1f39.html/1648867372204-756cdbdf-1f58-4759-a93b-74b7ea248566.png" alt="img"></p><h1 id="二：数据倾斜发生的原理"><a href="#二：数据倾斜发生的原理" class="headerlink" title="二：数据倾斜发生的原理"></a>二：数据倾斜发生的原理</h1><p>数据倾斜的原理很简单：在<strong>进行shuffle的时候，必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理</strong>，比如<strong>按照key进行聚合或join等操作</strong>。此时如果某个key对应的数据量特别大的话，就会发生数据倾斜。比如大部分key对应10条数据，但是个别key却对应了100万条数据，那么大部分task可能就只会分配到10条数据，然后1秒钟就运行完了；但是<strong>个别task可能分配到了100万数据(巨量数据)<strong>，要运行一两个小时。因此，整个</strong>Spark作业的运行进度是由运行时间最长的那个task决定的</strong>。</p><p>因此出现数据倾斜的时候，Spark作业看起来会运行得非常缓慢，甚至可能因为某个task处理的数据量过大导致内存溢出。</p><h1 id="三：实例-数据倾斜"><a href="#三：实例-数据倾斜" class="headerlink" title="三：实例 数据倾斜"></a>三：实例 数据倾斜</h1><p>下图就是一个很清晰的例子：hello这个key，在三个节点上对应了总共7条数据，这些数据都会被拉取到同一个task中进行处理；而world和you这两个key分别才对应1条数据，所以另外两个task只要分别处理1条数据即可。此时第一个task的运行时间可能是另外两个task的7倍，而整个stage的运行速度也由运行最慢的那个task所决定。</p><p><img src="/blog/ed3d1f39.html/1648867732857-4e4706a4-2f59-4470-a4a8-3fb9fdfd00d2.png" alt="img"></p><h1 id="四：如何定位导致数据倾斜的代码"><a href="#四：如何定位导致数据倾斜的代码" class="headerlink" title="四：如何定位导致数据倾斜的代码"></a>四：如何定位导致数据倾斜的代码</h1><p><strong>数据倾斜只会发生在shuffle过程中</strong>。这里给大家罗列一些常用的并且可能<strong>会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition</strong>等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</p><h2 id="4-1-某个task执行特别慢的情况"><a href="#4-1-某个task执行特别慢的情况" class="headerlink" title="4.1 某个task执行特别慢的情况"></a>4.1 某个task执行特别慢的情况</h2><p>首先要看的，就是数据倾斜发生在第几个stage中。</p><p>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage。此外，无论是使用yarn-client模式还是yarn-cluster模式，我们都可以<strong>在Spark Web UI上深入看一下当前这个stage各个task分配的数据量</strong>，从而进一步<strong>确定是不是task分配的数据不均匀导致了数据倾斜</strong>。</p><p>比如下图中，倒数第三列显示了每个task的运行时间。明显可以看到，有的task运行特别快，只需要几秒钟就可以运行完；而有的task运行特别慢，需要几分钟才能运行完，此时单从运行时间上看就已经能够确定发生数据倾斜了。此外，倒数第一列显示了每个task处理的数据量，明显可以看到，运行时间特别短的task只需要处理几百KB的数据即可，而<strong>运行时间特别长的task需要处理几千KB的数据</strong>，<strong>处理的数据量差了10倍</strong>。此时更加能够确定是发生了数据倾斜。</p><p><img src="/blog/ed3d1f39.html/1648870600764-44baba48-3bd0-4f0e-9c4c-06c289041072.png" alt="img"></p><p>知道<strong>数据倾斜发生在哪一个stage之后</strong>，接着我们就需要根据stage划分原理，<strong>推算出来发生倾斜的那个stage对应代码中的哪一部分</strong>，这部分代码中肯定会有一个<strong>shuffle类算子</strong>。精准推算stage与代码的对应关系，需要对Spark的源码有深入的理解，这里我们可以介绍一个相对简单实用的推算方法：只要看到Spark代码中出现了一个shuffle类算子或者是Spark SQL的SQL语句中出现了会导致shuffle的语句（比如group by语句），那么就可以判定，以那个地方为界限划分出了前后两个stage。</p><h2 id="4-2-代码实例"><a href="#4-2-代码实例" class="headerlink" title="4.2 代码实例"></a>4.2 代码实例</h2><p>这里我们就以Spark最基础的入门程序——单词计数来举例，如何用最简单的方法大致推算出一个stage对应的代码。如下示例，在整个代码中，只有一个reduceByKey是会发生shuffle的算子，因此就可以认为，以这个算子为界限，会划分出前后两个stage。 * stage0，主要是执行从textFile到map操作，以及执行shuffle write操作。shuffle write操作，我们可以简单理解为对pairs RDD中的数据进行分区操作，每个task处理的数据中，相同的key会写入同一个磁盘文件内。 * stage1，主要是执行从reduceByKey到collect操作，stage1的各个task一开始运行，就会首先执行shuffle read操作。执行shuffle read操作的task，会从stage0的各个task所在节点拉取属于自己处理的那些key，然后对同一个key进行全局性的聚合或join等操作，在这里就是对key的value值进行累加。stage1在执行完reduceByKey算子之后，就计算出了最终的wordCounts RDD，然后会执行collect算子，将所有数据拉取到Driver上，供我们遍历和打印输出。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> lines = sc.textFile(<span class="string">&quot;hdfs://...&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">val</span> pairs = words.map((_, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">wordCounts.collect().foreach(println(_))</span><br></pre></td></tr></table></figure><p>通过对单词计数程序的分析，希望能够让大家了解最基本的stage划分的原理，以及stage划分后shuffle操作是如何在两个stage的边界处执行的。然后我们就知道如何快速定位出发生数据倾斜的stage对应代码的哪一个部分了。比如我们在Spark Web UI或者本地log中发现，stage1的某几个task执行得特别慢，判定stage1出现了数据倾斜，那么就可以回到代码中定位出stage1主要包括了reduceByKey这个shuffle类算子，此时基本就可以确定是由educeByKey算子导致的数据倾斜问题。比如某个单词出现了100万次，其他单词才出现10次，那么stage1的某个task就要处理100万数据，整个stage的速度就会被这个task拖慢。</p><h2 id="4-3-某个task莫名其妙内存溢出的情况"><a href="#4-3-某个task莫名其妙内存溢出的情况" class="headerlink" title="4.3 某个task莫名其妙内存溢出的情况"></a>4.3 某个task莫名其妙内存溢出的情况</h2><p>这种情况下去定位出问题的代码就比较容易了。我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</p><p>但是大家要注意的是，不能单纯靠偶然的内存溢出就判定发生了数据倾斜。因为自己编写的代码的bug，以及偶然出现的数据异常，也可能会导致内存溢出。因此还是要按照上面所讲的方法，通过Spark Web UI查看报错的那个stage的各个task的运行时间以及分配的数据量，才能确定是否是由于数据倾斜才导致了这次内存溢出。</p><h2 id="4-4-查看导致数据倾斜的key的数据分布情况"><a href="#4-4-查看导致数据倾斜的key的数据分布情况" class="headerlink" title="4.4 查看导致数据倾斜的key的数据分布情况"></a>4.4 查看导致数据倾斜的key的数据分布情况</h2><p>知道了数据倾斜发生在哪里之后，通常需要分析一下那个执行了shuffle操作并且导致了数据倾斜的RDD&#x2F;Hive表，查看一下其中key的分布情况。这主要是为之后选择哪一种技术方案提供依据。针对不同的key分布与不同的shuffle算子组合起来的各种情况，可能需要选择不同的技术方案来解决。</p><p>此时根据你执行操作的情况不同，可以有很多种查看key分布的方式： 1. 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 2. 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect&#x2F;take到客户端打印一下，就可以看到key的分布情况。</p><h2 id="4-5-key分布示例"><a href="#4-5-key分布示例" class="headerlink" title="4.5 key分布示例"></a>4.5 key分布示例</h2><p>举例来说，对于上面所说的单词计数程序，如果确定了是stage1的reduceByKey算子导致了数据倾斜，那么就应该看看进行reduceByKey操作的RDD中的key分布情况，在这个例子中指的就是pairs RDD。如下示例，我们可以先对pairs采样10%的样本数据，然后使用countByKey算子统计出每个key出现的次数，最后在客户端遍历和打印样本数据中各个key的出现次数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sampledPairs = pairs.sample(<span class="literal">false</span>, <span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">val</span> sampledWordCounts = sampledPairs.countByKey()</span><br><span class="line">sampledWordCounts.foreach(println(_))</span><br></pre></td></tr></table></figure><h1 id="五：数据倾斜的解决方案"><a href="#五：数据倾斜的解决方案" class="headerlink" title="五：数据倾斜的解决方案"></a>五：数据倾斜的解决方案</h1><h2 id="解决方案一：使用Hive-ETL预处理数据"><a href="#解决方案一：使用Hive-ETL预处理数据" class="headerlink" title="解决方案一：使用Hive ETL预处理数据"></a>解决方案一：使用Hive ETL预处理数据</h2><p><strong>方案适用场景：</strong>导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p><p><strong>方案实现思路：</strong>此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p><p><strong>方案实现原理：</strong>这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把<strong>数据倾斜的发生提前到了Hive ETL中</strong>，避免Spark程序发生数据倾斜而已。</p><p><strong>方案优点：</strong>实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p><p><strong>方案缺点：</strong>治标不治本，Hive ETL中还是会发生数据倾斜。</p><p><strong>方案实践经验：</strong>在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p><p><strong>项目实践经验：</strong>在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p><h2 id="解决方案二：过滤少数导致倾斜的key"><a href="#解决方案二：过滤少数导致倾斜的key" class="headerlink" title="解决方案二：过滤少数导致倾斜的key"></a>解决方案二：过滤少数导致倾斜的key</h2><p><strong>方案适用场景：</strong>如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p><p><strong>方案实现思路：</strong>如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p><p><strong>方案实现原理：</strong>将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p><p><strong>方案优点：</strong>实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p><p><strong>方案缺点：</strong>适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p><p><strong>方案实践经验：</strong>在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取<strong>每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</strong></p><h2 id="解决方案三：提高shuffle操作的并行度"><a href="#解决方案三：提高shuffle操作的并行度" class="headerlink" title="解决方案三：提高shuffle操作的并行度"></a>解决方案三：提高shuffle操作的并行度</h2><p><strong>方案适用场景：</strong>如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p><p><strong>方案实现思路：</strong>在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p><p><strong>方案实现原理：</strong>增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p><p><img src="/blog/ed3d1f39.html/1648879873365-498c4baf-0c86-4637-b1a1-ca41e41f181e.png" alt="img"></p><p><strong>方案优点：</strong>实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p><p><strong>方案缺点：</strong>只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p><p><strong>方案实践经验：</strong>该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用最简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p><h2 id="解决方案四：两阶段聚合（局部聚合-全局聚合）"><a href="#解决方案四：两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="解决方案四：两阶段聚合（局部聚合+全局聚合）"></a>解决方案四：两阶段聚合（局部聚合+全局聚合）</h2><p><strong>方案适用场景：</strong>对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p><p><strong>方案实现思路：</strong>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p><p><strong>方案实现原理：</strong>将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p><p><strong>方案优点：</strong>对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p><p><strong>方案缺点：</strong>仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p><p><img src="/blog/ed3d1f39.html/1648880080869-a084c2f2-4084-411b-8401-1e4252766ddc.png" alt="img"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 第一步，给RDD中的每个key都打上一个随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; randomPrefixRdd = rdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,Long&gt;, String, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                <span class="type">int</span> <span class="variable">prefix</span> <span class="operator">=</span> random.nextInt(<span class="number">10</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Long&gt;(prefix + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第二步，对打上随机前缀的key进行局部聚合。</span></span><br><span class="line">JavaPairRDD&lt;String, Long&gt; localAggrRdd = randomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Long <span class="title function_">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第三步，去除RDD中每个key的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; removedRandomPrefixRdd = localAggrRdd.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;String,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;String, Long&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">long</span> <span class="variable">originalKey</span> <span class="operator">=</span> Long.valueOf(tuple._1.split(<span class="string">&quot;_&quot;</span>)[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Long&gt;(originalKey, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 第四步，对去除了随机前缀的RDD进行全局聚合。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; globalAggrRdd = removedRandomPrefixRdd.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Long <span class="title function_">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br></pre></td></tr></table></figure><h3 id><a href="#" class="headerlink" title></a></h3><h2 id="解决方案五：将reduce-join转为map-join"><a href="#解决方案五：将reduce-join转为map-join" class="headerlink" title="解决方案五：将reduce join转为map join"></a>解决方案五：将reduce join转为map join</h2><p><strong>方案适用场景：</strong>在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p><p><strong>方案实现思路：</strong>不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p><p><strong>方案实现原理：</strong>普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p><p><img src="/blog/ed3d1f39.html/1648880677360-20f904a3-03f6-44f4-bef4-01194d0e20f6.png" alt="img"></p><p><strong>方案优点：</strong>对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p><p><strong>方案缺点：</strong>适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将数据量比较小的RDD的数据，collect到Driver中来。</span></span><br><span class="line">List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1.collect()</span><br><span class="line"><span class="comment">// 然后使用Spark的广播功能，将小RDD的数据转换成广播变量，这样每个Executor就只有一份RDD的数据。</span></span><br><span class="line"><span class="comment">// 可以尽可能节省内存空间，并且减少网络传输性能开销。</span></span><br><span class="line"><span class="keyword">final</span> Broadcast&lt;List&lt;Tuple2&lt;Long, Row&gt;&gt;&gt; rdd1DataBroadcast = sc.broadcast(rdd1Data);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对另外一个RDD执行map类操作，而不再是join类操作。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRdd = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, String, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// 在算子函数中，通过广播变量，获取到本地Executor中的rdd1数据。</span></span><br><span class="line">                List&lt;Tuple2&lt;Long, Row&gt;&gt; rdd1Data = rdd1DataBroadcast.value();</span><br><span class="line">                <span class="comment">// 可以将rdd1的数据转换为一个Map，便于后面进行join操作。</span></span><br><span class="line">                Map&lt;Long, Row&gt; rdd1DataMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;Long, Row&gt;();</span><br><span class="line">                <span class="keyword">for</span>(Tuple2&lt;Long, Row&gt; data : rdd1Data) &#123;</span><br><span class="line">                    rdd1DataMap.put(data._1, data._2);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 获取当前RDD数据的key以及value。</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> tuple._1;</span><br><span class="line">                <span class="type">String</span> <span class="variable">value</span> <span class="operator">=</span> tuple._2;</span><br><span class="line">                <span class="comment">// 从rdd1数据Map中，根据key获取到可以join到的数据。</span></span><br><span class="line">                <span class="type">Row</span> <span class="variable">rdd1Value</span> <span class="operator">=</span> rdd1DataMap.get(key);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(key, <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Row&gt;(value, rdd1Value));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 这里得提示一下。</span></span><br><span class="line"><span class="comment">// 上面的做法，仅仅适用于rdd1中的key没有重复，全部是唯一的场景。</span></span><br><span class="line"><span class="comment">// 如果rdd1中有多个相同的key，那么就得用flatMap类的操作，在进行join的时候不能用map，而是得遍历rdd1所有数据进行join。</span></span><br><span class="line"><span class="comment">// rdd2中每条数据都可能会返回多条join后的数据。</span></span><br></pre></td></tr></table></figure><h2 id="解决方案六：采样倾斜key并分拆join操作"><a href="#解决方案六：采样倾斜key并分拆join操作" class="headerlink" title="解决方案六：采样倾斜key并分拆join操作"></a>解决方案六：采样倾斜key并分拆join操作</h2><p><strong>方案适用场景：</strong>两个RDD&#x2F;Hive表进行join的时候，如果数据量都比较大，无法采用“解决方案五”，那么此时可以看一下两个RDD&#x2F;Hive表中的key分布情况。如果出现数据倾斜，是因为其中某一个RDD&#x2F;Hive表中的少数几个key的数据量过大，而另一个RDD&#x2F;Hive表中的所有key都分布比较均匀，那么采用这个解决方案是比较合适的。</p><p><strong>方案实现思路：</strong> * 对包含少数几个数据量过大的key的那个RDD，通过sample算子采样出一份样本来，然后统计一下每个key的数量，计算出来数据量最大的是哪几个key。 * 然后将这几个key对应的数据从原来的RDD中拆分出来，形成一个单独的RDD，并给每个key都打上n以内的随机数作为前缀，而不会导致倾斜的大部分key形成另外一个RDD。 * 接着将需要join的另一个RDD，也过滤出来那几个倾斜key对应的数据并形成一个单独的RDD，将每条数据膨胀成n条数据，这n条数据都按顺序附加一个0~n的前缀，不会导致倾斜的大部分key也形成另外一个RDD。 * 再将附加了随机前缀的独立RDD与另一个膨胀n倍的独立RDD进行join，此时就可以将原先相同的key打散成n份，分散到多个task中去进行join了。 * 而另外两个普通的RDD就照常join即可。 * 最后将两次join的结果使用union算子合并起来即可，就是最终的join结果。</p><p><strong>方案实现原理：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。具体原理见下图。</p><p><strong>方案优点：</strong>对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p><p><strong>方案缺点：</strong>如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p><p><img src="/blog/ed3d1f39.html/1648881217470-bd1c1dc7-5fd4-4b13-8b3a-e9635a821029.png" alt="img"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先从包含了少数几个导致数据倾斜key的rdd1中，采样10%的样本数据。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; sampledRDD = rdd1.sample(<span class="literal">false</span>, <span class="number">0.1</span>);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对样本数据RDD统计出每个key的出现次数，并按出现次数降序排序。</span></span><br><span class="line"><span class="comment">// 对降序排序后的数据，取出top 1或者top 100的数据，也就是key最多的前n个数据。</span></span><br><span class="line"><span class="comment">// 具体取出多少个数据量最多的key，由大家自己决定，我们这里就取1个作为示范。</span></span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; mappedSampledRDD = sampledRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Long&gt;(tuple._1, <span class="number">1L</span>);</span><br><span class="line">            &#125;     </span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; countedSampledRDD = mappedSampledRDD.reduceByKey(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Long, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Long <span class="title function_">call</span><span class="params">(Long v1, Long v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">JavaPairRDD&lt;Long, Long&gt; reversedSampledRDD = countedSampledRDD.mapToPair( </span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,Long&gt;, Long, Long&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Long, Long&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Long&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Long&gt;(tuple._2, tuple._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="keyword">final</span> <span class="type">Long</span> <span class="variable">skewedUserid</span> <span class="operator">=</span> reversedSampledRDD.sortByKey(<span class="literal">false</span>).take(<span class="number">1</span>).get(<span class="number">0</span>)._2;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 从rdd1中分拆出导致数据倾斜的key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; skewedRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"><span class="comment">// 从rdd1中分拆出不导致数据倾斜的普通key，形成独立的RDD。</span></span><br><span class="line">JavaPairRDD&lt;Long, String&gt; commonRDD = rdd1.filter(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Tuple2&lt;Long,String&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> !tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125; </span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// rdd2，就是那个所有key的分布相对较为均匀的rdd。</span></span><br><span class="line"><span class="comment">// 这里将rdd2中，前面获取到的key对应的数据，过滤出来，分拆成单独的rdd，并对rdd中的数据使用flatMap算子都扩容100倍。</span></span><br><span class="line"><span class="comment">// 对扩容的每条数据，都打上0～100的前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; skewedRdd2 = rdd2.filter(</span><br><span class="line">         <span class="keyword">new</span> <span class="title class_">Function</span>&lt;Tuple2&lt;Long,Row&gt;, Boolean&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Boolean <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Row&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> tuple._1.equals(skewedUserid);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).flatMapToPair(<span class="keyword">new</span> <span class="title class_">PairFlatMapFunction</span>&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(</span></span><br><span class="line"><span class="params">                    Tuple2&lt;Long, Row&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Row&gt;(i + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">              </span><br><span class="line">        &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的导致倾斜的key的独立rdd，每条数据都打上100以内的随机前缀。</span></span><br><span class="line"><span class="comment">// 然后将这个rdd1中分拆出来的独立rdd，与上面rdd2中分拆出来的独立rdd，进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD1 = skewedRDD.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                <span class="type">int</span> <span class="variable">prefix</span> <span class="operator">=</span> random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(prefix + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">        .join(skewedUserid2infoRDD)</span><br><span class="line">        .mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;String,Tuple2&lt;String,Row&gt;&gt;, Long, Tuple2&lt;String, Row&gt;&gt;() &#123;</span><br><span class="line">                        <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">                        <span class="meta">@Override</span></span><br><span class="line">                        <span class="keyword">public</span> Tuple2&lt;Long, Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(</span></span><br><span class="line"><span class="params">                            Tuple2&lt;String, Tuple2&lt;String, Row&gt;&gt; tuple)</span></span><br><span class="line">                            <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                            <span class="type">long</span> <span class="variable">key</span> <span class="operator">=</span> Long.valueOf(tuple._1.split(<span class="string">&quot;_&quot;</span>)[<span class="number">1</span>]);</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Long, Tuple2&lt;String, Row&gt;&gt;(key, tuple._2);</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将rdd1中分拆出来的包含普通key的独立rdd，直接与rdd2进行join。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD2 = commonRDD.join(rdd2);</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 将倾斜key join后的结果与普通key join后的结果，uinon起来。</span></span><br><span class="line"><span class="comment">// 就是最终的join结果。</span></span><br><span class="line">JavaPairRDD&lt;Long, Tuple2&lt;String, Row&gt;&gt; joinedRDD = joinedRDD1.union(joinedRDD2);</span><br></pre></td></tr></table></figure><h2 id="解决方案七：使用随机前缀和扩容RDD进行join"><a href="#解决方案七：使用随机前缀和扩容RDD进行join" class="headerlink" title="解决方案七：使用随机前缀和扩容RDD进行join"></a>解决方案七：使用随机前缀和扩容RDD进行join</h2><p><strong>方案适用场景：</strong>如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义，此时就只能使用最后一种方案来解决问题了。</p><p><strong>方案实现思路：</strong> * 该方案的实现思路基本和“解决方案六”类似，首先查看RDD&#x2F;Hive表中的数据分布情况，找到那个造成数据倾斜的RDD&#x2F;Hive表，比如有多个key都对应了超过1万条数据。 * 然后将该RDD的每条数据都打上一个n以内的随机前缀。 * 同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。 * 最后将两个处理后的RDD进行join即可。</p><p><strong>方案实现原理：</strong>将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案与“解决方案六”的不同之处就在于，上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p><p><strong>方案优点：</strong>对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p><p><strong>方案缺点：</strong>该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p><p><strong>方案实践经验：</strong>曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先将其中一个key分布相对较为均匀的RDD膨胀100倍。</span></span><br><span class="line">JavaPairRDD&lt;String, Row&gt; expandedRDD = rdd1.flatMapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFlatMapFunction</span>&lt;Tuple2&lt;Long,Row&gt;, String, Row&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;Tuple2&lt;String, Row&gt;&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, Row&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                List&lt;Tuple2&lt;String, Row&gt;&gt; list = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;Tuple2&lt;String, Row&gt;&gt;();</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">                    list.add(<span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Row&gt;(<span class="number">0</span> + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 其次，将另一个有数据倾斜key的RDD，每条数据都打上100以内的随机前缀。</span></span><br><span class="line">JavaPairRDD&lt;String, String&gt; mappedRDD = rdd2.mapToPair(</span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Long,String&gt;, String, String&gt;() &#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, String&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Long, String&gt; tuple)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line">                <span class="type">int</span> <span class="variable">prefix</span> <span class="operator">=</span> random.nextInt(<span class="number">100</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, String&gt;(prefix + <span class="string">&quot;_&quot;</span> + tuple._1, tuple._2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 将两个处理后的RDD进行join即可。</span></span><br><span class="line">JavaPairRDD&lt;String, Tuple2&lt;String, Row&gt;&gt; joinedRDD = mappedRDD.join(expandedRDD);</span><br></pre></td></tr></table></figure><h2 id="解决方案八：多种方案组合使用"><a href="#解决方案八：多种方案组合使用" class="headerlink" title="解决方案八：多种方案组合使用"></a>解决方案八：多种方案组合使用</h2><p>在实践中发现，很多情况下，如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，<strong>预处理</strong>一部分数据，并<strong>过滤</strong>一部分数据来缓解；其次可以对某些shuffle操作<strong>提升并行度</strong>，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。大家需要对这些方案的思路和原理都透彻理解之后，在实践中根据各种不同的情况，灵活运用多种方案，来解决自己的数据倾斜问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。&lt;/p&gt;
&lt;h1 id=&quot;一：数据倾斜发生时的现象&quot;&gt;&lt;a href=&quot;#一：数据倾斜发生时的现象&quot; class=&quot;headerlink&quot; title=&quot;一：数据倾斜发生时的现象&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 性能调优——资源调优</title>
    <link href="https://aiyingke.cn/blog/5f7211d0.html/"/>
    <id>https://aiyingke.cn/blog/5f7211d0.html/</id>
    <published>2023-02-22T03:18:31.000Z</published>
    <updated>2023-02-22T03:21:13.582Z</updated>
    
    <content type="html"><![CDATA[<h2 id="调优概述"><a href="#调优概述" class="headerlink" title="调优概述"></a>调优概述</h2><p>在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为参数设置。很多Spark初学者，通常不知道该设置哪些必要的参数，以及如何设置这些参数，最后就只能胡乱设置，甚至压根儿不设置。资源参数设置的不合理，可能会导致没有充分利用集群资源，作业运行会极其缓慢；或者设置的资源过大，队列没有足够的资源来提供，进而导致各种异常。总之，无论是哪种情况，都会导致Spark作业的运行效率低下，甚至根本无法运行。因此我们必须对Spark作业的资源使用原理有一个清晰的认识，并知道在Spark作业运行过程中，有哪些资源参数是可以设置的，以及如何设置合适的参数值。</p><h2 id="Spark作业基本运行原理"><a href="#Spark作业基本运行原理" class="headerlink" title="Spark作业基本运行原理"></a>Spark作业基本运行原理</h2><p><img src="/blog/5f7211d0.html/1648805615385-560485a2-b665-4472-a0b0-41dfc9b6a557.png" alt="img"></p><p>详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动，也可能在集群中某个工作节点上启动。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器（可以是Spark Standalone集群，也可以是其他的资源管理集群，美团•大众点评使用的是YARN作为资源管理集群）申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。</p><p>在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。</p><p>Spark是根据shuffle类算子来进行stage的划分。如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。</p><p>当我们在代码中执行了cache&#x2F;persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。</p><p>因此Executor的内存主要分为三块：第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的20%；第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的20%；第三块是让RDD持久化时使用，默认占Executor总内存的60%。</p><p>task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。</p><p>以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。</p><h2 id="资源参数调优"><a href="#资源参数调优" class="headerlink" title="资源参数调优"></a>资源参数调优</h2><p>了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p><h3 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h3><ul><li>参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</li><li>参数调优建议：每个Spark作业的运行一般设置<strong>50~100个左右的Executor进程比较合适</strong>，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；<strong>设置的太多的话，大部分队列可能无法给予充分的资源</strong>。</li></ul><h3 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h3><ul><li>参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</li><li>参数调优建议：<strong>每个Executor进程的内存设置4G~8G较为合适</strong>。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己<strong>团队的资源队列的最大内存限制是多少</strong>，<strong>num-executors乘以executor-memory，是不能超过队列的最大内存量的。</strong>此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列<strong>最大总内存的1&#x2F;3~1&#x2F;2</strong>，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</li></ul><h3 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h3><ul><li>参数说明：该参数用于设置每个<strong>Executor进程的CPU core数量</strong>。这个参数决定了<strong>每个Executor进程并行执行task线程的能力</strong>。因为<strong>每个CPU core同一时间只能执行一个task线程</strong>，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</li><li>参数调优建议：<strong>Executor的CPU core数量设置为2~4个较为合适</strong>。同样得根据不同部门的资源队列来定，可以看看<strong>自己的资源队列的最大CPU core限制是多少</strong>，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么<strong>num-executors * executor-cores不要超过队列总CPU core的1&#x2F;3~1&#x2F;2左右比较合适</strong>，也是避免影响其他同学的作业运行。</li></ul><h3 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h3><ul><li>参数说明：该参数用于设置Driver进程的内存。</li><li>参数调优建议：Driver的内存通常来说不设置，或者设置<strong>1G左右应该就够了</strong>。唯一需要注意的一点是，如果<strong>需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大</strong>，否则会出现OOM内存溢出的问题。</li></ul><h3 id="spark-default-parallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a>spark.default.parallelism</h3><ul><li>参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果<strong>不设置可能会直接影响你的Spark作业性能</strong>。</li><li>参数调优建议：<strong>Spark作业的默认task数量为500~1000个较为合适。</strong>很多同学常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），<strong>如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃</strong>。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，<strong>那么90%的Executor进程可能根本就没有task执行</strong>，<strong>也就是白白浪费了资源</strong>！因此Spark官网建议的设置原则是，设置该参数为<strong>num-executors * executor-cores的2~3倍较为合适</strong>，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</li></ul><h3 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.storage.memoryFraction</h3><ul><li>参数说明：该参数用于设置<strong>RDD持久化数据</strong>在Executor内存中<strong>能占的比例</strong>，默认是<strong>0.6</strong>。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</li><li>参数调优建议：如果Spark作业中，<strong>有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中</strong>。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是<strong>如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适</strong>。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><h3 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h3><ul><li>参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，<strong>默认是0.2</strong>。也就是说，<strong>Executor默认只有20%的内存用来进行该操作</strong>。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</li><li>参数调优建议：<strong>如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。</strong>此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</li></ul><p>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p><h2 id="资源参数参考示例"><a href="#资源参数参考示例" class="headerlink" title="资源参数参考示例"></a>资源参数参考示例</h2><p>以下是一份spark-submit命令的示例，大家可以参考一下，并根据自己的实际情况进行调节：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">--master yarn-cluster \</span><br><span class="line">--num-executors 100 \</span><br><span class="line">--executor-memory 6G \</span><br><span class="line">--executor-cores 4 \</span><br><span class="line">--driver-memory 1G \</span><br><span class="line">--conf spark.default.parallelism=1000 \</span><br><span class="line">--conf spark.storage.memoryFraction=0.5 \</span><br><span class="line">--conf spark.shuffle.memoryFraction=0.3 \</span><br></pre></td></tr></table></figure><p>根据实践经验来看，大部分Spark作业经过本次基础篇所讲解的开发调优与资源调优之后，一般都能以较高的性能运行了，足以满足我们的需求。但是在不同的生产环境和项目背景下，可能会遇到其他更加棘手的问题（比如各种数据倾斜），也可能会遇到更高的性能要求。为了应对这些挑战，需要使用更高级的技巧来处理这类问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;调优概述&quot;&gt;&lt;a href=&quot;#调优概述&quot; class=&quot;headerlink&quot; title=&quot;调优概述&quot;&gt;&lt;/a&gt;调优概述&lt;/h2&gt;&lt;p&gt;在开发完Spark作业之后，就该为作业配置合适的资源了。Spark的资源参数，基本都可以在spark-submit命令中作为</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 性能调优——开发调优</title>
    <link href="https://aiyingke.cn/blog/b222527c.html/"/>
    <id>https://aiyingke.cn/blog/b222527c.html/</id>
    <published>2023-02-22T02:55:14.000Z</published>
    <updated>2023-02-22T03:19:43.806Z</updated>
    
    <content type="html"><![CDATA[<p>大多数spark作业的性能主要就是消耗了shuffle过程，因为该环节包含了<strong>大量的磁盘IO、序列化、网络数据传输</strong>等操作。</p><p>影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个Spark的性能调优中占到一小部分而已。</p><p>开发调优、资源调优、数据倾斜调优、shuffle调优；</p><p>Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化</p><h2 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h2><p><strong>对于同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据。****对多次使用的RDD进行持久化</strong></p><p>一些Spark初学者在刚开始开发Spark作业时，或者是有经验的工程师在开发RDD lineage极其冗长的Spark作业时，可能会忘了自己之前对于某一份数据已经创建过一个RDD了，从而导致对于同一份数据，创建了多个RDD。这就意味着，我们的Spark作业会进行多次重复计算来创建多个代表相同数据的RDD，进而增加了作业的性能开销。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。</span><br><span class="line"></span><br><span class="line">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span><br><span class="line">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。</span><br><span class="line">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span><br><span class="line">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">val rdd2 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class="line">rdd2.reduce(...)</span><br><span class="line"></span><br><span class="line">// 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span><br><span class="line">// 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，然后对这一个RDD执行了多次算子操作。</span><br><span class="line">// 但是要注意到这里为止优化还没有结束，由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</span><br><span class="line">// 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，才能保证一个RDD被多次使用时只被计算一次。</span><br><span class="line">val rdd1 = sc.textFile(&quot;hdfs://192.168.0.1:9000/hello.txt&quot;)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p><img src="/blog/b222527c.html/1648795120926-e6c6fea0-1a01-4481-8ffc-3bdf0551b1ad.png" alt="img"></p><p>如果要求持久化数据可能丢失的情况下，还是要保证高性能，那么就在第一次计算RDD 时，消耗一些性能，对 RDD 进行 checkpoint 操作。这样，即使持久化数据丢失了，也可以直接读取其 checkpoint 数据。</p><h2 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h2><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能地复用一个RDD。比如说，有一个RDD的数据格式是key-value类型的，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD，因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">// 错误的做法。</span><br><span class="line"></span><br><span class="line">// 有一个&lt;Long, String&gt;格式的RDD，即rdd1。</span><br><span class="line">// 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class="line">JavaRDD&lt;String&gt; rdd2 = rdd1.map(...)</span><br><span class="line"></span><br><span class="line">// 分别对rdd1和rdd2执行了不同的算子操作。</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd2.map(...)</span><br><span class="line"></span><br><span class="line">// 正确的做法。</span><br><span class="line"></span><br><span class="line">// 上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</span><br><span class="line">// 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</span><br><span class="line"></span><br><span class="line">// 其实在这种情况下完全可以复用同一个RDD。</span><br><span class="line">// 我们可以使用rdd1，既做reduceByKey操作，也做map操作。</span><br><span class="line">// 在进行第二个map操作时，只使用每个数据的tuple._2，也就是rdd1中的value值，即可。</span><br><span class="line">JavaPairRDD&lt;Long, String&gt; rdd1 = ...</span><br><span class="line">rdd1.reduceByKey(...)</span><br><span class="line">rdd1.map(tuple._2...)</span><br><span class="line"></span><br><span class="line">// 第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</span><br><span class="line">// 但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，rdd1实际上还是会被计算两次。</span><br><span class="line">// 因此还需要配合“原则三：对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</span><br></pre></td></tr></table></figure><h2 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h2><p>当你在Spark代码中多次对一个RDD做了算子操作后，恭喜，你已经实现Spark作业第一步的优化了，也就是尽可能复用RDD。此时就该在这个基础之上，进行第二步优化了，也就是要保证对一个RDD执行多次算子操作时，这个RDD本身仅仅被计算一次。</p><p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p><p>因此对于这种情况，我们的建议是：对多次使用的RDD进行持久化。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD，再执行算子操作。</p><p><strong>对多次使用的RDD进行持久化的代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 正确的做法。</span></span><br><span class="line"><span class="comment">// cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></span><br><span class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。</span></span><br><span class="line"><span class="comment">// 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;hdfs://192.168.0.1:9000/hello.txt&quot;</span>).cache()</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></span><br><span class="line"><span class="comment">// 比如说，StorageLevel.MEMORY_AND_DISK_SER表示，内存充足时优先持久化到内存中，内存不充足时持久化到磁盘文件中。</span></span><br><span class="line"><span class="comment">// 而且其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></span><br><span class="line"><span class="comment">// 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。</span></span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">&quot;hdfs://192.168.0.1:9000/hello.txt&quot;</span>).persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">rdd1.map(...)</span><br><span class="line">rdd1.reduce(...)</span><br></pre></td></tr></table></figure><p><strong>Spark的持久化级别</strong></p><table><thead><tr><th><strong>持久化级别</strong></th><th><strong>含义解释</strong></th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>使用未序列化的Java对象格式，将数据保存在内存中。如果内存不够存放所有的数据，则数据可能就不会进行持久化。那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍。这是默认的持久化策略，使用cache()方法时，实际就是使用的这种持久化策略。</td></tr><tr><td>MEMORY_AND_DISK</td><td>使用未序列化的Java对象格式，优先尝试将数据保存在内存中。如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘文件中的数据会被读取出来使用。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>基本含义同MEMORY_ONLY。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>基本含义同MEMORY_AND_DISK。唯一的区别是，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组。这种方式更加节省内存，从而可以避免持久化的数据占用过多内存导致频繁GC。</td></tr><tr><td>DISK_ONLY</td><td>使用未序列化的Java对象格式，将数据全部写入磁盘文件中。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, 等等.</td><td>对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化的数据，都复制一份副本，并将副本保存到其他节点上。这种基于副本的持久化机制主要用于进行容错。假如某个节点挂掉，节点的内存或磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本。如果没有副本的话，就只能将这些数据从源头处重新计算一遍了。</td></tr></tbody></table><p>持久化：写入磁盘中</p><p>序列化：将RDD的转换为字节数组</p><p><strong>如何选择一种最合适的持久化策略</strong></p><ul><li>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</li><li>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</li><li>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</li><li>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</li></ul><h2 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h2><p>如果有可能的话，要尽量避免使用shuffle类算子。因为Spark作业运行过程中，最消耗性能的地方就是shuffle过程。shuffle过程，简单来说，就是<strong>将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合或join等操作</strong>。比如<strong>reduceByKey</strong>、<strong>join</strong>等算子，都会触发shuffle操作。</p><p>shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会<strong>发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作</strong>。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。</p><p>因此在我们的开发过程中，<strong>能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。</strong>这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p><p><strong>Broadcast与map进行join代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传统的join操作会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></span><br><span class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></span><br><span class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</span><br><span class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></span><br><span class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的，那么就判定可以进行join。</span></span><br><span class="line"><span class="comment">// 此时就可以根据自己需要的方式，将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注意，以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></span><br><span class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></span><br></pre></td></tr></table></figure><h2 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h2><p>spark.shuffle.consolidateFiles</p><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p><p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p><p>比如如下两幅图，就是典型的例子，分别基于reduceByKey和groupByKey进行单词计数。其中第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><strong>第一张图是groupByKey的原理图，可以看到，没有进行任何本地聚合时，所有数据都会在集群节点之间传输；</strong></p><p><img src="/blog/b222527c.html/1648804303841-ea514c86-2c77-4231-92c6-c634540d78ac.png" alt="img"></p><p><strong>第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</strong></p><p>第二张图是reduceByKey的原理图，可以看到，每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合。</p><p><img src="/blog/b222527c.html/1648804507865-7f2d5a07-bfd6-4943-b1f1-beb5f9c11542.png" alt="img"></p><h2 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h2><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p><p><strong>使用reduceByKey&#x2F;aggregateByKey替代groupByKey</strong></p><p><strong>使用mapPartitions替代普通map</strong></p><p>mapPartitions类的算子，一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p><p><strong>使用foreachPartitions替代foreach</strong></p><p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下；但是如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p><p><strong>使用filter之后进行coalesce操作</strong></p><p>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量，将RDD中的数据压缩到更少的partition中去。因为filter之后，RDD的每个partition中都会有很多数据被过滤掉，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p><p><strong>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</strong></p><p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的。</p><h2 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h2><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p><p>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p><p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p><p><strong>广播大变量的代码示例</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></span><br><span class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line">rdd1.map(list1...)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></span><br><span class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></span><br><span class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点上远程拉取一份放到本地Executor内存中。</span></span><br><span class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></span><br><span class="line"><span class="keyword">val</span> list1 = ...</span><br><span class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</span><br><span class="line">rdd1.map(list1Broadcast...)</span><br></pre></td></tr></table></figure><h2 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h2><p>在Spark中，主要有三个地方涉及到了序列化： </p><p>* 在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。 </p><p>* 将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。 </p><p>* 使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</p><p>对于这三种出现序列化的地方，我们都可以通过使用Kryo序列化类库，来优化序列化和反序列化的性能。Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream&#x2F;ObjectInputStream API来进行序列化和反序列化。但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，<strong>性能高10倍左右</strong>。Spark之所以默认没有使用Kryo作为序列化类库，是因为<strong>Kryo要求最好要注册所有需要进行序列化的自定义类型</strong>，因此对于开发者来说，这种方式比较麻烦。</p><p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建SparkConf对象。</span></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</span><br><span class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></span><br><span class="line">conf.set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line"><span class="comment">// 注册要序列化的自定义类型。</span></span><br><span class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</span><br></pre></td></tr></table></figure><h2 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h2><p>Java中，有三种类型比较耗费内存： </p><p>* 对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。 </p><p>* 字符串，每个字符串内部都有一个字符数组以及长度等额外信息。 </p><p>* 集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</p><p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p><p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;大多数spark作业的性能主要就是消耗了shuffle过程，因为该环节包含了&lt;strong&gt;大量的磁盘IO、序列化、网络数据传输&lt;/strong&gt;等操作。&lt;/p&gt;
&lt;p&gt;影响一个Spark作业性能的因素，主要还是代码开发、资源参数以及数据倾斜，shuffle调优只能在整个S</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 常用算子</title>
    <link href="https://aiyingke.cn/blog/bda70396.html/"/>
    <id>https://aiyingke.cn/blog/bda70396.html/</id>
    <published>2023-02-20T08:43:05.000Z</published>
    <updated>2023-02-21T04:10:04.901Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：算子概述"><a href="#一：算子概述" class="headerlink" title="一：算子概述"></a>一：算子概述</h1><h2 id="1-1-什么是算子？"><a href="#1-1-什么是算子？" class="headerlink" title="1.1 什么是算子？"></a>1.1 什么是算子？</h2><ul><li><p>英文：Operator</p></li><li><p>狭义：一个函数空间到另一个函数空间的映射</p></li><li><p>广义：一个空间到另个一个空间的映射</p></li><li><p>白话：一个事物从一个状态到另一个状态的过程</p></li><li><p>实质：映射，即关系</p></li></ul><h2 id="1-2-算子的重要作用"><a href="#1-2-算子的重要作用" class="headerlink" title="1.2 算子的重要作用"></a>1.2 算子的重要作用</h2><ul><li>算子越多，灵活性越高，编程的可选方式就越多</li><li>算子越多，表现能力强，可以灵活应对各种复杂场景</li></ul><h2 id="1-3-MapReduce-和-Spark-算子比较"><a href="#1-3-MapReduce-和-Spark-算子比较" class="headerlink" title="1.3 MapReduce 和 Spark 算子比较"></a>1.3 MapReduce 和 Spark 算子比较</h2><ul><li>MapReduce 只有2个算子，map和reduce，绝大多数场景下，需要复杂的编程来完成业务需求</li><li>Spark 有80多个算子，可以灵活组合应对不同的业务场景</li></ul><h1 id="二：Spark算子"><a href="#二：Spark算子" class="headerlink" title="二：Spark算子"></a>二：Spark算子</h1><h2 id="2-1-转换算子（transformation）"><a href="#2-1-转换算子（transformation）" class="headerlink" title="2.1 转换算子（transformation）"></a>2.1 转换算子（transformation）</h2><p>此种算子不会真正的触发提交作业，只有作业被提交后才会触发转换计算</p><ul><li>value型转换算子（处理的数据项是value型）<ul><li>输入分区：输出分区 &#x3D; 1 ： 1<ul><li>map算子</li><li>flatMap算子</li><li>mapPartitions算子</li></ul></li><li>输入分区：输出分区 &#x3D; n ： 1<ul><li>union算子</li><li>cartesian算子</li></ul></li><li>输入分区 ：输出分区 &#x3D; n ： n<ul><li>groupBy算子</li></ul></li><li>输出分区为输入分区的子集<ul><li>filter算子</li><li>distinct算子</li><li>substract算子</li><li>sample算子</li><li>takeSample算子</li></ul></li><li>cache型算子<ul><li>cache算子</li><li>persist算子</li></ul></li></ul></li><li>key-value型转换算子（处理的数据类型是key-value型）<ul><li>输入分区：输出分区 &#x3D; 1： 1<ul><li>mapValues 算子</li></ul></li><li>对单个RDD聚集<ul><li>combineByKey算子</li><li>reduceByKey算子</li><li>partitionBy算子</li></ul></li><li>对两个RDD聚合<ul><li>cogroup算子</li></ul></li><li>连接<ul><li>join算子</li><li>leftOutJoin算子</li><li>rightOutJoin算子</li></ul></li></ul></li></ul><h2 id="2-2-行动算子（action）"><a href="#2-2-行动算子（action）" class="headerlink" title="2.2 行动算子（action）"></a>2.2 行动算子（action）</h2><p>这种算子会触发sparkContent提交作业；</p><ul><li>无输出（不生成文件）<ul><li>foreach算子</li></ul></li><li>HDFS<ul><li>saveAsTextFile算子</li><li>saveAsObjectFile算子</li></ul></li><li>scala集合和数据类型<ul><li>collect算子</li><li>collectAsMap算子</li><li>reduceByKeyLocally算子</li><li>lookup算子</li><li>count算子</li><li>top算子</li><li>reduce算子</li><li>fold算子</li><li>aggregate算子</li></ul></li></ul><h1 id="三：常见算子的应用场景"><a href="#三：常见算子的应用场景" class="headerlink" title="三：常见算子的应用场景"></a>三：常见算子的应用场景</h1><h2 id="3-1-转换算子（transformation）"><a href="#3-1-转换算子（transformation）" class="headerlink" title="3.1 转换算子（transformation）"></a>3.1 转换算子（transformation）</h2><h3 id="3-1-1-value型转换算子"><a href="#3-1-1-value型转换算子" class="headerlink" title="3.1.1 value型转换算子"></a>3.1.1 value型转换算子</h3><h4 id="（1）map"><a href="#（1）map" class="headerlink" title="（1）map"></a>（1）map</h4><p>类比mapreduce中的map操作，给定一个输入，由map函数操作后，成为一个新的元素输出；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;Hello&quot;,&quot;Word&quot;,&quot;你好&quot;,&quot;世界&quot;),2)</span><br><span class="line">val second = first.map(_.length)</span><br><span class="line">second.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361528641-4cccbb47-74f0-4033-a43f-a80da85ecadd.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(1 to 5,2)</span><br><span class="line">first.map(1 to _).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361687877-18444583-2032-4606-b1c2-59d7a858da27.png"></p><h4 id="（2）flatMap"><a href="#（2）flatMap" class="headerlink" title="（2）flatMap"></a>（2）flatMap</h4><p>给定一个二维的输入（线式输入），将返回的所有结果打平成一个一维的集合结构（点式集合输出）;</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(1 to 5,2)</span><br><span class="line">first.flatMap(1 to _).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629361925095-d5967335-e5d8-4136-b9dc-40aeaf574e03.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;),2)</span><br><span class="line">first.flatMap(x =&gt;List(x,x,x)).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362079746-1298c815-0023-41a3-9c3c-d3adf7f565c4.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;),2)</span><br><span class="line">first.flatMap(x =&gt; List(x+&quot;_1&quot;,x+&quot;_2&quot;,x+&quot;_3&quot;)).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362222555-086c43f1-848f-4990-9fbd-d369bdb90f42.png"></p><h4 id="（3）mapPartitions"><a href="#（3）mapPartitions" class="headerlink" title="（3）mapPartitions"></a>（3）mapPartitions</h4><p>以分区为单位进行计算处理；</p><p>在map过程中，需要频繁创建额外对象时，如文件输出流操作、jdbc操作、socket操作，使用mapPartitions算子；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Seq(1,2,3,4,5),3)</span><br><span class="line">var rdd2 = rdd.mapPartitions(partition =&gt; &#123;</span><br><span class="line">// 在此处可以加入jdbc一次初始化多少次使用的代码</span><br><span class="line">partition.map(num =&gt; num * num)</span><br><span class="line">&#125;)</span><br><span class="line">rdd2.max</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362591147-55198903-ddac-4824-9735-ad45e0648084.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Seq(1,2,3,4),3)</span><br><span class="line">var rdd2 = rdd.mapPartitions(partition =&gt;&#123;</span><br><span class="line">partition.flatMap(1 to _)</span><br><span class="line">&#125;)</span><br><span class="line">rdd2.count</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362787238-09fde06d-4c8a-4a76-bf4f-81cd63d4888d.png"></p><h4 id="（4）glom"><a href="#（4）glom" class="headerlink" title="（4）glom"></a>（4）glom</h4><p>以分区为单位，将每个分区的值形成一个数组；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(Seq(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;,&quot;four&quot;,&quot;five&quot;),3)</span><br><span class="line">a.glom.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629362965109-f019c760-0cca-48e7-b77e-72a72b865c73.png"></p><p>由上诉得到：分组的依据是平均分组</p><h4 id="（5）union"><a href="#（5）union" class="headerlink" title="（5）union"></a>（5）union</h4><p>将2个rdd合并成一个rdd，不去重；有时可能会发生多个分区合并成一个分区的情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 4,2)</span><br><span class="line">val b = sc.parallelize(6 to 10,2)</span><br><span class="line">a.union(b).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363148863-5e326e0c-8090-44dc-9235-fc502b1b020d.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(a union b).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363185922-78c5a488-f745-423f-ba9f-5a1a8aed72f0.png"></p><h4 id="（6）groupBy"><a href="#（6）groupBy" class="headerlink" title="（6）groupBy"></a>（6）groupBy</h4><p>输入分区和输出分区 n : n型</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(Seq(1,2,3,4,5,56,67),3)</span><br><span class="line">a.groupBy(x =&gt; &#123;if(x&gt;10) &quot;&gt;10&quot; else &quot;&lt;=10&quot;&#125;).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363353742-3a6f1e5a-9e2b-4830-aa7a-6746670268cf.png"></p><h4 id="（7）filter"><a href="#（7）filter" class="headerlink" title="（7）filter"></a>（7）filter</h4><p>输出为输入的子集；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 4,3)</span><br><span class="line">val b = a.filter(_%4 == 0)</span><br><span class="line">b.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363470345-d3527b6f-d8cc-4cda-9075-5295be2fc72f.png"></p><h4 id="（8）distinct"><a href="#（8）distinct" class="headerlink" title="（8）distinct"></a>（8）distinct</h4><p>输出分区为输入分区的子集，全局去重；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,3)</span><br><span class="line">val b = sc.parallelize(2 to 9,3)</span><br><span class="line">a.union(b).distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363723963-576ad83b-d590-475d-81b6-09e20703585d.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val c = sc.parallelize(List(&quot;小红&quot;,&quot;消化&quot;,&quot;不良&quot;,&quot;消化&quot;))</span><br><span class="line">c.distinct.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629363870738-97f17c6b-c2c6-4145-be17-a744e0697ecb.png"></p><h4 id="（9）cache"><a href="#（9）cache" class="headerlink" title="（9）cache"></a>（9）cache</h4><p>cache将rdd元素从磁盘缓存到内存中，数据反复被使用的场景使用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,2)</span><br><span class="line">val b = sc.parallelize(2 to 4,2)</span><br><span class="line">a.union(b).count</span><br><span class="line">a.distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629365250564-1438ac38-78bf-4c84-b7c5-1337feebeeca.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3,2)</span><br><span class="line">val b = sc.parallelize(2 to 5,3)</span><br><span class="line">val c = a.union(b).cache</span><br><span class="line">c.count</span><br><span class="line">c.distinct().collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629365410401-cff04c88-5a41-4aa0-a8cd-3d9bc4f8c045.png"></p><h3 id="3-1-2-key-value型转换算子"><a href="#3-1-2-key-value型转换算子" class="headerlink" title="3.1.2 key-value型转换算子"></a>3.1.2 key-value型转换算子</h3><h4 id="（1）mapValues"><a href="#（1）mapValues" class="headerlink" title="（1）mapValues"></a>（1）mapValues</h4><p>输入分区：输出分区 &#x3D; 1 ： 1</p><p>针对key-value型数据中的value进行map操作，而不对key进行处理；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张三&quot;,1),(&quot;李四&quot;,2),(&quot;王五&quot;,3)),2)</span><br><span class="line">val second = first.mapValues(x =&gt; x+1)</span><br><span class="line">second.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629366240118-49f73664-486d-4acd-a779-46fe1aa28042.png"></p><h4 id="（2）★-combineByKey-★"><a href="#（2）★-combineByKey-★" class="headerlink" title="（2）★ combineByKey ★"></a>（2）★ combineByKey ★</h4><p>定义</p><p>def combineByKey [C] (</p><p>createCombiner: (V) &#x3D;&gt; C,</p><p>mergeValue: (C,V) &#x3D;&gt; C,</p><p>mergeCombiners: (C,C) &#x3D;&gt; C): RDD[(String, C)]</p><p>元素</p><p>createCombiner对每个分区内的同组元素如何聚合，形成一个累加器</p><p>mergeValue 将累加器与新遇到的值进行合并的方法</p><p>mergeCombiners 每个分区都是独立处理，故同一个键可以有多个累加器。如果两个或者多个分区都对应同一个键的累加器，用方法将各个分区的结果进行合并。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张一&quot;,4),(&quot;张一&quot;,2),(&quot;张三&quot;,3)),2)</span><br><span class="line">val second =first.combineByKey(List(_), (x:List[Int],y:Int) =&gt; y::x, (x:List[Int], y:List[Int]) =&gt; x:::y)</span><br><span class="line">second.collect </span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629367854547-9a9fa874-7f66-4d7d-a216-a511bfb5b502.png"></p><h4 id="（3）reduceByKey"><a href="#（3）reduceByKey" class="headerlink" title="（3）reduceByKey"></a>（3）reduceByKey</h4><p>按key聚合后对组进行规约处理，求和</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;小米&quot;,&quot;华硕&quot;,&quot;很郁闷&quot;),2)</span><br><span class="line">val second = first.map(x =&gt; (x,1))</span><br><span class="line">second.reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629370722139-e07aa367-7b31-4818-a81e-fb58087606bf.png"></p><h4 id="（4）join"><a href="#（4）join" class="headerlink" title="（4）join"></a>（4）join</h4><p>对 key-value 结构的rdd进行按key的join操作，最后将V部分做flatMap打平操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张三&quot;,11),(&quot;李四&quot;,12)),2)</span><br><span class="line">val seconed = sc.parallelize(List((&quot;张一&quot;,2),(&quot;李二&quot;,3),(&quot;李四&quot;,3)),3)</span><br><span class="line">first.join(seconed).collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629371167534-5d370cc0-f9b8-4785-a30a-f89d51d0d802.png"></p><h2 id="3-2-行动算子-action"><a href="#3-2-行动算子-action" class="headerlink" title="3.2 行动算子 action"></a>3.2 行动算子 action</h2><p>该类型算子会触发SparkContext提交作业，触发RDD DAG的执行</p><h4 id="（1）无输出型，不落地本地文件或hsfs文件"><a href="#（1）无输出型，不落地本地文件或hsfs文件" class="headerlink" title="（1）无输出型，不落地本地文件或hsfs文件"></a>（1）无输出型，不落地本地文件或hsfs文件</h4><p>foreach算子</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;华米&quot;,&quot;小米&quot;,&quot;苹果&quot;,&quot;华米&quot;,&quot;三星&quot;),2)</span><br><span class="line">first.foreach(println _)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629371520895-6d080f91-c686-4d4f-9dfa-514612d51c7e.png"></p><h4 id="（2）HDFS输出型"><a href="#（2）HDFS输出型" class="headerlink" title="（2）HDFS输出型"></a>（2）HDFS输出型</h4><p>saveAsTextFile算子</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;华为&quot;,&quot;三星&quot;,&quot;华为&quot;),2)</span><br><span class="line">// 指定本地保存的目录(不存在的目录)</span><br><span class="line">first.saveAsTextFile(&quot;file:///home/shijiaxin/spark_output5&quot;)</span><br><span class="line">=====================================</span><br><span class="line">===        BUG               ========</span><br><span class="line">=====================================</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629378669316-8304be2d-269b-453c-9b8b-7e7f16265a81.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">=================killed==================</span><br><span class="line"></span><br><span class="line">// 指定保存hdfs保存目录，默认路径hdfs中/user/当前用户</span><br><span class="line">first.saveAsTextFile(&quot;spark_shell_output_txt&quot;)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629372362391-1dfc078a-4ead-4c9d-8ffa-6f748a04ef61.png"></p><h4 id="（3）scala集合和数据类型"><a href="#（3）scala集合和数据类型" class="headerlink" title="（3）scala集合和数据类型"></a>（3）scala集合和数据类型</h4><p>collect算子</p><p>相当于toArray操作，将分布式RDD返回成为一个scala array数组结果，实际是Driver所在的机器节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;, &quot;华为&quot;, &quot;花粉&quot;,  &quot;苹果&quot; ), 2)</span><br><span class="line">first.collect</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629373371312-99b79ad2-7b62-4fbe-89d1-470a492d60ec.png"></p><h4 id="（4）collectAsMap算子"><a href="#（4）collectAsMap算子" class="headerlink" title="（4）collectAsMap算子"></a>（4）collectAsMap算子</h4><p>相当于 toMap 操作，将分布式 RDD的 kv 对形式，返回成为一个 scala map集合，实际上Driver所在的机器节点，在处理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List((&quot;张一&quot;,1),(&quot;礼仪&quot;,1),(&quot;张一&quot;,3),(&quot;白虎&quot;,3)),2)</span><br><span class="line">first.collectAsMap</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629373634686-a0154efc-0222-4cae-9f7a-6b288f90d852.png"></p><h4 id="（5）lookup算子"><a href="#（5）lookup算子" class="headerlink" title="（5）lookup算子"></a>（5）lookup算子</h4><p>对键值对类型的 RDD操作，返回指定key对应的元素形成的Seq</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val first = sc.parallelize(List(&quot;小米&quot;,&quot;爱华为&quot;,&quot;爱尔兰&quot;,&quot;cs&quot;),2)</span><br><span class="line">val second = first.map(x =&gt; (&#123;if (x.contains(&quot;爱&quot;)) &quot;有爱&quot; else &quot;无爱&quot;&#125;, x))</span><br><span class="line">second.lookup(&quot;有爱&quot;)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629373958406-9e27c6d8-7109-47ca-a073-30376d225f0d.png"></p><h4 id="（6）reduce算子"><a href="#（6）reduce算子" class="headerlink" title="（6）reduce算子"></a>（6）reduce算子</h4><p>先对两个元素进行reduce函数操作，然后将结果和迭代器取出的下一个元素进行reduce函数操作，直到迭代器遍历完所有元素,得到最后结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(1 to 3, 3)</span><br><span class="line">a.reduce(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629375707373-e049fe20-3095-484c-88c9-214f8c84ea39.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val a = sc.parallelize(List((&quot;one&quot;,1),(&quot;two&quot;,2),(&quot;three&quot;,3)),3)（最后的3 代表3个分区）</span><br><span class="line">val a = sc.parallelize(List((&quot;one&quot;,1),(&quot;two&quot;,2),(&quot;three&quot;,3)),2)</span><br><span class="line">a.reduce( (x,y) =&gt;(&quot;sum&quot;, x._2 + y._3))._2  // 此处不能为3（代表元组的第二列）</span><br><span class="line">a.reduce( (x,y)=&gt;(&quot;sum&quot;,x._2+y._2))._2</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/1629376091510-d07bf1c2-a468-4864-b43f-8c28f45d46e7.png"></p><h4 id="（7）fold算子fold算子："><a href="#（7）fold算子fold算子：" class="headerlink" title="（7）fold算子fold算子："></a>（7）fold算子fold算子：</h4><p>def fold (zeroValue: T)(op: (T, T) &#x3D;&gt; T) : T</p><p>先对rdd分区的每一个分区进行op函数</p><p>在调用op函数过程中将zeroValue参与计算</p><p>最后在对所有分区的结果调用op函数，同事在此处进行zeroValue再次参与计算</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是41 公式：（1+2+3+4+5+6+10）+10</span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),1).fold(10)(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/image-20230221120323612.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是51 公式：（1+2+3+10）+（4+5+6+10）+10 </span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),2).fold(10)(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/image-20230221120533230.png"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 和是61 公式：（1+2+10）+（3+4+10）+（5+6+10）+10 </span><br><span class="line">sc.parallelize(List(1,2,3,4,5,6),3).fold(10)(_+_)</span><br></pre></td></tr></table></figure><p><img src="/blog/bda70396.html/image-20230221120553144.png"></p><h1 id="四：其他常用算子"><a href="#四：其他常用算子" class="headerlink" title="四：其他常用算子"></a>四：其他常用算子</h1><ul><li>cartesian算子</li><li>subtract算子</li><li>sample算子</li><li>takeSample算子</li><li>persist算子</li><li>cogroup算子</li><li>leftOuterJoin算子</li><li>rightOuterJoin算子</li><li>saveAsObjectFile算子</li><li>count算子</li><li>top算子</li><li>aggregate算子</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：算子概述&quot;&gt;&lt;a href=&quot;#一：算子概述&quot; class=&quot;headerlink&quot; title=&quot;一：算子概述&quot;&gt;&lt;/a&gt;一：算子概述&lt;/h1&gt;&lt;h2 id=&quot;1-1-什么是算子？&quot;&gt;&lt;a href=&quot;#1-1-什么是算子？&quot; class=&quot;headerli</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Flink 架构设计</title>
    <link href="https://aiyingke.cn/blog/50ac08de.html/"/>
    <id>https://aiyingke.cn/blog/50ac08de.html/</id>
    <published>2023-02-19T17:27:27.000Z</published>
    <updated>2023-02-19T18:36:19.393Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Flink-架构设计图"><a href="#一：Flink-架构设计图" class="headerlink" title="一：Flink 架构设计图"></a>一：Flink 架构设计图</h1><p><img src="/blog/50ac08de.html/1630318437801-84613c7b-a14d-4311-b66b-9b3ac56824bd.png"></p><h2 id="1-1-分层设计说明"><a href="#1-1-分层设计说明" class="headerlink" title="1.1 分层设计说明"></a>1.1 分层设计说明</h2><ul><li>物理部署层 -deploy层<ul><li>负责解决Flink的部署模式问题</li><li>支持多种部署模式：本地部署、集群部署（standalone&#x2F;yarn&#x2F;mesos）、云（GCE&#x2F;EC2）以及 kubernetes 。</li><li>通过该层支持不同平台的部署，用户可以根据自身场景和需求使用对应的部署模式；</li></ul></li><li>Runtime核心层<ul><li>是Flink分布式计算框架的核心实现层，负责对上层不同接口提供基础服务。</li><li>支持分布式steam作业的执行、jobGraph到ExecutionGraph的映射转换以及任务调度等。</li><li>将DataStream和DataSet转成统一的可执行的Task Operator，达到在流式计算引擎下同时处理批量计算和流式计算的目的。</li></ul></li><li>API &amp; Libraries 层<ul><li>负责更好的开发用户体验，包括易用性、开发效率、执行效率、状态管理等方面。</li><li>Flink同时提供了支撑流计算和批处理的接口，同时在这基础上抽象出不同的应用类型的组件库，如：<ul><li>基于流处理的CEP（复杂事件处理库）</li><li>Table &amp; Sql库</li><li>基于批处理的FlinkML（机器学习库）</li><li>图处理库(Gelly)</li></ul></li><li>API层包括两部分<ul><li>流计算应用的DataStream API</li><li>批处理应用的DataSet API</li><li>统一的API，方便用于直接操作状态和时间等底层数据<ul><li>提供了丰富的数据处理高级API，例如Map、FllatMap操作等</li><li>并提供了比较低级的Process Function API</li></ul></li></ul></li></ul></li></ul><h1 id="二：运行模式"><a href="#二：运行模式" class="headerlink" title="二：运行模式"></a>二：运行模式</h1><h2 id="2-1-运行模式核心区分点"><a href="#2-1-运行模式核心区分点" class="headerlink" title="2.1 运行模式核心区分点"></a>2.1 运行模式核心区分点</h2><ul><li>集群生命周期和资源隔离保证</li><li>应用程序的main()方法是在客户端还是在集群上执行</li></ul><h2 id="2-2-所有模式分类说明"><a href="#2-2-所有模式分类说明" class="headerlink" title="2.2 所有模式分类说明"></a>2.2 所有模式分类说明</h2><ul><li>本地运行模式</li><li>standalone模式</li><li>集群运行模式<ul><li>经常是指flink on yarn集群模式</li><li>yarn也可以换成mesos,Kubernetes(k8s)等资源管理平台替换</li><li>共三种<ul><li>session 模式</li><li>per-job 模式</li><li>application 模式</li></ul></li></ul></li></ul><h2 id="2-3-本地运行模式"><a href="#2-3-本地运行模式" class="headerlink" title="2.3 本地运行模式"></a>2.3 本地运行模式</h2><ul><li>运行过程：一个机器启动一个进程的多线程来模拟分布式计算。</li><li>主要用于代码测试</li></ul><h2 id="2-4-standalone模式"><a href="#2-4-standalone模式" class="headerlink" title="2.4 standalone模式"></a>2.4 standalone模式</h2><ul><li>运行过程：完全独立的Flink集群的模式，各个环节均Flink自己搞定。并没有yarn、mesos的统一资源调度平台。</li><li>主要是只有纯Flink纯计算的场景，商用场景极少。</li></ul><h2 id="2-5-集群运行模式"><a href="#2-5-集群运行模式" class="headerlink" title="2.5 集群运行模式"></a>2.5 集群运行模式</h2><h3 id="（1）Flink-Session-集群（会话模式）"><a href="#（1）Flink-Session-集群（会话模式）" class="headerlink" title="（1）Flink Session 集群（会话模式）"></a>（1）Flink Session 集群（会话模式）</h3><ul><li><ul><li>集群生命周期：<ul><li>在 Flink Session 集群中，客户端连接到一个预先存在的、长期运行的集群；</li><li>该集群可以接受多个作业提交。即使所有作业完成后，集群（和 JobManager）仍将继续运行直到手动停止 session 为止。</li><li>因此，<code>Flink Session 集群的寿命不受任何 Flink 作业寿命的约束</code>。</li></ul></li><li>TaskManager slot 由 ResourceManager 在提交作业时分配，并在作业完成时释放。<ul><li>由于所有作业都共享同一集群，因此在集群资源方面存在一些竞争——例如提交工作阶段的网络带宽。</li><li>此共享设置的局限性在于，如果 TaskManager 崩溃，则在此 TaskManager 上运行 task 的所有作业都将失败；</li><li>再比如，如果 JobManager 上发生一些致命错误，它将影响集群中正在运行的所有作业。</li></ul></li><li>其他注意事项：<ul><li>拥有一个预先存在的集群可以节省大量时间申请资源和启动 TaskManager。</li><li>有种场景很重要，作业执行时间短并且启动时间长会对端到端的用户体验产生负面的影响。<ul><li>就像对简短查询的交互式分析一样，希望作业可以使用现有资源快速执行计算。</li></ul></li></ul></li><li>Flink Session 集群也被称为 session 模式下的 Flink 集群。</li><li>工作模式<ul><li>附加模式（默认）<ul><li>特点<ul><li>客户端与Flink作业集群相互同步</li></ul></li><li>细节描述<ul><li>yarn-session.sh 客户端将 Flink 集群提交给 YARN，但客户端保持运行，跟踪集群的状态。</li><li>如果集群失败，客户端将显示错误。如果客户端被终止，它也会通知集群关闭。</li></ul></li></ul></li><li>分离模式（-d或–detached）<ul><li>特点<ul><li>客户端与Flink作业集群相互异步，客户端提交完成后即可退出</li></ul></li><li>细节描述<ul><li>yarn-session.sh 客户端将Flink集群提交给YARN，然后客户端返回。</li><li>需要再次调用客户端或 YARN 工具来停止 Flink 集群。</li></ul></li></ul></li></ul></li><li>工作流程特征说明<ul><li>多个不同的FlinkJob向同一个Flink Session会话上提交作业，由这一个统一个的FlinkSession管理所有的Flink作业。</li><li>工作流程示意图</li></ul></li></ul></li><li><p><img src="/blog/50ac08de.html/1630323312227-b12cae9d-16c2-47b1-a1cf-e697d719c689.png"></p></li></ul><h3 id="（2）Flink-Job-集群（per-job模式）"><a href="#（2）Flink-Job-集群（per-job模式）" class="headerlink" title="（2）Flink Job 集群（per-job模式）"></a>（2）Flink Job 集群（per-job模式）</h3><ul><li>Flink Job 集群也被称为 job (or per-job) 模式下的 Flink 集群。</li><li>集群生命周期：<ul><li>在 Flink Job 集群中，可用的集群管理器（例如 YARN）用于为每个提交的作业启动一个集群，并且该集群仅可用于该作业。</li><li>在这里客户端首先从集群管理器请求资源启动 JobManager，然后将作业提交给在这个进程中运行的 Dispatcher。然后根据作业的资源请求惰性的分配 TaskManager。</li><li>一旦作业完成，Flink Job 集群将被拆除。</li></ul></li><li>资源隔离：<ul><li>JobManager 中的致命错误仅影响在 Flink Job 集群中运行的一个作业。</li></ul></li><li>其他注意事项：<ul><li>由于 ResourceManager 必须应用并等待外部资源管理组件来启动 TaskManager 进程和分配资源，所以其实时计算性并没有session模式强</li><li>因此 Flink Job 集群更适合长期运行、具有高稳定性要求且对较长的启动时间不敏感的大型作业；</li></ul></li><li>工作流程特征说明：<ul><li>多个不同的FlinkJob向各自由统一资源管理器(Yarn)生成的专用Flink Session会话上提交作业，由作业所属的FlinkSession管理自己的Flink作业。</li><li>工作流程示意图</li></ul></li></ul><p><img src="/blog/50ac08de.html/1630323597224-1c423290-947a-44b4-9101-7adce28b5202.png"></p><h3 id="（3）Flink-Application-集群（应用模式）"><a href="#（3）Flink-Application-集群（应用模式）" class="headerlink" title="（3）Flink Application 集群（应用模式）"></a>（3）Flink Application 集群（应用模式）</h3><ul><li>Flink Job 集群可以看做是 Flink Application 集群”客户端运行“的替代方案。</li><li>该模式为yarn session和yarn per-job模式的折中选择。</li><li>集群生命周期：<ul><li>Flink Application 集群是与Flink作业执行直接相关的运行模式，并且 <code>main()方法在集群上而不是客户端上运行</code>。</li><li>提交作业是一个单步骤过程：<ul><li>无需先启动 Flink 集群，然后将作业提交到现有的 session 集群。</li><li>将应用程序逻辑和依赖打包成一个可执行的作业 JAR 中，由入口机客户端提交jar包和相关资源到hdfs当中。</li><li>由调度启动的集群当中JobManager来拉取已由上一步上传完成的运行代码所需要的所有资源。</li><li>如果有JobManager HA设置的话，将会同时启动多个JobManager作HA保障，此时将面临JobManager的选择，最终由一个胜出的JobManager作为Active进程推进下边的执行。</li><li>由运行JobManager进程的集群入口点节点机器（ApplicationClusterEntryPoint）负责调用 main()方法来提取 JobGraph，即作为用户程序的解析和提交的客户端程序与集群进行交互，直到作业运行完成。</li><li>如果一个main()方法中有多个env.execute()&#x2F;executeAsync()调用，在Application模式下，这些作业会被视为属于同一个应用，在同一个集群中执行（如果在Per-Job模式下，就会启动多个集群）</li><li>该模式也允许我们像在 Kubernetes 上部署任何其他应用程序一样部署 Flink 应用程序。</li><li>因此，Flink Application 集群的寿命与 Flink 应用程序的寿命有关。</li></ul></li></ul></li><li>资源隔离：<ul><li>在 Flink Application 集群中，ResourceManager 和 Dispatcher 作用于单个的 Flink 应用程序，相比于 Flink Session 集群，它提供了更好的隔离。</li></ul></li><li>工作流程特征说明：<ul><li>将各个环节更进一步进行专用化处理，相当于每个FlinkJob都有一套专用的服务角色进程。</li></ul></li></ul><h3 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h3><ul><li>应用场景<ul><li>本地布署模式：demo、代码测试场景。</li><li>Session模式：集群资源充分、频繁任务提交、小作业居多、实时性要求高的场景。(该模式较少）</li><li>Per-Job模式：作业少、大作业、实时性要求低的场景。</li><li>Application模式：实时性要求不太高、安全性有一定要求均可以使用，普遍适用性最强。</li></ul></li><li>生产环境使用说明<ul><li>建议用per-job或是application模式，提供了更好的资源隔离性和安全性。</li></ul></li></ul><h1 id="三：运行流程"><a href="#三：运行流程" class="headerlink" title="三：运行流程"></a>三：运行流程</h1><h2 id="3-1-工作流程图"><a href="#3-1-工作流程图" class="headerlink" title="3.1 工作流程图"></a>3.1 工作流程图</h2><p><img src="/blog/50ac08de.html/1630324699314-e7922476-6b00-4315-894e-e9d794163070.png"></p><h2 id="3-2-运行时核心角色组成"><a href="#3-2-运行时核心角色组成" class="headerlink" title="3.2 运行时核心角色组成"></a>3.2 运行时核心角色组成</h2><ul><li>由两种类型的进程组成，一个 JobManager 和一个或者多个 TaskManager。</li><li>Client 客户端不是运行时和程序执行的一部分，而是用于准备数据流并将其发送给 JobManager。</li><li>提交任务完成之后，Client可以断开连接（分离模式），或保持连接来接收进程报告（附加模式）。</li><li>Client可以作为触发执行 Java&#x2F;Scala 程序的一部分运行，也可以在命令行进程.&#x2F;bin&#x2F;flink run …中运行。</li><li>可以通过多种方式启动 JobManager 和 TaskManager：直接在机器上作为standalone 集群启动、在容器中启动、或者通过YARN或Mesos等资源框架管理并启动。TaskManager 连接到 JobManagers，宣布自己可用，并被分配工作。</li><li>Actor System<ul><li>各个角色组件互相通信的消息传递系统中间件。</li><li>Actor是一种并发编程模型，与另一种模型共享内存完全相反，Actor模型share nothing，即没有任何共享。</li><li>所有的线程(或进程)通过消息传递的方式进行合作(通信)，这些线程(或进程)称为Actor。</li><li>以其简单、高效著称</li><li>缺点<ul><li>唯一的缺点是不能实现真正意义上的并行， 而是通过并发实现的并行效果，会存在一定的不确定性。</li><li>纯消息通信，实时性和粒度控制上会略弱于共享内存的方式。</li></ul></li></ul></li></ul><h2 id="3-3-核心组成角色剖析"><a href="#3-3-核心组成角色剖析" class="headerlink" title="3.3 核心组成角色剖析"></a>3.3 核心组成角色剖析</h2><ul><li>JobManager<ul><li>JobManager 具有许多与协调 Flink 应用程序的分布式执行有关的职责：<ul><li>它决定何时调度下一个 task（或一组 task）、对完成的 task 或执行失败做出反应、协调 checkpoint、并且协调从失败中恢复等等。这个进程由三个不同的组件组成：<ul><li>ResourceManager<ul><li>ResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的最小单位。Flink 为不同的环境和资源提供者（例如 YARN、Mesos、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。</li></ul></li><li>Dispatcher<ul><li>Dispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。</li></ul></li><li>JobMaster<ul><li>JobMaster 负责管理单个JobGraph的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。</li><li>始终至少有一个 JobManager。高可用（HA）设置中可能有多个 JobManager，其中一个始终是 leader，其他的则是 standby。</li></ul></li></ul></li></ul></li></ul></li><li>TaskManager<ul><li>TaskManager（也称为 worker）执行作业流的 task，并且缓存和交换数据流。</li><li>必须始终至少有一个 TaskManager。在 TaskManager 中资源调度的最小单位是 task slot。</li><li>TaskManager 中 task slot 的数量表示并发处理 task 的数量。请注意一个 task slot 中可以执行多个算子。</li></ul></li></ul><h2 id="3-4-yarn模式提交任务的工作流程"><a href="#3-4-yarn模式提交任务的工作流程" class="headerlink" title="3.4 yarn模式提交任务的工作流程"></a>3.4 yarn模式提交任务的工作流程</h2><ul><li>工作流程图：</li></ul><p><img src="/blog/50ac08de.html/1630325042367-d709bf3d-24b0-4907-9f10-64693abdf588.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Flink-架构设计图&quot;&gt;&lt;a href=&quot;#一：Flink-架构设计图&quot; class=&quot;headerlink&quot; title=&quot;一：Flink 架构设计图&quot;&gt;&lt;/a&gt;一：Flink 架构设计图&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/50ac08de.h</summary>
      
    
    
    
    <category term="Flink" scheme="https://aiyingke.cn/categories/Flink/"/>
    
    
    <category term="Flink" scheme="https://aiyingke.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 概述</title>
    <link href="https://aiyingke.cn/blog/72cd4c87.html/"/>
    <id>https://aiyingke.cn/blog/72cd4c87.html/</id>
    <published>2023-02-19T16:45:05.000Z</published>
    <updated>2023-02-19T17:27:46.943Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：产生背景"><a href="#一：产生背景" class="headerlink" title="一：产生背景"></a>一：产生背景</h1><h2 id="1-1-历史背景"><a href="#1-1-历史背景" class="headerlink" title="1.1 历史背景"></a>1.1 历史背景</h2><ul><li>随着互联网应用的快速发展，<code>实时流数据产生日益增多和普遍化</code>。如日常生活、金融、驾驶、LBS、电商等众多领域。</li><li>实时数据的处理和挖掘能够带来离线数据处理和挖掘<code>更多的社会发展和商业价值</code>。</li><li>如何快速响应和处理这些大规模的实时数据流，成为众多互联网大厂的当务之急。</li><li>在flink之前也出现了很多<code>流数据处理引擎</code>，包括storm、sparkstreaming等知名流行框架，但各自均有较明显的不足，导致没有达到理想的流处理引擎的标准要求。</li></ul><h2 id="1-2-优秀的流处理引擎"><a href="#1-2-优秀的流处理引擎" class="headerlink" title="1.2 优秀的流处理引擎"></a>1.2 优秀的流处理引擎</h2><ul><li>优秀流处理引擎标准要求<ul><li>低延迟、高吞吐量、容错性、窗口时间语义化、编程效率高与运行效果好的用户体验；</li></ul></li><li>storm<ul><li>优点：低延迟</li><li>缺点：其它要求都较差一些</li></ul></li><li>sparkstreaming<ul><li>优点：高吞吐量、容错性高</li><li>缺点：其它要求都较差一些</li></ul></li></ul><h1 id="二：基本介绍"><a href="#二：基本介绍" class="headerlink" title="二：基本介绍"></a>二：基本介绍</h1><h2 id="2-1-概念说明"><a href="#2-1-概念说明" class="headerlink" title="2.1 概念说明"></a>2.1 概念说明</h2><ul><li>由Apache软件基金会开发的开源流处理框架</li><li>其核心是用Java和Scala编写的框架和分布式处理引擎</li><li>用于对无界和有界数据流进行有状态计算。<ul><li>无界数据流: 即为实时流数据；</li><li>有界数据流：即为离线数据，也称为批处理数据；</li></ul></li></ul><h2 id="2-2-特点特征"><a href="#2-2-特点特征" class="headerlink" title="2.2 特点特征"></a>2.2 特点特征</h2><ul><li>被设计为在所有常见的集群环境中运行，以内存速度和任何规模执行计算。</li><li>能够达到实时流处理引擎的全部标准要求。<ul><li>低延迟、高吞吐量、容错性、窗口时间语义化、编程效率高与运行效果好的用户体验；</li></ul></li></ul><h1 id="三：应用场景"><a href="#三：应用场景" class="headerlink" title="三：应用场景"></a>三：应用场景</h1><h2 id="3-1-官方说明"><a href="#3-1-官方说明" class="headerlink" title="3.1 官方说明"></a>3.1 官方说明</h2><ul><li>事件驱动型应用</li><li>数据分析型应用</li><li>数据管道 ETL</li></ul><h2 id="3-2-实际情况"><a href="#3-2-实际情况" class="headerlink" title="3.2 实际情况"></a>3.2 实际情况</h2><ul><li>要求严格的实时流处理场景</li></ul><h1 id="四：代码实现"><a href="#四：代码实现" class="headerlink" title="四：代码实现"></a>四：代码实现</h1><h2 id="4-1-实现方式"><a href="#4-1-实现方式" class="headerlink" title="4.1 实现方式"></a>4.1 实现方式</h2><ul><li>Java API</li><li>Scala API</li></ul><h2 id="4-2-统一数据处理过程抽象"><a href="#4-2-统一数据处理过程抽象" class="headerlink" title="4.2 统一数据处理过程抽象"></a>4.2 统一数据处理过程抽象</h2><ul><li>将实时和批处理的数据过程，均抽象成三个过程，即Source-&gt;Transform-&gt;Sink。<ul><li>Source为源数据读入，即Source算子。</li><li>Transform是数据转换处理过程，即Transform算子。</li><li>Sink即数据接收器，即数据落地到存储层，即Sink算子。</li></ul></li><li>代码实现复杂度<ul><li>丰富的API和算子操作；</li><li>抽象封装统一性较高，支持类SQL编程，编程复杂度并不高。</li></ul></li></ul><h1 id="五：版本发展"><a href="#五：版本发展" class="headerlink" title="五：版本发展"></a>五：版本发展</h1><table><thead><tr><th><strong>版本</strong></th><th><strong>发行日期</strong></th><th><strong>备注</strong></th></tr></thead><tbody><tr><td><strong>Flink 0.6-incubating</strong></td><td><strong>2014-08-26</strong></td><td><strong>初步得到团队内部认可，正在快速迭代中</strong></td></tr><tr><td><strong>Flink 0.9.0-milestone</strong></td><td><strong>2015-04-13</strong></td><td><strong>有重大进展，得正式对外发布，</strong></td></tr><tr><td><strong>0.9</strong></td><td><strong>2015-09-01</strong></td><td><strong>从此时开始引入阿里巴巴，并成为阿里系主干业务实时流处理引擎，内部改良优化后命名为blink</strong></td></tr><tr><td>0.10</td><td>2016-02-11</td><td></td></tr><tr><td><strong>1.0</strong></td><td><strong>2016-05-11</strong></td><td><strong>很有里程碑、代表性的一个版本</strong></td></tr><tr><td>1.1</td><td>2017-03-22</td><td></td></tr><tr><td>1.2</td><td>2017-04-26</td><td></td></tr><tr><td>1.3</td><td>2018-03-15</td><td></td></tr><tr><td>1.4</td><td>2018-03-08</td><td></td></tr><tr><td>1.5</td><td>2018-10-29</td><td></td></tr><tr><td>1.6</td><td>2018-10-29</td><td></td></tr><tr><td><strong>1、在2019年初，blink在阿里内部经过多年的商用实践，增加了N多新特性，并得到广泛应用和成熟化，正式对外开源，并捐赠给Apache Flink社区，并成为其下的分支方式开源并逐步融合后，依然以Flink为主进一步推进开源进程。****2、阿里系以9000万欧元收购了创业公司 Data Artisans，即Flink的开发团队所属公司。</strong></td><td></td><td></td></tr><tr><td>1.7</td><td>2019-02-15</td><td></td></tr><tr><td>1.8</td><td>2019-04-09</td><td></td></tr><tr><td><strong>1.9</strong></td><td><strong>2019-08-19</strong></td><td><strong>目前市占率较高的一个版本</strong></td></tr><tr><td>1.10</td><td>2020-02-11</td><td></td></tr><tr><td>1.11</td><td>2020-07-06</td><td><strong>从此版本开始，加入很多新特性，支持hadoop3.x版本</strong></td></tr><tr><td>1.12</td><td>2020-12-08</td><td></td></tr><tr><td>Flink 1.13.0</td><td>2021-04-30</td><td></td></tr><tr><td><strong>Flink 1.13.1</strong></td><td><strong>2021-05-28</strong></td><td><strong>版本迭代很快，社区很活跃，发展非常快****已是稳定版。</strong></td></tr></tbody></table><ul><li>Flink版本在早期就得到阿里认可，并进行集团内部孵化和二次开发、商用实践，命名为Blink。</li><li>Blink的主要贡献是在用户体验上，包括SQL、webUI等方面。</li><li>在2019年进行了开源反馈给社区，从此更多的是以Flink merge Blink新功能后，以Flink为主继续推进开源。</li><li>基于市场量、成熟度、社区丰富度等方面，通常选择1.13.1版本。</li></ul><h1 id="六：市场前景"><a href="#六：市场前景" class="headerlink" title="六：市场前景"></a>六：市场前景</h1><ul><li>现实情况<ul><li>学习成本较高、应用场景较垂直，其实际开发者在市场上是比较衡缺的。</li><li>相对于更广大的中小型公司，Flink的使用量最主要是集中在中大型互联网科技公司。</li></ul></li><li>发展趋势<ul><li>商业市场、各种大型IT企业对大规模实时数据场景需求旺盛。</li><li>Flink在实时数据处理方面的架构设计与商用实践表现较为突出。</li><li>得到阿里系的商业收购+大规模人力财力物力的支持，未来发展不可限量。</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：产生背景&quot;&gt;&lt;a href=&quot;#一：产生背景&quot; class=&quot;headerlink&quot; title=&quot;一：产生背景&quot;&gt;&lt;/a&gt;一：产生背景&lt;/h1&gt;&lt;h2 id=&quot;1-1-历史背景&quot;&gt;&lt;a href=&quot;#1-1-历史背景&quot; class=&quot;headerlink&quot; </summary>
      
    
    
    
    <category term="Flink" scheme="https://aiyingke.cn/categories/Flink/"/>
    
    
    <category term="Flink" scheme="https://aiyingke.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Spark 交互操作</title>
    <link href="https://aiyingke.cn/blog/75610dc7.html/"/>
    <id>https://aiyingke.cn/blog/75610dc7.html/</id>
    <published>2023-02-19T07:25:39.000Z</published>
    <updated>2023-02-19T08:01:27.442Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Spark-运行模式"><a href="#一：Spark-运行模式" class="headerlink" title="一：Spark 运行模式"></a>一：Spark 运行模式</h1><p>即作业以什么样的模式去执行，主要是单机、分布式两种方式的细节选择。</p><table><thead><tr><th><strong>序号</strong></th><th align="center"><strong>模式名称</strong></th><th align="center"><strong>特点</strong></th><th align="center"><strong>应用场景</strong></th></tr></thead><tbody><tr><td>1</td><td align="center">本地运行模式(local)</td><td align="center">单台机器多线程来模拟spark分布式计算</td><td align="center">机器资源不够测试验证程序逻辑的正确性</td></tr><tr><td>2</td><td align="center">伪分布式模式</td><td align="center">单台机器多进程来模拟spark分布式计算</td><td align="center">机器资源不够测试验证程序逻辑的正确性</td></tr><tr><td>3</td><td align="center">standalone(client)</td><td align="center">独立布署spark计算集群自带clustermanagerdriver运行在spark submit client端</td><td align="center">机器资源充分纯用spark计算框架任务提交后在spark submit   client端实时查看反馈信息数据共享性弱测试使用还可以，生产环境极少使用该种模式</td></tr><tr><td>4</td><td align="center">standalone(cluster)</td><td align="center">独立布署spark计算集群自带clustermanagerdriver运行在spark worker node端</td><td align="center">机器资源充分纯用spark计算框架任务提交后将退出spark   submit client端数据共享性弱测试和生产环境均可以自由使用，但更多用于生产环境</td></tr><tr><td>5</td><td align="center">spark on yarn(yarn-client)</td><td align="center">以yarn集群为基础只添加spark计算框架相关包driver运行在yarn client上</td><td align="center">机器资源充分多种计算框架混用数据共享性强任务提交后在yarn client端实时查看反馈信息</td></tr><tr><td>6</td><td align="center">spark on yarn(yarn-cluster)</td><td align="center">以yarn集群为基础只添加spark计算框架相关包driver运行在集群的am contianer中</td><td align="center">机器资源充分多种计算框架混用数据共享性强任务提交后将退出yarn client端</td></tr><tr><td>7</td><td align="center">spark on mesos&#x2F;ec2</td><td align="center">与spark on yarn类似</td><td align="center">与spark on yarn类似在国内应用较少</td></tr></tbody></table><h1 id="二：Spark-用户交互方式"><a href="#二：Spark-用户交互方式" class="headerlink" title="二：Spark 用户交互方式"></a>二：Spark 用户交互方式</h1><ol><li>spark-shell：spark命令行方式来操作spark作业。<ul><li>多用于简单的学习、测试、简易作业操作。</li></ul></li><li>spark-submit：通过程序脚本，提交相关的代码、依赖等来操作spark作业。<ul><li>最多见的提交任务的交互方式，简单易用、参数齐全。</li></ul></li><li>spark-sql：通过sql的方式操作spark作业。<ul><li>sql相关的学习、测试、生产环境研发均可以使用该直接操作交互方式。</li></ul></li><li>spark-class：最低层的调用方式，其它调用方式多是最终转化到该方式中去提交。<ul><li>直接使用较少</li></ul></li><li>sparkR、sparkPython：通过其它非java、非scala语言直接操作spark作业的方式。<ul><li>R、python语言使用者的交互方式。</li></ul></li></ol><h1 id="三：Spark-Shell-操作"><a href="#三：Spark-Shell-操作" class="headerlink" title="三：Spark-Shell 操作"></a>三：Spark-Shell 操作</h1><h2 id="3-1-交互方式定位"><a href="#3-1-交互方式定位" class="headerlink" title="3.1 交互方式定位"></a>3.1 交互方式定位</h2><ul><li>一个强大的交互式数据操作与分析的工具，提供一个简单的方式快速学习spark相关的API。</li></ul><h2 id="3-2-启动方式"><a href="#3-2-启动方式" class="headerlink" title="3.2 启动方式"></a>3.2 启动方式</h2><ul><li>前置环境：已将spark-shell等交互式脚本已加入系统PATH变量，可在任意位置使用。</li><li>以本地2个线程来模拟运行spark相关操作，该数量一般与本机的cpu核数相一致为最佳spark-shell –master local[2]</li></ul><h2 id="3-3-相关参数"><a href="#3-3-相关参数" class="headerlink" title="3.3 相关参数"></a>3.3 相关参数</h2><ul><li>参数列表获取方式：spark-shell –help</li></ul><h1 id="四：Spark-submit-操作"><a href="#四：Spark-submit-操作" class="headerlink" title="四：Spark-submit 操作"></a>四：Spark-submit 操作</h1><h2 id="4-1-交互方式定位"><a href="#4-1-交互方式定位" class="headerlink" title="4.1 交互方式定位"></a>4.1 交互方式定位</h2><ul><li>最常用的通过程序脚本，提交相关的代码、依赖等来操作spark作业的方式。</li></ul><h2 id="4-2-启动方式"><a href="#4-2-启动方式" class="headerlink" title="4.2 启动方式"></a>4.2 启动方式</h2><ul><li>spark-submit提交任务的模板</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --class &lt;main-class&gt; \</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --jars jar_list_by_comma \</span><br><span class="line">  --conf &lt;key&gt;=&lt;value&gt; \</span><br><span class="line">  ... # other options</span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure><h2 id="4-3-spark-submit-详细参数说明"><a href="#4-3-spark-submit-详细参数说明" class="headerlink" title="4.3 spark-submit 详细参数说明"></a>4.3 spark-submit 详细参数说明</h2><table><thead><tr><th align="center">参数名</th><th>参数说明</th></tr></thead><tbody><tr><td align="center">–master</td><td>master 的地址，提交任务到哪里执行，例如 spark:&#x2F;&#x2F;host:port,   yarn, local</td></tr><tr><td align="center">–deploy-mode</td><td>在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</td></tr><tr><td align="center">–class</td><td>应用程序的主类，仅针对 java 或 scala 应用</td></tr><tr><td align="center">–name</td><td>应用程序的名称</td></tr><tr><td align="center">–jars</td><td>用逗号分隔的本地jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</td></tr><tr><td align="center">–packages</td><td>包含在driver 和executor 的  classpath 中的 jar 的 maven 坐标</td></tr><tr><td align="center">–exclude-packages</td><td>为了避免冲突   而指定不包含的 package</td></tr><tr><td align="center">–repositories</td><td>远程 repository</td></tr><tr><td align="center">–conf PROP&#x3D;VALUE</td><td>指定 spark 配置属性的值， 例如  -conf spark.executor.extraJavaOptions&#x3D;”-XX:MaxPermSize&#x3D;256m”</td></tr><tr><td align="center">–properties-file</td><td>加载的配置文件，默认为 conf&#x2F;spark-defaults.conf</td></tr><tr><td align="center">–driver-memory</td><td>Driver内存，默认 1G</td></tr><tr><td align="center">–driver-java-options</td><td>传给 driver 的额外的 Java 选项</td></tr><tr><td align="center">–driver-library-path</td><td>传给 driver 的额外的库路径</td></tr><tr><td align="center">–driver-class-path</td><td>传给 driver 的额外的类路径</td></tr><tr><td align="center">–driver-cores</td><td>Driver 的核数，默认是1。在 yarn 或者 standalone 下使用</td></tr><tr><td align="center">–executor-memory</td><td>每个 executor 的内存，默认是1G</td></tr><tr><td align="center">–total-executor-cores</td><td>所有 executor 总共的核数。仅仅在  mesos 或者 standalone 下使用</td></tr><tr><td align="center">–num-executors</td><td>启动的 executor 数量。默认为2。在 yarn 下使用</td></tr><tr><td align="center">–executor-core</td><td>每个 executor 的核数。在yarn或者standalone下使用</td></tr></tbody></table><h2 id="4-4-关于–master取值的特别说明"><a href="#4-4-关于–master取值的特别说明" class="headerlink" title="4.4 关于–master取值的特别说明"></a>4.4 关于–master取值的特别说明</h2><table><thead><tr><th>local</th><th>本地worker线程中运行spark，完全没有并行</th></tr></thead><tbody><tr><td>local[K]</td><td>在本地work线程中启动K个线程运行spark</td></tr><tr><td>local[*]</td><td>启动与本地work机器的core个数相同的线程数来运行spark</td></tr><tr><td>spark:&#x2F;&#x2F;HOST:PORT</td><td>连接指定的standalone集群的master，默认7077端口</td></tr><tr><td>mesos:&#x2F;&#x2F;HOST:PORT</td><td>连接到mesos集群，国内用的极少</td></tr><tr><td>yarn</td><td>使用yarn的cluster或者yarn的client模式连接。取决于–deploy-mode参数，由deploy-mode的取值为client或是cluster来最终决定。也可以用yarn-client或是yarn-cluster进行二合一参数使用，保留–master去掉—deploy-mode参数亦可。–master   yarn-client，相当于—master   yarn –deploy-mode client的二合一</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Spark-运行模式&quot;&gt;&lt;a href=&quot;#一：Spark-运行模式&quot; class=&quot;headerlink&quot; title=&quot;一：Spark 运行模式&quot;&gt;&lt;/a&gt;一：Spark 运行模式&lt;/h1&gt;&lt;p&gt;即作业以什么样的模式去执行，主要是单机、分布式两种方式的细节</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 相关术语</title>
    <link href="https://aiyingke.cn/blog/f278f015.html/"/>
    <id>https://aiyingke.cn/blog/f278f015.html/</id>
    <published>2023-02-19T07:00:48.000Z</published>
    <updated>2023-02-19T07:19:37.117Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：RDD-Resilient-Distributed-DataSet"><a href="#一：RDD-Resilient-Distributed-DataSet" class="headerlink" title="一：RDD (Resilient Distributed DataSet)"></a>一：RDD (Resilient Distributed DataSet)</h1><ul><li>弹性分布式数据集，是对数据集在spark存储和计算过程中的一种抽象。</li><li>是一组只读、可分区的分布式数据集合。</li><li>一个RDD 包含多个分区Partition(类似于MapReduce中的InputSplit中的block)，分区是依照一定的规则的，将具有相同规则的属性的数据记录放在一起。</li><li>横向上可切分并行计算，以分区Partition为切分后的最小存储和计算单元。</li><li>纵向上可进行内外存切换使用，即当数据在内存不足时，可以用外存磁盘来补充。</li></ul><h1 id="二：Partition-（分区）"><a href="#二：Partition-（分区）" class="headerlink" title="二：Partition （分区）"></a>二：Partition （分区）</h1><ul><li>Partition类似hadoop的Split中的block，计算是以partition为单位进行的，提供了一种划分数据的方式。</li><li>Partition的划分依据有很多，常见的有Hash分区、范围分区等，也可以自己定义的，像HDFS文件，划分的方式就和MapReduce一样，以文件的block来划分不同的partition。</li><li>一个Partition交给一个Task去计算处理。</li></ul><h1 id="三：算子"><a href="#三：算子" class="headerlink" title="三：算子"></a>三：算子</h1><ul><li>英文简称：Operator，简称op</li><li>广义上讲，对任何函数进行某一项操作都可以认为是一个算子</li><li>通俗上讲，算子即为映射、关系、变换。</li><li>MapReduce算子，主要分为两个，即为Map和Reduce两个主要操作的算子，导致灵活可用性比较差。</li><li>Spark算子，分为两大类，即为Transformation和Action类，合计有80多个。</li></ul><h1 id="四：Transformation类算子"><a href="#四：Transformation类算子" class="headerlink" title="四：Transformation类算子"></a>四：Transformation类算子</h1><ul><li>操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。</li><li>细分类<ul><li>Value数据类型的Transformation算子</li><li>Key-Value数据类型的Transfromation算子</li></ul></li></ul><h1 id="五：Action类算子"><a href="#五：Action类算子" class="headerlink" title="五：Action类算子"></a>五：Action类算子</h1><ul><li>会触发 Spark 提交作业（Job），并将数据输出 Spark 系统。</li></ul><h1 id="六：窄依赖"><a href="#六：窄依赖" class="headerlink" title="六：窄依赖"></a>六：窄依赖</h1><ul><li>如果一个父RDD的每个分区只被子RDD的一个分区使用 —-&gt; 一对一关系</li></ul><h1 id="七：宽依赖"><a href="#七：宽依赖" class="headerlink" title="七：宽依赖"></a>七：宽依赖</h1><ul><li>如果一个父RDD的每个分区要被子RDD 的多个分区使用 —-&gt; 一对多关系</li></ul><h1 id="八：Application"><a href="#八：Application" class="headerlink" title="八：Application"></a>八：Application</h1><ul><li>Spark Application的概念和MapReduce中的job或者yarn中的application类似，指的是用户编写的Spark应用程序，包含了一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码</li><li>一般是指整个Spark项目从开发、测试、布署、运行的全部。</li></ul><h1 id="九：Driver"><a href="#九：Driver" class="headerlink" title="九：Driver"></a>九：Driver</h1><ul><li>运行main函数并且创建SparkContext的程序。</li><li>称为驱动程序，Driver Program类似于hadoop的wordcount程序中的driver类的main函数。</li></ul><h1 id="十：Cluster-Manager"><a href="#十：Cluster-Manager" class="headerlink" title="十：Cluster Manager"></a>十：Cluster Manager</h1><ul><li>集群的资源管理器，在集群上获取资源的服务。如Yarn、Mesos、Spark Standalone等。</li><li>以Yarn为例，驱动程序会向Yarn申请计算我这个任务需要多少的内存，多少CPU等，后由Cluster Manager会通过调度告诉驱动程序可以使用，然后驱动程序将任务分配到既定的Worker Node上面执行。</li></ul><h1 id="十一：WorkerNode"><a href="#十一：WorkerNode" class="headerlink" title="十一：WorkerNode"></a>十一：WorkerNode</h1><ul><li>集群中任何一个可以运行spark应用代码的节点。</li><li>Worker Node就是物理机器节点，可以在上面启动Executor进程。</li></ul><h1 id="十二：Executor"><a href="#十二：Executor" class="headerlink" title="十二：Executor"></a>十二：Executor</h1><ul><li>Application运行在Worker节点上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上，每个Application都有各自独立专享的一批Executor。</li><li>Executor即为spark概念的资源容器，类比于yarn的container容器，真正承载Task的运行与管理，以多线程的方式运行Task，更加高效快速。</li></ul><h1 id="十三：Task"><a href="#十三：Task" class="headerlink" title="十三：Task"></a>十三：Task</h1><ul><li>与Hadoop中的Map Task或者Reduce Task是类同的。</li><li>分配到executor上的基本工作单元，执行实际的计算任务。</li><li>Task分为两类，即为ShuffleMapTask和ResultTask。<ul><li>ShuffleMapTask：即为Map任务和发生Shuffle的任务的操作，由Transformation操作组成，其输出结果是为下个阶段任务(ResultTask)进行做准备，不是最终要输出的结果。</li><li>ResultTask：即为Action操作触发的Job作业的最后一个阶段任务，其输出结果即为Application最终的输出或存储结果。</li></ul></li></ul><h1 id="十四：Job（作业）"><a href="#十四：Job（作业）" class="headerlink" title="十四：Job（作业）"></a>十四：Job（作业）</h1><ul><li>Spark RDD里的每个action的计算会生成一个job。</li><li>用户提交的Job会提交给DAGScheduler（Job调度器），Job会被分解成Stage去执行，每个Stage由一组相同计算规则的Task组成，该组Task也称为TaskSet，实际交由TaskScheduler去调度Task的机器执行节点，最终完成作业的执行。</li></ul><h1 id="十五：Stage（阶段）"><a href="#十五：Stage（阶段）" class="headerlink" title="十五：Stage（阶段）"></a>十五：Stage（阶段）</h1><ul><li>Stage是Job的组成部分，每个Job可以包含1个或者多个Stage。</li><li>Job切分成Stage是以Shuffle作为分隔依据，Shuffle前是一个Stage，Shuffle后是一个Stage。即为按RDD宽窄依赖来划分Stage。  </li><li>每个Job会被拆分很多组Task，每组任务被称为Stage，也可称TaskSet，一个作业可以被分为一个或多个阶段。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：RDD-Resilient-Distributed-DataSet&quot;&gt;&lt;a href=&quot;#一：RDD-Resilient-Distributed-DataSet&quot; class=&quot;headerlink&quot; title=&quot;一：RDD (Resilient Distr</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark 架构设计</title>
    <link href="https://aiyingke.cn/blog/6a386de9.html/"/>
    <id>https://aiyingke.cn/blog/6a386de9.html/</id>
    <published>2023-02-19T06:36:20.000Z</published>
    <updated>2023-02-19T07:06:56.914Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：架构总览"><a href="#一：架构总览" class="headerlink" title="一：架构总览"></a>一：架构总览</h1><p><img src="/blog/6a386de9.html/1629182444027-d40b52f7-91de-4983-9fe1-c9c7e82c2a3d.png"></p><h1 id="二：角色作用"><a href="#二：角色作用" class="headerlink" title="二：角色作用"></a>二：角色作用</h1><ul><li>Client：面向用户，对外提供接口，提交代码的入口。</li><li>Driver Program：驱动器程序，用于解耦客户端和内部实际操作，将用户程序转化为任务。</li><li>SparkContent：Spark 上下文，承接作用，用于配置上下文环境。</li><li>Cluster Manager（Resource Manager）：集群资源管理器，统一资源管理与任务调度。</li><li>Application Master：任务的执行，调度指挥者。</li><li>Worker Node：工作节点，任务的实际执行者。</li></ul><h1 id="三：角色间关系"><a href="#三：角色间关系" class="headerlink" title="三：角色间关系"></a>三：角色间关系</h1><ol><li>客户端接收到用户指令、代码；</li><li>驱动器服务于客户端，承接指令传达给集群资源管理器；</li><li>集群资源管理器根据当前情况，进行资源调度，生成一个任务调度者 AM（Application Master）</li><li>AM 给相应的工作节点分配任务；</li><li>工作节点执行任务，执行完毕，结果返回给 AM，并向资源管理器汇报自身资源情况，任务已完成当前空闲状态。</li><li>AM 接收到计算结果进行汇总，返回给客户端。</li></ol><h1 id="四：工作特性"><a href="#四：工作特性" class="headerlink" title="四：工作特性"></a>四：工作特性</h1><ul><li>内存计算</li><li>多线程</li><li>缓存</li><li>每一个 AM 都有一批专享的 Executor，以多线程方式启动多个 Task 任务，并行的线程计算任务缓存 RDD数据缓存块，存储复用的数据模块。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：架构总览&quot;&gt;&lt;a href=&quot;#一：架构总览&quot; class=&quot;headerlink&quot; title=&quot;一：架构总览&quot;&gt;&lt;/a&gt;一：架构总览&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/6a386de9.html/1629182444027-d40b52f7-9</summary>
      
    
    
    
    <category term="Spark" scheme="https://aiyingke.cn/categories/Spark/"/>
    
    
    <category term="Spark" scheme="https://aiyingke.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 数据压缩</title>
    <link href="https://aiyingke.cn/blog/40919b9a.html/"/>
    <id>https://aiyingke.cn/blog/40919b9a.html/</id>
    <published>2023-02-10T07:37:26.000Z</published>
    <updated>2023-02-10T08:42:28.062Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：概述"><a href="#一：概述" class="headerlink" title="一：概述"></a>一：概述</h1><h2 id="1-1-优缺点"><a href="#1-1-优缺点" class="headerlink" title="1.1 优缺点"></a>1.1 优缺点</h2><ul><li>压缩的优点：以减少磁盘 IO、减少磁盘存储空间。</li><li>压缩的缺点：增加 CPU 开销。</li></ul><h2 id="1-2-压缩原则"><a href="#1-2-压缩原则" class="headerlink" title="1.2 压缩原则"></a>1.2 压缩原则</h2><ul><li>运算密集型的 Job，少用压缩</li><li>IO 密集型的 Job，多用压缩</li></ul><h1 id="二：MR-支持的压缩编码"><a href="#二：MR-支持的压缩编码" class="headerlink" title="二：MR 支持的压缩编码"></a>二：MR 支持的压缩编码</h1><h2 id="2-1-压缩算法对比介绍"><a href="#2-1-压缩算法对比介绍" class="headerlink" title="2.1 压缩算法对比介绍"></a>2.1 压缩算法对比介绍</h2><table><thead><tr><th>压缩格式</th><th>Hadoop 是否自带</th><th>算法</th><th>文件扩展名</th><th>是否可切片</th><th>换成压缩格式后，原来的程序是否需要修改</th></tr></thead><tbody><tr><td>DEFLATE</td><td>是，直接使用</td><td>DEFLATE</td><td>.deflate</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>Gzip</td><td>是，直接使用</td><td>DEFLATE</td><td>.gz</td><td>否</td><td>和文本处理一样，不需要修改</td></tr><tr><td>bzip2</td><td>是，直接使用</td><td>bzip2</td><td>.bz2</td><td>是</td><td>和文本处理一样，不需要修改</td></tr><tr><td>LZO</td><td>否，需要安装</td><td>LZO</td><td>.lzo</td><td>是</td><td>需要建索引，还需要指定 输入格式</td></tr><tr><td>Snappy</td><td>是，直接使用</td><td>Snappy</td><td>.snappy</td><td>否</td><td>和文本处理一样，不需要修改</td></tr></tbody></table><h2 id="2-2-压缩性能的比较"><a href="#2-2-压缩性能的比较" class="headerlink" title="2.2 压缩性能的比较"></a>2.2 压缩性能的比较</h2><table><thead><tr><th>压缩算法</th><th>原始文件大小</th><th>压缩文件大小</th><th>压缩速度</th><th>解压速度</th></tr></thead><tbody><tr><td>gzip</td><td>8.3GB</td><td>1.8GB</td><td>17.5MB&#x2F;s</td><td>58MB&#x2F;s</td></tr><tr><td>bzip2</td><td>8.3GB</td><td>1.1GB</td><td>2.4MB&#x2F;s</td><td>9.5MB&#x2F;s</td></tr><tr><td>LZO</td><td>8.3GB</td><td>2.9GB</td><td>49.3MB&#x2F;s</td><td>74.6MB&#x2F;s</td></tr></tbody></table><p><a href="http://google.github.io/snappy/">http://google.github.io/snappy/</a></p><p>Snappy compresses at about 250 MB&#x2F;sec or more and decompresses at about 500 MB&#x2F;sec or more.</p><h1 id="三：压缩方式选择"><a href="#三：压缩方式选择" class="headerlink" title="三：压缩方式选择"></a>三：压缩方式选择</h1><p>压缩方式选择时重点考虑：</p><ul><li>压缩&#x2F;解压缩速度</li><li>压缩率（压缩后存储大小）</li><li>压缩后是否 可以支持切片。</li></ul><h2 id="3-1-Gzip-压缩"><a href="#3-1-Gzip-压缩" class="headerlink" title="3.1 Gzip 压缩"></a>3.1 Gzip 压缩</h2><ul><li>优点：压缩率比较高；</li><li>缺点：不支持 Split；压缩&#x2F;解压速度一般；</li></ul><h2 id="3-2-Bzip2-压缩"><a href="#3-2-Bzip2-压缩" class="headerlink" title="3.2 Bzip2 压缩"></a>3.2 Bzip2 压缩</h2><ul><li>优点：压缩率高；支持 Split；</li><li>缺点：压缩&#x2F;解压速度慢。</li></ul><h2 id="3-3-Lzo-压缩"><a href="#3-3-Lzo-压缩" class="headerlink" title="3.3 Lzo 压缩"></a>3.3 Lzo 压缩</h2><ul><li>优点：压缩&#x2F;解压速度比较快；支持 Split；</li><li>缺点：压缩率一般；想支持切片需要额外创建索引。</li></ul><h2 id="3-4-Snappy-压缩"><a href="#3-4-Snappy-压缩" class="headerlink" title="3.4 Snappy 压缩"></a>3.4 Snappy 压缩</h2><ul><li>优点：压缩和解压缩速度快；</li><li>缺点：不支持 Split；压缩率一般；</li></ul><h1 id="四：压缩位置选择"><a href="#四：压缩位置选择" class="headerlink" title="四：压缩位置选择"></a>四：压缩位置选择</h1><p>压缩可以在 MapReduce 作用的任意阶段启用。</p><p><img src="/blog/40919b9a.html/image-20230210155254105.png"></p><h1 id="五：压缩参数配置"><a href="#五：压缩参数配置" class="headerlink" title="五：压缩参数配置"></a>五：压缩参数配置</h1><h2 id="5-1-为了支持多种压缩-x2F-解压缩算法，Hadoop-引入了编码-x2F-解码器"><a href="#5-1-为了支持多种压缩-x2F-解压缩算法，Hadoop-引入了编码-x2F-解码器" class="headerlink" title="5.1 为了支持多种压缩&#x2F;解压缩算法，Hadoop 引入了编码&#x2F;解码器"></a>5.1 为了支持多种压缩&#x2F;解压缩算法，Hadoop 引入了编码&#x2F;解码器</h2><table><thead><tr><th>压缩格式</th><th>对应的编码&#x2F;解码器</th></tr></thead><tbody><tr><td>DEFLATE</td><td>org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td>gzip</td><td>org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td>bzip2</td><td>org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td>LZO</td><td>com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td>Snappy</td><td>org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><h2 id="5-2-要在-Hadoop-中启用压缩，可以配置如下参数"><a href="#5-2-要在-Hadoop-中启用压缩，可以配置如下参数" class="headerlink" title="5.2 要在 Hadoop 中启用压缩，可以配置如下参数"></a>5.2 要在 Hadoop 中启用压缩，可以配置如下参数</h2><table><thead><tr><th>参数</th><th>默认值</th><th>阶段</th><th>建议</th></tr></thead><tbody><tr><td>io.compression.codecs  （在 core-site.xml 中配置）</td><td>无，这个需要在命令行输入 hadoop checknative 查看</td><td>输入压缩</td><td>Hadoop 使用文件扩展 名判断是否支持某种编解码器</td></tr><tr><td>mapreduce.map.output.compr ess（在 mapred-site.xml 中 配置）</td><td>false</td><td>mapper 输出</td><td>这个参数设为 true 启 用压缩</td></tr><tr><td>mapreduce.map.output.compr ess.codec（在 mapredsite.xml 中配置）</td><td>org.apache.hadoop.io.com press.DefaultCodec</td><td>mapper 输出</td><td>企业多使用 LZO 或 Snappy 编解码器在此 阶段压缩数据</td></tr><tr><td>mapreduce.output.fileoutpu tformat.compress（在 mapred-site.xml 中配置）</td><td>false</td><td>reducer 输出</td><td>这个参数设为 true 启 用压缩</td></tr><tr><td>mapreduce.output.fileoutpu tformat.compress.codec（在 mapred-site.xml 中配置）</td><td>org.apache.hadoop.io.com press.DefaultCodec</td><td>reducer 输出</td><td>使用标准工具或者编 解码器，如 gzip 和 bzip2</td></tr></tbody></table><h1 id="六：压缩实操案例"><a href="#六：压缩实操案例" class="headerlink" title="六：压缩实操案例"></a>六：压缩实操案例</h1><h2 id="6-1-Map-输出端采用压缩"><a href="#6-1-Map-输出端采用压缩" class="headerlink" title="6.1 Map 输出端采用压缩"></a>6.1 Map 输出端采用压缩</h2><p>即使你的 MapReduce 的输入输出文件都是未压缩的文件，你仍然可以对 Map 任务的中 间结果输出做压缩，因为它要写在硬盘并且通过网络传输到 Reduce 节点，对其压缩可以提高很多性能。</p><p>Hadoop 源码支持的压缩格式有：BZip2Codec、DefaultCodec</p><h3 id="（1）驱动器-Driver"><a href="#（1）驱动器-Driver" class="headerlink" title="（1）驱动器 Driver"></a>（1）驱动器 Driver</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.mapCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 16:28 2023/2/10</span></span><br><span class="line"><span class="comment">         * Description: 开启map端输出压缩</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">        configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output888&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-保持不变"><a href="#（2）Mapper-保持不变" class="headerlink" title="（2）Mapper 保持不变"></a>（2）Mapper 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.mapCompress;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-保持不变"><a href="#（3）Reducer-保持不变" class="headerlink" title="（3）Reducer 保持不变"></a>（3）Reducer 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.mapCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:41 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="6-2-Reduce-输出端采用压缩"><a href="#6-2-Reduce-输出端采用压缩" class="headerlink" title="6.2 Reduce 输出端采用压缩"></a>6.2 Reduce 输出端采用压缩</h2><h3 id="（1）驱动器-Driver-1"><a href="#（1）驱动器-Driver-1" class="headerlink" title="（1）驱动器 Driver"></a>（1）驱动器 Driver</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.reduceCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.BZip2Codec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.CompressionCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.DefaultCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.compress.GzipCodec;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 16:28 2023/2/10</span></span><br><span class="line"><span class="comment">         * Description: 开启map端输出压缩</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        configuration.setBoolean(<span class="string">&quot;mapreduce.map.output.compress&quot;</span>, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置map端输出压缩方式</span></span><br><span class="line">        configuration.setClass(<span class="string">&quot;mapreduce.map.output.compress.codec&quot;</span>, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setCombinerClass(WordCountReducer.class);</span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 16:33 2023/2/10</span></span><br><span class="line"><span class="comment">         * Description: 设置reduce端输出压缩开启</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileOutputFormat.setCompressOutput(job, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置压缩的方式</span></span><br><span class="line"><span class="comment">//        FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span></span><br><span class="line"><span class="comment">//        FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);</span></span><br><span class="line">        FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output666&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-保持不变-1"><a href="#（2）Mapper-保持不变-1" class="headerlink" title="（2）Mapper 保持不变"></a>（2）Mapper 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.reduceCompress;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-保持不变-1"><a href="#（3）Reducer-保持不变-1" class="headerlink" title="（3）Reducer 保持不变"></a>（3）Reducer 保持不变</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.compress.reduceCompress;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:41 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：概述&quot;&gt;&lt;a href=&quot;#一：概述&quot; class=&quot;headerlink&quot; title=&quot;一：概述&quot;&gt;&lt;/a&gt;一：概述&lt;/h1&gt;&lt;h2 id=&quot;1-1-优缺点&quot;&gt;&lt;a href=&quot;#1-1-优缺点&quot; class=&quot;headerlink&quot; title=&quot;1.1</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce 开发总结</title>
    <link href="https://aiyingke.cn/blog/6e775d30.html/"/>
    <id>https://aiyingke.cn/blog/6e775d30.html/</id>
    <published>2023-02-10T06:58:36.000Z</published>
    <updated>2023-02-10T07:37:33.108Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：输入数据接口：InputFormat"><a href="#一：输入数据接口：InputFormat" class="headerlink" title="一：输入数据接口：InputFormat"></a>一：输入数据接口：InputFormat</h1><ul><li>默认使用的实现类是：TextInputFormat</li><li>TextInputFormat 的功能逻辑是：一次读一行文本，然后将该行的起始偏移量作为 key，行内容作为 value 返回。</li><li>CombineTextInputFormat 可以把多个小文件合并成一个切片处理，提高处理效率。</li></ul><h1 id="二：逻辑处理接口：Mapper"><a href="#二：逻辑处理接口：Mapper" class="headerlink" title="二：逻辑处理接口：Mapper"></a>二：逻辑处理接口：Mapper</h1><ul><li>用户根据业务需求实现其中三个方法：map() setup() cleanup ()</li></ul><h1 id="三：Partitioner-分区"><a href="#三：Partitioner-分区" class="headerlink" title="三：Partitioner 分区"></a>三：Partitioner 分区</h1><ul><li>有默认实现 HashPartitioner，逻辑是根据 key 的哈希值和 numReduces 来返回一个 分区号；key.hashCode()&amp;Integer.MAXVALUE % numReduces</li><li>如果业务上有特别的需求，可以自定义分区。</li></ul><h1 id="四：Comparable-排序"><a href="#四：Comparable-排序" class="headerlink" title="四：Comparable 排序"></a>四：Comparable 排序</h1><ul><li>当我们用自定义的对象作为 key 来输出时，就必须要实现 WritableComparable 接 口，重写其中的 compareTo()方法。</li><li>部分排序：对最终输出的每一个文件进行内部排序。</li><li>全排序：对所有数据进行排序，通常只有一个 Reduce。</li><li>二次排序：排序的条件有两个。</li></ul><h1 id="五：Combiner-合并"><a href="#五：Combiner-合并" class="headerlink" title="五：Combiner 合并"></a>五：Combiner 合并</h1><ul><li>Combiner 合并可以提高程序执行效率，减少 IO 传输。但是使用时必须不能影响原有的 业务处理结果。</li></ul><h1 id="六：逻辑处理接口：Reducer"><a href="#六：逻辑处理接口：Reducer" class="headerlink" title="六：逻辑处理接口：Reducer"></a>六：逻辑处理接口：Reducer</h1><ul><li>用户根据业务需求实现其中三个方法：reduce() setup() cleanup ()</li></ul><h1 id="七：输出数据接口：OutputFormat"><a href="#七：输出数据接口：OutputFormat" class="headerlink" title="七：输出数据接口：OutputFormat"></a>七：输出数据接口：OutputFormat</h1><ul><li>用户根据业务需求实现其中三个方法：reduce() setup() cleanup ()</li><li>用户还可以自定义 OutputFormat。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：输入数据接口：InputFormat&quot;&gt;&lt;a href=&quot;#一：输入数据接口：InputFormat&quot; class=&quot;headerlink&quot; title=&quot;一：输入数据接口：InputFormat&quot;&gt;&lt;/a&gt;一：输入数据接口：InputFormat&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce-ETL数据清洗</title>
    <link href="https://aiyingke.cn/blog/e164e43a.html/"/>
    <id>https://aiyingke.cn/blog/e164e43a.html/</id>
    <published>2023-02-10T04:41:19.000Z</published>
    <updated>2023-02-10T05:49:10.717Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：数据清洗（ETL）"><a href="#一：数据清洗（ETL）" class="headerlink" title="一：数据清洗（ETL）"></a>一：数据清洗（ETL）</h1><p>“ETL，是英文 Extract-Transform-Load 的缩写，用来描述将数据从来源端经过抽取 （Extract）、转换（Transform）、加载（Load）至目的端的过程。ETL 一词较常用在数据仓库，但其对象并不限于数据仓库。</p><p>在运行核心业务 MapReduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户 要求的数据。<code>清理的过程往往只需要运行 Mapper 程序，不需要运行 Reduce 程序。</code></p><h1 id="二：案例分析"><a href="#二：案例分析" class="headerlink" title="二：案例分析"></a>二：案例分析</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>去除日志中字段个数小于等于 11 的日志。</p><p>（1）输入数据：web.log</p><p>（2）期望输出数据：每行字段长度都大于 11。</p><h2 id="2-2-需求分析"><a href="#2-2-需求分析" class="headerlink" title="2.2 需求分析"></a>2.2 需求分析</h2><p>在 Map 阶段对输入的数据根据规则进行过滤清洗。</p><h2 id="2-3-代码实现"><a href="#2-3-代码实现" class="headerlink" title="2.3 代码实现"></a>2.3 代码实现</h2><h3 id="（1）编写-WebLogMapper-类"><a href="#（1）编写-WebLogMapper-类" class="headerlink" title="（1）编写 WebLogMapper 类"></a>（1）编写 WebLogMapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.ETL;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 12:52 2023/2/10</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.解析日志</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">res</span> <span class="operator">=</span> parseLog(line, context);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.日志不合法退出</span></span><br><span class="line">        <span class="keyword">if</span> (!res) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.日志合法直接写出</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 12:57 2023/2/10</span></span><br><span class="line"><span class="comment">     * Description: 日志清洗规则类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">boolean</span> <span class="title function_">parseLog</span><span class="params">(String line, Context context)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.按空格分割</span></span><br><span class="line">        <span class="keyword">final</span> String[] s = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.过滤日志长度大于11的数据</span></span><br><span class="line">        <span class="keyword">if</span> (s.length &gt; <span class="number">11</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-WebLogDriver-类"><a href="#（2）编写-WebLogDriver-类" class="headerlink" title="（2）编写 WebLogDriver 类"></a>（2）编写 WebLogDriver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.ETL;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 13:01 2023/2/10</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WebLogDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="comment">// 参数：输入输出路径</span></span><br><span class="line">        args = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&quot;Y:\\Temp\\input&quot;</span>, <span class="string">&quot;Y:\\Temp\\output2&quot;</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取job信息</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 加载jar包</span></span><br><span class="line">        job.setJarByClass(WebLogMapper.class);</span><br><span class="line">        <span class="comment">// 关联map</span></span><br><span class="line">        job.setMapperClass(WebLogMapper.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置最终输出类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置 ReduceTask 个数为0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置输入和输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 提交</span></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：数据清洗（ETL）&quot;&gt;&lt;a href=&quot;#一：数据清洗（ETL）&quot; class=&quot;headerlink&quot; title=&quot;一：数据清洗（ETL）&quot;&gt;&lt;/a&gt;一：数据清洗（ETL）&lt;/h1&gt;&lt;p&gt;“ETL，是英文 Extract-Transform-Load 的缩</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce Join 应用</title>
    <link href="https://aiyingke.cn/blog/bb94c1b7.html/"/>
    <id>https://aiyingke.cn/blog/bb94c1b7.html/</id>
    <published>2023-02-01T08:10:56.000Z</published>
    <updated>2023-02-03T07:01:03.329Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Reduce-Join"><a href="#一：Reduce-Join" class="headerlink" title="一：Reduce Join"></a>一：Reduce Join</h1><p>​Map 端的主要工作：为来自不同表或文件的 key&#x2F;value 对，打标签以区别不同来源的记 录。然后用连接字段作为 key，其余部分和新加的标志作为 value，最后进行输出。</p><p>​Reduce 端的主要工作：在 Reduce 端以连接字段作为 key 的分组已经完成，我们只需要 在每一个分组当中将那些来源于不同文件的记录（在 Map 阶段已经打标志）分开，最后进 行合并就 ok 了。</p><h1 id="二：Reduce-Join-案例实操"><a href="#二：Reduce-Join-案例实操" class="headerlink" title="二：Reduce Join 案例实操"></a>二：Reduce Join 案例实操</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>订单数据表</p><img src="/blog/bb94c1b7.html/image-20230201175235245.png" style="zoom:50%;"><p>商品信息表</p><img src="/blog/bb94c1b7.html/image-20230201175323217.png" alt="image-20230201175323217" style="zoom:50%;"><img src="/blog/bb94c1b7.html/image-20230201175344682.png" alt="image-20230201175344682" style="zoom:50%;"><p>将商品信息表中数据根据商品 pid 合并到订单数据表中。</p><p>最终数据形式：</p><img src="/blog/bb94c1b7.html/image-20230201175431751.png" style="zoom:50%;"><h2 id="2-2-需求分析"><a href="#2-2-需求分析" class="headerlink" title="2.2 需求分析"></a>2.2 需求分析</h2><p>​通过将关联条件作为 Map 输出的 key，将两表满足 Join 条件的数据并携带数据所来源 的文件信息，发往同一个 ReduceTask，在 Reduce 中进行数据的串联。</p><h2 id="2-3-Reduce端表合并（数据倾斜）"><a href="#2-3-Reduce端表合并（数据倾斜）" class="headerlink" title="2.3 Reduce端表合并（数据倾斜）"></a>2.3 Reduce端表合并（数据倾斜）</h2><p><img src="/blog/bb94c1b7.html/image-20230201175657723.png"></p><h2 id="2-4-代码实现"><a href="#2-4-代码实现" class="headerlink" title="2.4 代码实现"></a>2.4 代码实现</h2><h3 id="（1）创建商品和订单合并后的-TableBean-类"><a href="#（1）创建商品和订单合并后的-TableBean-类" class="headerlink" title="（1）创建商品和订单合并后的 TableBean 类"></a>（1）创建商品和订单合并后的 TableBean 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.NoArgsConstructor;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:01 2023/2/1</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String id;  <span class="comment">//订单ID</span></span><br><span class="line">    <span class="keyword">private</span> String pid;  <span class="comment">//产品ID</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> amount;  <span class="comment">//产品数量</span></span><br><span class="line">    <span class="keyword">private</span> String pName;  <span class="comment">//产品名称</span></span><br><span class="line">    <span class="keyword">private</span> String flag;  <span class="comment">//判断订单表（order）或者产品表（pd）的标志字段</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 最终输出形态</span></span><br><span class="line">        <span class="keyword">return</span> id + <span class="string">&quot;\t&quot;</span> + pName + <span class="string">&quot;\t&quot;</span> + amount;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeUTF(id);</span><br><span class="line">        dataOutput.writeUTF(pid);</span><br><span class="line">        dataOutput.writeInt(amount);</span><br><span class="line">        dataOutput.writeUTF(pName);</span><br><span class="line">        dataOutput.writeUTF(flag);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.id = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.pid = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.amount = dataInput.readInt();</span><br><span class="line">        <span class="built_in">this</span>.pName = dataInput.readUTF();</span><br><span class="line">        <span class="built_in">this</span>.flag = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-TableMapper-类"><a href="#（2）编写-TableMapper-类" class="headerlink" title="（2）编写 TableMapper 类"></a>（2）编写 TableMapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:38 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, TableBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String filename;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">TableBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取对应文件名称</span></span><br><span class="line">        <span class="type">InputSplit</span> <span class="variable">split</span> <span class="operator">=</span> context.getInputSplit();</span><br><span class="line">        <span class="type">FileSplit</span> <span class="variable">fileSplit</span> <span class="operator">=</span> (FileSplit) split;</span><br><span class="line">        filename = fileSplit.getPath().getName();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, TableBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 获取一行</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 判断是哪个文件，然后针对文件进行不同的操作</span></span><br><span class="line">        String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (filename.contains(<span class="string">&quot;order&quot;</span>)) &#123;</span><br><span class="line">            <span class="comment">// 订单表的处理</span></span><br><span class="line">            <span class="comment">// 封装 outK</span></span><br><span class="line">            outK.set(split[<span class="number">1</span>]); <span class="comment">//两表相同的字段 用于进入同一个reduce     pid</span></span><br><span class="line">            <span class="comment">// 封装 outV</span></span><br><span class="line">            outV.setId(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setPid(split[<span class="number">1</span>]);</span><br><span class="line">            outV.setAmount(Integer.parseInt(split[<span class="number">2</span>])); <span class="comment">// int 类型</span></span><br><span class="line">            outV.setPName(<span class="string">&quot; &quot;</span>);  <span class="comment">// 该表中未含有这个字段，设置为空</span></span><br><span class="line">            outV.setFlag(<span class="string">&quot;order&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 商品表的处理</span></span><br><span class="line">            <span class="comment">// 封装 outK</span></span><br><span class="line">            outK.set(split[<span class="number">0</span>]);</span><br><span class="line">            <span class="comment">// 封装 outV</span></span><br><span class="line">            outV.setId(<span class="string">&quot; &quot;</span>);</span><br><span class="line">            outV.setPid(split[<span class="number">0</span>]);</span><br><span class="line">            outV.setAmount(<span class="number">0</span>); <span class="comment">// int 类型</span></span><br><span class="line">            outV.setPName(split[<span class="number">1</span>]);  <span class="comment">// 该表中未含有这个字段，设置为空</span></span><br><span class="line">            outV.setFlag(<span class="string">&quot;pd&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 写出 KV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）编写-TableReducer-类"><a href="#（3）编写-TableReducer-类" class="headerlink" title="（3）编写 TableReducer 类"></a>（3）编写 TableReducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.beanutils.BeanUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationTargetException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:59 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, TableBean, TableBean, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;TableBean&gt; values, Reducer&lt;Text, TableBean, TableBean, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        ArrayList&lt;TableBean&gt; orderBeans = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="type">TableBean</span> <span class="variable">pdBean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (TableBean value : values) &#123;</span><br><span class="line">            <span class="comment">// 判断数据来自哪个表</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="string">&quot;order&quot;</span>.equals(value.getFlag())) &#123;</span><br><span class="line">                <span class="comment">// 订单表</span></span><br><span class="line">                <span class="comment">// 创建一个临时TableBean对象接收value,不可以直接进行赋值，hadoop内部进行了优化 传递过来的对象仅有地址，因此直接赋值 只会保留最后一个对象的信息</span></span><br><span class="line">                <span class="type">TableBean</span> <span class="variable">tmpOrderBean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TableBean</span>();</span><br><span class="line"></span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(tmpOrderBean, value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException | IllegalAccessException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 将临时创建的对象 添加进入集合中</span></span><br><span class="line">                orderBeans.add(tmpOrderBean);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    BeanUtils.copyProperties(pdBean,value);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InvocationTargetException | IllegalAccessException e) &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//遍历集合 orderBeans,替换掉每个 orderBean 的 pid 为 pName,然后写出</span></span><br><span class="line">        <span class="keyword">for</span> (TableBean orderBean : orderBeans) &#123;</span><br><span class="line">            <span class="comment">// 同pid 进同一个reduce 故而pName 唯一</span></span><br><span class="line">            orderBean.setPName(pdBean.getPName());</span><br><span class="line">            <span class="comment">// 写出修改后的对象</span></span><br><span class="line">            context.write(orderBean,NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）编写-TableDriver-类"><a href="#（4）编写-TableDriver-类" class="headerlink" title="（4）编写 TableDriver 类"></a>（4）编写 TableDriver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.reduceJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:16 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(TableDriver.class);</span><br><span class="line">        job.setMapperClass(TableMapper.class);</span><br><span class="line">        job.setReducerClass(TableReducer.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(TableBean.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(TableBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input\\&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output5\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）运行结果"><a href="#（5）运行结果" class="headerlink" title="（5）运行结果"></a>（5）运行结果</h3><img src="/blog/bb94c1b7.html/image-20230203142810183.png" alt="image-20230203142810183" style="zoom: 67%;"><h3 id="（6）总结"><a href="#（6）总结" class="headerlink" title="（6）总结"></a>（6）总结</h3><p>​缺点：这种方式中，合并的操作是在 Reduce 阶段完成，Reduce 端的处理压力太大，Map 节点的运算负载则很低，资源利用率不高，且在 Reduce 阶段极易产生数据倾斜。</p><p>​解决方案：Map 端实现数据合并。</p><h1 id="三：Map-Join"><a href="#三：Map-Join" class="headerlink" title="三：Map Join"></a>三：Map Join</h1><h2 id="3-1-应用场景"><a href="#3-1-应用场景" class="headerlink" title="3.1 应用场景"></a>3.1 应用场景</h2><p>Map Join 适用于一张表十分小、一张表很大的场景。</p><h2 id="3-2-优点"><a href="#3-2-优点" class="headerlink" title="3.2 优点"></a>3.2 优点</h2><p>在 Reduce 端处理过多的表，非常容易产生数据倾斜；</p><p>在 Map 端缓存多张表，提前处理业务逻辑，这样增加 Map 端业务，减少 Reduce 端数 据的压力，尽可能的减少数据倾斜。</p><h2 id="3-3-实现手段——采用-DistributedCache"><a href="#3-3-实现手段——采用-DistributedCache" class="headerlink" title="3.3 实现手段——采用 DistributedCache"></a>3.3 实现手段——采用 DistributedCache</h2><p>（1）在 Mapper 的 setup 阶段，将文件读取到缓存集合中。</p><p>（2）在 Driver 驱动类中加载缓存。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//缓存普通文件到 Task 运行节点。</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///y:/Temp/input/pd.txt&quot;</span>));</span><br><span class="line"><span class="comment">//如果是集群运行,需要设置 HDFS 路径</span></span><br><span class="line">job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;hdfs://hadoop101:8020/tmp/cache/pd.txt&quot;</span>));</span><br></pre></td></tr></table></figure><h1 id="四：Map-Join-案例实操"><a href="#四：Map-Join-案例实操" class="headerlink" title="四：Map Join 案例实操"></a>四：Map Join 案例实操</h1><h2 id="4-1-需求"><a href="#4-1-需求" class="headerlink" title="4.1 需求"></a>4.1 需求</h2><p><img src="/blog/bb94c1b7.html/image-20230203143257142.png"></p><h2 id="4-2-需求分析"><a href="#4-2-需求分析" class="headerlink" title="4.2 需求分析"></a>4.2 需求分析</h2><p>MapJoin 适用于关联表中有小表的情形；</p><p><img src="/blog/bb94c1b7.html/image-20230203143357885.png"></p><h2 id="4-3-代码实现"><a href="#4-3-代码实现" class="headerlink" title="4.3 代码实现"></a>4.3 代码实现</h2><h3 id="（1）先在-MapJoinDriver-驱动类中添加缓存文件"><a href="#（1）先在-MapJoinDriver-驱动类中添加缓存文件" class="headerlink" title="（1）先在 MapJoinDriver 驱动类中添加缓存文件"></a>（1）先在 MapJoinDriver 驱动类中添加缓存文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.mapJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.net.URISyntaxException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:16 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException, URISyntaxException &#123;</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(<span class="keyword">new</span> <span class="title class_">Configuration</span>());</span><br><span class="line"></span><br><span class="line">        job.setJarByClass(TableDriver.class);</span><br><span class="line">        job.setMapperClass(TableMapper.class);</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">         * CreateTime: 14:37 2023/2/3</span></span><br><span class="line"><span class="comment">         * Description: 加载缓存数据</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.addCacheFile(<span class="keyword">new</span> <span class="title class_">URI</span>(<span class="string">&quot;file:///Y:/Temp/pd.txt&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//Map 端 Join 的逻辑不需要 Reduce 阶段，设置 reduceTask 数量为 0</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\input\\&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output2\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）在-MapJoinMapper-类中的-setup-方法中读取缓存文件"><a href="#（2）在-MapJoinMapper-类中的-setup-方法中读取缓存文件" class="headerlink" title="（2）在 MapJoinMapper 类中的 setup 方法中读取缓存文件"></a>（2）在 MapJoinMapper 类中的 setup 方法中读取缓存文件</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.mapJoin;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang3.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.nio.charset.StandardCharsets;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:38 2023/2/2</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TableMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Map&lt;String, String&gt; pdMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Author: Rupert-Tears</span></span><br><span class="line"><span class="comment">     * CreateTime: 14:43 2023/2/3</span></span><br><span class="line"><span class="comment">     * Description: 任务开始前 先将pd数据缓存进入 pdMap</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 通过缓存文件得到小表数据 pd.txt</span></span><br><span class="line">        <span class="keyword">final</span> URI[] cacheFiles = context.getCacheFiles();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(cacheFiles[<span class="number">0</span>]);</span><br><span class="line">        <span class="comment">// 获取文件系统对象 并开启流</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(context.getConfiguration());</span><br><span class="line">        <span class="keyword">final</span> <span class="type">FSDataInputStream</span> <span class="variable">open</span> <span class="operator">=</span> fileSystem.open(path);</span><br><span class="line">        <span class="comment">// 通过包装流转换为reader,方便按行读取</span></span><br><span class="line">        <span class="type">BufferedReader</span> <span class="variable">reader</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(open, StandardCharsets.UTF_8));</span><br><span class="line">        <span class="comment">// 逐行读取、按行处理</span></span><br><span class="line">        String line;</span><br><span class="line">        <span class="keyword">while</span> (StringUtils.isNotEmpty(line = reader.readLine())) &#123;</span><br><span class="line">            <span class="comment">// 切割一行     11 小米</span></span><br><span class="line">            <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            pdMap.put(split[<span class="number">0</span>], split[<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 关闭流</span></span><br><span class="line">        IOUtils.closeStream(reader);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 读取大表数据</span></span><br><span class="line">        <span class="comment">// 1001 11 2</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">        <span class="comment">// 通过大表的每行数据的pid 去取出pdMap 中的pName</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">pName</span> <span class="operator">=</span> pdMap.get(split[<span class="number">1</span>]);</span><br><span class="line">        <span class="comment">// 将大表每行数据的pid替换为pName</span></span><br><span class="line">        text.set(split[<span class="number">0</span>] + <span class="string">&quot;\t&quot;</span> + pName + <span class="string">&quot;\t&quot;</span> + split[<span class="number">2</span>]);</span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(text, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Reduce-Join&quot;&gt;&lt;a href=&quot;#一：Reduce-Join&quot; class=&quot;headerlink&quot; title=&quot;一：Reduce Join&quot;&gt;&lt;/a&gt;一：Reduce Join&lt;/h1&gt;&lt;p&gt;​		Map 端的主要工作：为来自不同表或文件的 k</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce 内核源码解析</title>
    <link href="https://aiyingke.cn/blog/77ff229c.html/"/>
    <id>https://aiyingke.cn/blog/77ff229c.html/</id>
    <published>2022-12-19T04:04:08.000Z</published>
    <updated>2023-02-01T07:06:40.694Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：MapTask-工作机制"><a href="#一：MapTask-工作机制" class="headerlink" title="一：MapTask 工作机制"></a>一：MapTask 工作机制</h1><p><img src="/blog/77ff229c.html/image-20221219121005329.png"></p><p>（1）Read 阶段：MapTask 通过 InputFormat 获得的 RecordReader，从输入 InputSplit 中 解析出一个个 key&#x2F;value。</p><p>（2）Map 阶段：该节点主要是将解析出的 key&#x2F;value 交给用户编写 map()函数处理，并 产生一系列新的 key&#x2F;value。</p><p>（3）Collect 收集阶段：在用户编写 map()函数中，当数据处理完成后，一般会调用 OutputCollector.collect()输出结果。在该函数内部，它会将生成的 key&#x2F;value 分区（调用 Partitioner），并写入一个环形内存缓冲区中。</p><p>（4）Spill 阶段：即“溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上， 生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p><p><strong>溢写阶段详情：</strong></p><p>步骤 1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号 Partition 进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在 一起，且同一分区内所有数据按照 key 有序。</p><p>步骤 2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文 件 output&#x2F;spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之 前，对每个分区中的数据进行一次聚集操作。</p><p>步骤 3：将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元 信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大 小超过 1MB，则将内存索引写到文件 output&#x2F;spillN.out.index 中。</p><p>（5）Merge 阶段：当所有数据处理完成后，MapTask 对所有临时文件进行一次合并， 以确保最终只会生成一个数据文件。</p><p>当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件 output&#x2F;file.out 中，同时生成相应的索引文件 output&#x2F;file.out.index。 </p><p>在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多 轮递归合并的方式。每轮合并 mapreduce.task.io.sort.factor（默认 10）个文件，并将产生的文 件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。 </p><p>让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量 小文件产生的随机读取带来的开销。</p><h1 id="二：ReduceTask-工作机制"><a href="#二：ReduceTask-工作机制" class="headerlink" title="二：ReduceTask 工作机制"></a>二：ReduceTask 工作机制</h1><p><img src="/blog/77ff229c.html/image-20221219121913332.png"></p><p>（1）Copy 阶段：ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数 据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p><p>（2）Sort 阶段：在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁 盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。按照 MapReduce 语义，用 户编写 reduce()函数输入数据是按 key 进行聚集的一组数据。为了将 key 相同的数据聚在一 起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经实现对自己的处理结果进行了 局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。</p><p>（3）Reduce 阶段：reduce()函数将计算结果写到 HDFS 上。</p><h1 id="三：ReduceTask-并行度决定机制"><a href="#三：ReduceTask-并行度决定机制" class="headerlink" title="三：ReduceTask 并行度决定机制"></a>三：ReduceTask 并行度决定机制</h1><p>MapTask 并行度由切片个数决定，切片个数由输入文件和切片规则决定。</p><h2 id="3-1-设置-ReduceTask-并行度（个数）"><a href="#3-1-设置-ReduceTask-并行度（个数）" class="headerlink" title="3.1 设置 ReduceTask 并行度（个数）"></a>3.1 设置 ReduceTask 并行度（个数）</h2><p>ReduceTask 的并行度同样影响整个 Job 的执行并发度和执行效率，但与 MapTask 的并发数由切片数决定不同，ReduceTask 数量的决定是可以直接手动设置</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认值是 1，手动设置为 4</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">4</span>);</span><br></pre></td></tr></table></figure><h2 id="3-2-实验：测试-ReduceTask-多少合适"><a href="#3-2-实验：测试-ReduceTask-多少合适" class="headerlink" title="3.2 实验：测试 ReduceTask 多少合适?"></a>3.2 实验：测试 ReduceTask 多少合适?</h2><p>（1）实验环境：1 个 Master 节点，16 个 Slave 节点：CPU:8GHZ，内存: 2G</p><p>（2）实验结论：</p><p>观察实验数据，可知满足正态分布，故而可以得出最优解。</p><p><img src="/blog/77ff229c.html/image-20221219122251264.png"></p><h2 id="3-2-注意事项"><a href="#3-2-注意事项" class="headerlink" title="3.2 注意事项"></a>3.2 注意事项</h2><ul><li>ReduceTask&#x3D;0，表示没有Reduce阶段，输出文件个数和Map个数一致。</li><li>ReduceTask默认值就是1，所以输出文件个数为一个。</li><li>如果数据分布不均匀，就有可能在Reduce阶段产生数据倾斜。</li><li>ReduceTask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全 局汇总结果，就只能有1个ReduceTask。</li><li>具体多少个ReduceTask，需要根据集群性能而定。</li><li>如果分区数不是1，但是ReduceTask为1，是否执行分区过程。答案是：不执行分区过程。因为在MapTask的源码中，执行分区的前提是先判断ReduceNum个数是否大于1。不大于1 肯定不执行。</li></ul><h1 id="四：MapTask-amp-ReduceTask-源码解析"><a href="#四：MapTask-amp-ReduceTask-源码解析" class="headerlink" title="四：MapTask &amp; ReduceTask 源码解析"></a>四：MapTask &amp; ReduceTask 源码解析</h1><h2 id="4-1-MapTask-源码解析流程"><a href="#4-1-MapTask-源码解析流程" class="headerlink" title="4.1 MapTask 源码解析流程"></a>4.1 MapTask 源码解析流程</h2><p><img src="/blog/77ff229c.html/image-20230201145206556.png"></p><h2 id="4-2-ReduceTask-源码解析流程"><a href="#4-2-ReduceTask-源码解析流程" class="headerlink" title="4.2 ReduceTask 源码解析流程"></a>4.2 ReduceTask 源码解析流程</h2><p><img src="/blog/77ff229c.html/image-20230201150523559.png"></p><p><img src="/blog/77ff229c.html/image-20230201150555973.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：MapTask-工作机制&quot;&gt;&lt;a href=&quot;#一：MapTask-工作机制&quot; class=&quot;headerlink&quot; title=&quot;一：MapTask 工作机制&quot;&gt;&lt;/a&gt;一：MapTask 工作机制&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/blog/77ff22</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce OutputFormat数据输出</title>
    <link href="https://aiyingke.cn/blog/83c17171.html/"/>
    <id>https://aiyingke.cn/blog/83c17171.html/</id>
    <published>2022-12-19T01:55:35.000Z</published>
    <updated>2022-12-19T03:20:56.570Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：OutputFormat-接口实现类"><a href="#一：OutputFormat-接口实现类" class="headerlink" title="一：OutputFormat 接口实现类"></a>一：OutputFormat 接口实现类</h1><h2 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h2><p>OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat 接口。下面我们介绍几种常见的OutputFormat实现类。</p><h2 id="1-2-OutputFormat实现类"><a href="#1-2-OutputFormat实现类" class="headerlink" title="1.2 OutputFormat实现类"></a>1.2 OutputFormat实现类</h2><p><img src="/blog/83c17171.html/image-20221219095707511.png"></p><h2 id="1-3-默认输出格式TextOutputFormat"><a href="#1-3-默认输出格式TextOutputFormat" class="headerlink" title="1.3 默认输出格式TextOutputFormat"></a>1.3 默认输出格式TextOutputFormat</h2><h2 id="1-4-自定义OutputFormat"><a href="#1-4-自定义OutputFormat" class="headerlink" title="1.4 自定义OutputFormat"></a>1.4 自定义OutputFormat</h2><h3 id="（1）-应用场景"><a href="#（1）-应用场景" class="headerlink" title="（1） 应用场景"></a>（1） 应用场景</h3><p>例如：输出数据到MySQL&#x2F;HBase&#x2F;Elasticsearch等存储框架中。</p><h3 id="（2）自定义OutputFormat步骤"><a href="#（2）自定义OutputFormat步骤" class="headerlink" title="（2）自定义OutputFormat步骤"></a>（2）自定义OutputFormat步骤</h3><ul><li>自定义一个类继承FileOutputFormat</li><li>改写RecordWriter，具体改写输出数据的方法write()</li></ul><h1 id="二：自定义-OutputFormat-案例实操"><a href="#二：自定义-OutputFormat-案例实操" class="headerlink" title="二：自定义 OutputFormat 案例实操"></a>二：自定义 OutputFormat 案例实操</h1><h2 id="2-1-需求"><a href="#2-1-需求" class="headerlink" title="2.1 需求"></a>2.1 需求</h2><p>过滤输入的 log 日志，包含 aiyingke 的网站输出到 e:&#x2F;aiyingke.log，不包含 aiyingke 的网站输出到 e:&#x2F;other.log；</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>log.txt</li></ul><h3 id="（2）期望输出数据"><a href="#（2）期望输出数据" class="headerlink" title="（2）期望输出数据"></a>（2）期望输出数据</h3><ul><li>aiyingke.log</li><li>other.log</li></ul><h2 id="2-2-需求分析"><a href="#2-2-需求分析" class="headerlink" title="2.2 需求分析"></a>2.2 需求分析</h2><ul><li>需求</li><li>输入数据</li><li>输出数据</li><li>自定义一个 OutputFormat 类<ul><li>创建一个类LogRecordWriter继承RecordWriter</li><li>创建两个文件的输出流：aiyingke，other</li><li>如果输入数据包含aiyingke，输出到aiyingkeOut流 如果不包含aiyingke，输出到otherOut流</li></ul></li><li>驱动类 Driver<ul><li>要将自定义的输出格式组件设置到job中</li><li>job.setOutputFormatClass(LogOutputFormat.class) ;</li></ul></li></ul><h2 id="2-3-代码实现"><a href="#2-3-代码实现" class="headerlink" title="2.3 代码实现"></a>2.3 代码实现</h2><h3 id="（1）编写-LogMapper-类"><a href="#（1）编写-LogMapper-类" class="headerlink" title="（1）编写 LogMapper 类"></a>（1）编写 LogMapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:08 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// https://aiyingke.cn</span></span><br><span class="line">        <span class="comment">// 互换 KV 位置</span></span><br><span class="line">        context.write(value, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）编写-LogReducer-类"><a href="#（2）编写-LogReducer-类" class="headerlink" title="（2）编写 LogReducer 类"></a>（2）编写 LogReducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:11 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, NullWritable, Text, NullWritable&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;NullWritable&gt; values, Reducer&lt;Text, NullWritable, Text, NullWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// key 相同情况,进入同一个reduce,遍历结果</span></span><br><span class="line">        <span class="keyword">for</span> (NullWritable value : values) &#123;</span><br><span class="line">            <span class="comment">// 写出</span></span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）自定义一个-LogOutputFormat-类"><a href="#（3）自定义一个-LogOutputFormat-类" class="headerlink" title="（3）自定义一个 LogOutputFormat 类"></a>（3）自定义一个 LogOutputFormat 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:15 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogOutputFormat</span> <span class="keyword">extends</span> <span class="title class_">FileOutputFormat</span>&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> RecordWriter&lt;Text, NullWritable&gt; <span class="title function_">getRecordWriter</span><span class="params">(TaskAttemptContext job)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 创建自定义的 LogRecordWriter</span></span><br><span class="line">        <span class="type">LogRecordWriter</span> <span class="variable">lrw</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LogRecordWriter</span>(job);</span><br><span class="line">        <span class="keyword">return</span> lrw;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）编写-LogRecordWriter-类"><a href="#（4）编写-LogRecordWriter-类" class="headerlink" title="（4）编写 LogRecordWriter 类"></a>（4）编写 LogRecordWriter 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 10:44 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogRecordWriter</span> <span class="keyword">extends</span> <span class="title class_">RecordWriter</span>&lt;Text, NullWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> FSDataOutputStream otherOut;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> FSDataOutputStream aiyingkeOut;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">LogRecordWriter</span><span class="params">(TaskAttemptContext job)</span> &#123;</span><br><span class="line">        <span class="comment">// 获取文件系统</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="type">FileSystem</span> <span class="variable">fileSystem</span> <span class="operator">=</span> FileSystem.get(job.getConfiguration());</span><br><span class="line">            <span class="comment">// 用文件系统对象创建两个输出流对应不同的目录</span></span><br><span class="line">            aiyingkeOut = fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\logOut\\aiyingke\\aiyingke.log&quot;</span>));</span><br><span class="line">            otherOut = fileSystem.create(<span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\logOut\\other\\other.log&quot;</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(e);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(Text text, NullWritable nullWritable)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="comment">// 根据当前行是否包含 aiyingke 决定采用哪个流输出</span></span><br><span class="line">        <span class="keyword">if</span> (line.contains(<span class="string">&quot;aiyingke&quot;</span>)) &#123;</span><br><span class="line">            aiyingkeOut.writeBytes(line + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            otherOut.writeBytes(line + <span class="string">&quot;\n&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        IOUtils.closeStream(aiyingkeOut);</span><br><span class="line">        IOUtils.closeStream(otherOut);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）编写-LogDriver-类"><a href="#（5）编写-LogDriver-类" class="headerlink" title="（5）编写 LogDriver 类"></a>（5）编写 LogDriver 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.outputformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 11:04 2022/12/19</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LogDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;Log&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(LogDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper  reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(LogMapper.class);</span><br><span class="line">        job.setReducerClass(LogReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 设置自定义的 OutputFormat</span></span><br><span class="line">        job.setOutputFormatClass(LogOutputFormat.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         *         虽然我们定义了 OutputFormat ,但是因为我们的 OutputFormat 继承自 FileOutputFormat</span></span><br><span class="line"><span class="comment">         *         而 FileOutputFormat 要输出一个 _SUCCESS 的标志文件,所以这里换得指定一个输出目录</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\log\\&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\logOut\\&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：OutputFormat-接口实现类&quot;&gt;&lt;a href=&quot;#一：OutputFormat-接口实现类&quot; class=&quot;headerlink&quot; title=&quot;一：OutputFormat 接口实现类&quot;&gt;&lt;/a&gt;一：OutputFormat 接口实现类&lt;/h1&gt;&lt;</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mapreduce Combiner合并</title>
    <link href="https://aiyingke.cn/blog/b3e50fdd.html/"/>
    <id>https://aiyingke.cn/blog/b3e50fdd.html/</id>
    <published>2022-12-18T10:32:39.000Z</published>
    <updated>2022-12-18T11:28:28.458Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：Combiner合并概述"><a href="#一：Combiner合并概述" class="headerlink" title="一：Combiner合并概述"></a>一：Combiner合并概述</h1><p>（1）Combiner是MR程序中Mapper和Reducer之外的一种组件。</p><p>（2）Combiner组件的父类就是Reducer。</p><p>（3）Combiner和Reducer的区别在于运行的位置</p><ul><li>Combiner是在每一个MapTask所在的节点运行</li><li>Reducer是接收全局所有Mapper的输出结果</li></ul><p>（4）Combiner的意义就是对每一个MapTask的输出进行局部汇总，以减小网络传输量。</p><p>（5）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv 应该跟Reducer的输入kv类型要对应起来。以下求平均值就不能够使用 Combiner；</p><p><img src="/blog/b3e50fdd.html/image-20221218183450549.png"></p><h1 id="二：自定义-Combiner-实现步骤"><a href="#二：自定义-Combiner-实现步骤" class="headerlink" title="二：自定义 Combiner 实现步骤"></a>二：自定义 Combiner 实现步骤</h1><h2 id="2-1-自定义一个-Combiner-继承-Reducer，重写-Reduce-方法"><a href="#2-1-自定义一个-Combiner-继承-Reducer，重写-Reduce-方法" class="headerlink" title="2.1 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法"></a>2.1 自定义一个 Combiner 继承 Reducer，重写 Reduce 方法</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">```</span><br><span class="line"></span><br><span class="line">## 2.2 在 Job 驱动类中设置</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 三：Combiner 合并案例实操</span><br><span class="line"></span><br><span class="line">## 3.1 需求</span><br><span class="line"></span><br><span class="line">统计过程中对每一个 MapTask 的输出进行局部汇总，以减小网络传输量即采用 Combiner 功能。</span><br><span class="line"></span><br><span class="line">### （1）数据输入</span><br><span class="line"></span><br><span class="line">- data.txt</span><br><span class="line"></span><br><span class="line">### （2）期望输出数据</span><br><span class="line"></span><br><span class="line">- Combine 输入数据多，输出时经过合并，输出数据降低。</span><br><span class="line"></span><br><span class="line">## 3.2 需求分析</span><br><span class="line"></span><br><span class="line">对每一个MapTask的输出局部汇总（Combiner）；</span><br><span class="line"></span><br><span class="line">![](./Mapreduce-Combiner合并/image-20221218183845643.png)</span><br><span class="line"></span><br><span class="line">## 3.3 代码实现（方案一）</span><br><span class="line"></span><br><span class="line">### （1）增加一个 WordCountCombiner 类继承 Reducer</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">package cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Author: Rupert Tears</span><br><span class="line"> * Date: Created in 19:18 2022/12/18</span><br><span class="line"> * Description: Thought is already is late, exactly is the earliest time.</span><br><span class="line"> */</span><br><span class="line">public class WordCountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    private final IntWritable outV = new IntWritable();</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context) throws IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        // 定义聚合变量</span><br><span class="line">        int sum = 0;</span><br><span class="line"></span><br><span class="line">        // 遍历求和</span><br><span class="line">        for (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 封装 outKV</span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        // 写出</span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）在-WordcountDriver-驱动类中指定-Combiner"><a href="#（2）在-WordcountDriver-驱动类中指定-Combiner" class="headerlink" title="（2）在 WordcountDriver 驱动类中指定 Combiner"></a>（2）在 WordcountDriver 驱动类中指定 Combiner</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置自定义的 Combiner</span></span><br><span class="line">job.setCombinerClass(WordCountCombiner.class);</span><br></pre></td></tr></table></figure><h2 id="3-4-代码实现（方案二）"><a href="#3-4-代码实现（方案二）" class="headerlink" title="3.4 代码实现（方案二）"></a>3.4 代码实现（方案二）</h2><h3 id="（1）将-WordcountReducer-作为-Combiner-在-WordcountDriver-驱动类中指定"><a href="#（1）将-WordcountReducer-作为-Combiner-在-WordcountDriver-驱动类中指定" class="headerlink" title="（1）将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定"></a>（1）将 WordcountReducer 作为 Combiner 在 WordcountDriver 驱动类中指定</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setCombinerClass(WordCountReducer.class);</span><br></pre></td></tr></table></figure><h2 id="3-5-代码汇总"><a href="#3-5-代码汇总" class="headerlink" title="3.5 代码汇总"></a>3.5 代码汇总</h2><h3 id="（1）驱动器类"><a href="#（1）驱动器类" class="headerlink" title="（1）驱动器类"></a>（1）驱动器类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       1.拿到配置环境变量</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       2.拿到job作业对象</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(configuration, <span class="string">&quot;WordCount-MR&quot;</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       3.设置主类，（即：告知集群入口jar包在哪）</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setJarByClass(WordCountDriver.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *      4.设置 mapper combiner reducer</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        <span class="comment">// 设置自定义的 Combiner</span></span><br><span class="line">        job.setCombinerClass(WordCountCombiner.class);</span><br><span class="line"><span class="comment">//        job.setCombinerClass(WordCountReducer.class);</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       5.设置输出 key value的类型</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat.class);</span><br><span class="line">        CombineTextInputFormat.setMaxInputSplitSize(job, <span class="number">4194304</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       6.设置输入、输出的数据路径</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\asdsad&quot;</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *       7.提交执行</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）Mapper-类"><a href="#（2）Mapper-类" class="headerlink" title="（2）Mapper 类"></a>（2）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:40 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个常用的IntWritable对象，对 java 整形的封装</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">intWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 暂存一个 String 的封装类</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 覆写map方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> InterruptedException 1.对输入的文件，以行为单位进行切分，按规则切分（空格）；</span></span><br><span class="line"><span class="comment">     *                              2.遍历切分完成后的集合或迭代器</span></span><br><span class="line"><span class="comment">     *                              3.组合&lt;key,value&gt;,通过Context进行输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Mapper&lt;Object, Text, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 将传入的数据转换为字符串 指定分割符号为逗号</span></span><br><span class="line">        <span class="type">StringTokenizer</span> <span class="variable">stringTokenizer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString(), <span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="comment">// 循环获取每个逗号分割出来的元素</span></span><br><span class="line">        <span class="keyword">while</span> (stringTokenizer.hasMoreTokens()) &#123;</span><br><span class="line">            <span class="comment">// 接收获取元素</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">word</span> <span class="operator">=</span> stringTokenizer.nextToken();</span><br><span class="line">            <span class="comment">// 添加分割后的元素到封装类中</span></span><br><span class="line">            text.set(word);</span><br><span class="line">            <span class="comment">// 上下文调度</span></span><br><span class="line">            context.write(text, intWritable);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Reducer-类"><a href="#（3）Reducer-类" class="headerlink" title="（3）Reducer 类"></a>（3）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 23:41 2022/12/16</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 定义一个存储 int类型的求和结果的变量</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="type">IntWritable</span> <span class="variable">sumIntWritable</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 以key相同的组为单位处理，即 Reducer 方法调用一次，处理同 key的组数据</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 定义一个总词频变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 循环计数</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">            count += val.get();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 将频数封装</span></span><br><span class="line">        sumIntWritable.set(count);</span><br><span class="line">        <span class="comment">// 上下文调度</span></span><br><span class="line">        context.write(key, sumIntWritable);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）Combiner-类"><a href="#（4）Combiner-类" class="headerlink" title="（4）Combiner 类"></a>（4）Combiner 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 19:18 2022/12/18</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCountCombiner</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">IntWritable</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Reducer&lt;Text, IntWritable, Text, IntWritable&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义聚合变量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 遍历求和</span></span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 封装 outKV</span></span><br><span class="line">        outV.set(sum);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 写出</span></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">// 日志输出</span><br><span class="line">Combine input records=6</span><br><span class="line">Combine output records=3</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：Combiner合并概述&quot;&gt;&lt;a href=&quot;#一：Combiner合并概述&quot; class=&quot;headerlink&quot; title=&quot;一：Combiner合并概述&quot;&gt;&lt;/a&gt;一：Combiner合并概述&lt;/h1&gt;&lt;p&gt;（1）Combiner是MR程序中Mappe</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Mapreduce WritableComparable排序</title>
    <link href="https://aiyingke.cn/blog/59213c23.html/"/>
    <id>https://aiyingke.cn/blog/59213c23.html/</id>
    <published>2022-12-18T09:34:38.000Z</published>
    <updated>2022-12-18T10:31:24.465Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：排序概述"><a href="#一：排序概述" class="headerlink" title="一：排序概述"></a>一：排序概述</h1><ul><li>排序是MapReduce框架中最重要的操作之一。</li><li>MapTask和ReduceTask均会对数据按 照key进行排序。该操作属于 Hadoop的默认行为。<code>任何应用程序中的数据均会被排序，而不管逻辑上是否需要</code>。</li><li>默认排序是按照<code>字典顺序排序</code>，且实现该排序的方法是<code>快速排序</code>。</li><li>对于MapTask，它会将处理的结果暂时放到环形缓冲区中，<code>当环形缓冲区使 用率达到一定阈值后，再对缓冲区中的数据进行一次快速排序</code>，并将这些有序数据溢写到磁盘上，而<code>当数据处理完毕后，它会对磁盘上所有文件进行归并排序</code>。</li><li>对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件，如果文件大 小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到 一定阈值，则进行一次归并排序以生成一个更大文件；如果内存中文件大小或者 数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完 毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。</li></ul><h1 id="二：排序分类"><a href="#二：排序分类" class="headerlink" title="二：排序分类"></a>二：排序分类</h1><h2 id="（1）部分排序"><a href="#（1）部分排序" class="headerlink" title="（1）部分排序"></a>（1）部分排序</h2><p>MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部有序。</p><h2 id="（2）全排序"><a href="#（2）全排序" class="headerlink" title="（2）全排序"></a>（2）全排序</h2><p>最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。但该方法在 处理大型文件时效率极低，因为一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。</p><h2 id="（3）辅助排序：（GroupingComparator分组）"><a href="#（3）辅助排序：（GroupingComparator分组）" class="headerlink" title="（3）辅助排序：（GroupingComparator分组）"></a>（3）辅助排序：（GroupingComparator分组）</h2><p>在Reduce端对key进行分组。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部 字段比较不相同）的key进入到同一个reduce方法时，可以采用分组排序。</p><h2 id="（4）二次排序"><a href="#（4）二次排序" class="headerlink" title="（4）二次排序"></a>（4）二次排序</h2><p>在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。</p><h1 id="三：自定义排序-WritableComparable-原理分析"><a href="#三：自定义排序-WritableComparable-原理分析" class="headerlink" title="三：自定义排序 WritableComparable 原理分析"></a>三：自定义排序 WritableComparable 原理分析</h1><p>bean 对象做为 key 传输，需要<code>实现 WritableComparable 接口重写 compareTo 方法</code>，就可 以实现排序。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean bean)</span> &#123;</span><br><span class="line"><span class="type">int</span> result;</span><br><span class="line"><span class="comment">// 按照总流量大小，倒序排列</span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; bean.getSumFlow()) &#123;</span><br><span class="line">result = -<span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; bean.getSumFlow()) &#123;</span><br><span class="line">result = <span class="number">1</span>;</span><br><span class="line">&#125;<span class="keyword">else</span> &#123;</span><br><span class="line">result = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="四：WritableComparable-排序案例实操（全排序）"><a href="#四：WritableComparable-排序案例实操（全排序）" class="headerlink" title="四：WritableComparable 排序案例实操（全排序）"></a>四：WritableComparable 排序案例实操（全排序）</h1><h2 id="4-1-需求"><a href="#4-1-需求" class="headerlink" title="4.1 需求"></a>4.1 需求</h2><p>对 phone_data .txt 中的数据，按照总流量进行倒叙排序；</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>phone_data .txt</li></ul><h3 id="（2）期望输出数据"><a href="#（2）期望输出数据" class="headerlink" title="（2）期望输出数据"></a>（2）期望输出数据</h3><p><img src="/blog/59213c23.html/image-20221218174341483.png"></p><h2 id="4-2-需求分析"><a href="#4-2-需求分析" class="headerlink" title="4.2 需求分析"></a>4.2 需求分析</h2><p><img src="/blog/59213c23.html/image-20221218174423794.png"></p><h2 id="4-3-代码实现"><a href="#4-3-代码实现" class="headerlink" title="4.3 代码实现"></a>4.3 代码实现</h2><h3 id="（1）FlowBean-对象实现-WritableComparable，增加比较功能"><a href="#（1）FlowBean-对象实现-WritableComparable，增加比较功能" class="headerlink" title="（1）FlowBean 对象实现 WritableComparable，增加比较功能"></a>（1）FlowBean 对象实现 WritableComparable，增加比较功能</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 实现WritableComparable接口</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 覆写compareTo方法</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 按照总流量比较,倒叙排列</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="（2）实体类"><a href="#（2）实体类" class="headerlink" title="（2）实体类"></a>（2）实体类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:40 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(FlowBean o)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 按照总流量比较,倒叙排列</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &gt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="built_in">this</span>.sumFlow &lt; o.sumFlow) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（3）Mapper-类"><a href="#（3）Mapper-类" class="headerlink" title="（3）Mapper 类"></a>（3）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:59 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, FlowBean, Text&gt; &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Text</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, FlowBean,Text &gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="comment">// 13736230513 2481 24681 27162</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割数据</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.封装 outK outV</span></span><br><span class="line">        outK.setUpFlow(Long.parseLong(split[<span class="number">1</span>]));</span><br><span class="line">        outK.setDownFlow(Long.parseLong(split[<span class="number">2</span>]));</span><br><span class="line">        outK.setSumFlow();</span><br><span class="line">        outV.set(split[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.写出 outK outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）Reducer-类"><a href="#（4）Reducer-类" class="headerlink" title="（4）Reducer 类"></a>（4）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:07 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;FlowBean, Text, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(FlowBean key, Iterable&lt;Text&gt; values, Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历values集合,循环写出,避免总流量相同的情况</span></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            <span class="comment">// 调换 KV 位置,反向写出</span></span><br><span class="line">            context.write(value, key);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）驱动器类"><a href="#（5）驱动器类" class="headerlink" title="（5）驱动器类"></a>（5）驱动器类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:14 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取job对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.关联 driver mapper reducer</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.设置 map 端输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean.class);</span><br><span class="line">        job.setMapOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.设置程序最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.设置程序输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\phone_data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6.提交job</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="五：WritableComparable-排序案例实操（区内排序）"><a href="#五：WritableComparable-排序案例实操（区内排序）" class="headerlink" title="五：WritableComparable 排序案例实操（区内排序）"></a>五：WritableComparable 排序案例实操（区内排序）</h1><h2 id="5-1-需求"><a href="#5-1-需求" class="headerlink" title="5.1 需求"></a>5.1 需求</h2><p>要求每个省份手机号输出的文件中按照总流量内部排序。</p><h2 id="5-2-需求分析"><a href="#5-2-需求分析" class="headerlink" title="5.2 需求分析"></a>5.2 需求分析</h2><p>基于前一个需求，增加自定义分区类，分区按照省份手机号设置。</p><p><img src="/blog/59213c23.html/image-20221218181040515.png"></p><h2 id="5-3-代码实现"><a href="#5-3-代码实现" class="headerlink" title="5.3 代码实现"></a>5.3 代码实现</h2><h3 id="（1）增加自定义分区类"><a href="#（1）增加自定义分区类" class="headerlink" title="（1）增加自定义分区类"></a>（1）增加自定义分区类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.writableComparableWithPartition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 18:13 2022/12/18</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner2</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;FlowBean, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(FlowBean flowBean, Text text, <span class="type">int</span> i)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取手机号前三位</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">prePhone</span> <span class="operator">=</span> phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义一个分区号变量 partition,根据 perPhone 设置分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line">        <span class="keyword">if</span> (<span class="string">&quot;136&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">0</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;137&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;138&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">2</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="string">&quot;139&quot;</span>.equals(prePhone)) &#123;</span><br><span class="line">            partition = <span class="number">3</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            partition = <span class="number">4</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 返回分区号</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）FlowDriver-设置分区类和-Reduce-数量"><a href="#（2）FlowDriver-设置分区类和-Reduce-数量" class="headerlink" title="（2）FlowDriver  设置分区类和 Reduce 数量"></a>（2）FlowDriver  设置分区类和 Reduce 数量</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置自定义分区器</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner2.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置对应的 ReduceTask 的个数</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h2 id="5-4-二次排序"><a href="#5-4-二次排序" class="headerlink" title="5.4 二次排序"></a>5.4 二次排序</h2><ul><li><p>若想要进行区内排序的二次排序，比如对某省份手机号的总流量进行降序；若总流量相同，按照上行流量降序排列；</p></li><li><p>仅需要在FlowBean中，对 compareTo 方法进行业务逻辑的增加。</p></li><li><pre><code class="java">    @Override    public int compareTo(FlowBean o) &#123;        // 按照总流量比较,倒叙排列（降序）        if (this.sumFlow &gt; o.sumFlow) &#123;            return -1;        &#125; else if (this.sumFlow &lt; o.sumFlow) &#123;            return 1;        &#125; else &#123;            /*             * Author: Rupert-Tears             * CreateTime: 18:29 2022/12/18             * Description: 二次排序,对上行流量升序排列             */            if (this.upFlow &gt; o.upFlow) &#123;                return 1;            &#125; else if (this.upFlow &lt; o.upFlow) &#123;                return -1;            &#125; else &#123;                return 0;            &#125;        &#125;    &#125;</code></pre></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：排序概述&quot;&gt;&lt;a href=&quot;#一：排序概述&quot; class=&quot;headerlink&quot; title=&quot;一：排序概述&quot;&gt;&lt;/a&gt;一：排序概述&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;排序是MapReduce框架中最重要的操作之一。&lt;/li&gt;
&lt;li&gt;MapTask和Reduce</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce Partition分区</title>
    <link href="https://aiyingke.cn/blog/ee4a420a.html/"/>
    <id>https://aiyingke.cn/blog/ee4a420a.html/</id>
    <published>2022-12-18T08:59:47.000Z</published>
    <updated>2022-12-18T09:31:50.705Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一：问题引出"><a href="#一：问题引出" class="headerlink" title="一：问题引出"></a>一：问题引出</h1><p>要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机 归属地不同省份输出到不同文件中（分区）</p><h1 id="二：默认Partitioner分区"><a href="#二：默认Partitioner分区" class="headerlink" title="二：默认Partitioner分区"></a>二：默认Partitioner分区</h1><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HashPartitioner</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;K, V&gt; &#123;</span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(K key, V value, <span class="type">int</span> numReduceTasks)</span> &#123;</span><br><span class="line"><span class="keyword">return</span> (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>默认分区是根据key的hashCode对ReduceTasks个数取模得到的。用户没法控制哪个 key存储到哪个分区。</li></ul><h1 id="三：自定义Partitioner-步骤"><a href="#三：自定义Partitioner-步骤" class="headerlink" title="三：自定义Partitioner 步骤"></a>三：自定义Partitioner 步骤</h1><h2 id="3-1-自定义类继承Partitioner，重写getPartition-方法"><a href="#3-1-自定义类继承Partitioner，重写getPartition-方法" class="headerlink" title="3.1 自定义类继承Partitioner，重写getPartition()方法"></a>3.1 自定义类继承Partitioner，重写getPartition()方法</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text key, FlowBean value, <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line"><span class="comment">// 控制分区代码逻辑</span></span><br><span class="line">… …</span><br><span class="line"><span class="keyword">return</span> partition;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-在Job驱动中，设置自定义Partitioner"><a href="#3-2-在Job驱动中，设置自定义Partitioner" class="headerlink" title="3.2 在Job驱动中，设置自定义Partitioner"></a>3.2 在Job驱动中，设置自定义Partitioner</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(CustomPartitioner.class);</span><br></pre></td></tr></table></figure><h2 id="3-3-自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask"><a href="#3-3-自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask" class="headerlink" title="3.3 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask"></a>3.3 自定义Partition后，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h1 id="四：分区总结"><a href="#四：分区总结" class="headerlink" title="四：分区总结"></a>四：分区总结</h1><p>（1）如果ReduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；</p><p>（2）如果1&lt;ReduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；</p><p>（3）如 果ReduceTask的数量&#x3D;1，则不管MapTask端输出多少个分区文件，最终结果都交给这一个 ReduceTask，最终也就只会产生一个结果文件 part-r-00000；</p><p>（4）分区号必须从零开始，逐一累加。</p><h1 id="五：案例分析"><a href="#五：案例分析" class="headerlink" title="五：案例分析"></a>五：案例分析</h1><ul><li>例如：假设自定义分区数为5，则 <ul><li>（1）job.setNumReduceTasks(1); </li><li>（2）job.setNumReduceTasks(2); </li><li>（3）job.setNumReduceTasks(6); 会正常运行，只不过会产生一个输出文件 会报错 大于5，程序会正常运行，会产生空文件</li></ul></li></ul><h1 id="六：Partition-分区案例实操"><a href="#六：Partition-分区案例实操" class="headerlink" title="六：Partition 分区案例实操"></a>六：Partition 分区案例实操</h1><h2 id="6-1-需求"><a href="#6-1-需求" class="headerlink" title="6.1 需求"></a>6.1 需求</h2><p>将统计结果按照手机归属地不同省份输出到不同文件中（分区）；</p><h3 id="（1）输入数据"><a href="#（1）输入数据" class="headerlink" title="（1）输入数据"></a>（1）输入数据</h3><ul><li>phone_data .txt</li></ul><h3 id="（2）期望输出数据"><a href="#（2）期望输出数据" class="headerlink" title="（2）期望输出数据"></a>（2）期望输出数据</h3><ul><li>手机号 136、137、138、139 开头都分别放到一个独立的 4 个文件中，其他开头的放到 一个文件中。</li></ul><h2 id="6-2-需求分析"><a href="#6-2-需求分析" class="headerlink" title="6.2 需求分析"></a>6.2 需求分析</h2><p><img src="/blog/ee4a420a.html/image-20221218171052800.png"></p><h2 id="6-3-代码实现"><a href="#6-3-代码实现" class="headerlink" title="6.3 代码实现"></a>6.3 代码实现</h2><h3 id="（1）增加分区类"><a href="#（1）增加分区类" class="headerlink" title="（1）增加分区类"></a>（1）增加分区类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 17:17 2022/12/18</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProvincePartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;Text, FlowBean&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(Text text, FlowBean flowBean, <span class="type">int</span> i)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取手机号前三位 prePhone</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> text.toString();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">prePhone</span> <span class="operator">=</span> phone.substring(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义一个分区号变量partition,根据prePhone设置分区号</span></span><br><span class="line">        <span class="type">int</span> partition;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span> (prePhone) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;136&quot;</span>:</span><br><span class="line">                partition = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;137&quot;</span>:</span><br><span class="line">                partition = <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;138&quot;</span>:</span><br><span class="line">                partition = <span class="number">2</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">&quot;139&quot;</span>:</span><br><span class="line">                partition = <span class="number">3</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                partition = <span class="number">4</span>;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最后返回分区号 partition</span></span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（2）在驱动函数中增加自定义数据分区设置和-ReduceTask-设置"><a href="#（2）在驱动函数中增加自定义数据分区设置和-ReduceTask-设置" class="headerlink" title="（2）在驱动函数中增加自定义数据分区设置和 ReduceTask 设置"></a>（2）在驱动函数中增加自定义数据分区设置和 ReduceTask 设置</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定自定义分区</span></span><br><span class="line">job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"><span class="comment">// 同时指定相应数量的 ReduceTask</span></span><br><span class="line">job.setNumReduceTasks(<span class="number">5</span>);</span><br></pre></td></tr></table></figure><h3 id="（3）驱动器类"><a href="#（3）驱动器类" class="headerlink" title="（3）驱动器类"></a>（3）驱动器类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:14 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowDriver</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取job对象</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">Configuration</span> <span class="variable">configuration</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">final</span> <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.关联 driver mapper reducer</span></span><br><span class="line">        job.setJarByClass(FlowDriver.class);</span><br><span class="line">        job.setMapperClass(FlowMapper.class);</span><br><span class="line">        job.setReducerClass(FlowReducer.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.设置 map 端输出 KV 类型</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.设置程序最终输出 KV 类型</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(FlowBean.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定自定义分区</span></span><br><span class="line">        job.setPartitionerClass(ProvincePartitioner.class);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 同时指定相应数量的 ReduceTask</span></span><br><span class="line">        job.setNumReduceTasks(<span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.设置程序输入输出路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\phone_data.txt&quot;</span>));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&quot;Y:\\Temp\\output1234\\&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 6.提交job</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">boolean</span> <span class="variable">b</span> <span class="operator">=</span> job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">        System.exit(b ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（4）Mapper-类"><a href="#（4）Mapper-类" class="headerlink" title="（4）Mapper 类"></a>（4）Mapper 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:59 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">Text</span> <span class="variable">outK</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.获取一行数据</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> value.toString();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.切割数据</span></span><br><span class="line">        <span class="keyword">final</span> String[] split = line.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.抓取手机号、上行流量、下行流量</span></span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">phone</span> <span class="operator">=</span> split[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">upFlow</span> <span class="operator">=</span> split[split.length - <span class="number">3</span>];</span><br><span class="line">        <span class="keyword">final</span> <span class="type">String</span> <span class="variable">downFlow</span> <span class="operator">=</span> split[split.length - <span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.封装 outK outV</span></span><br><span class="line">        outK.set(phone);</span><br><span class="line">        outV.setUpFlow(Long.parseLong(upFlow));</span><br><span class="line">        outV.setDownFlow(Long.parseLong(downFlow));</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 5.写出 outK outV</span></span><br><span class="line">        context.write(outK, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（5）Reducer-类"><a href="#（5）Reducer-类" class="headerlink" title="（5）Reducer 类"></a>（5）Reducer 类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 22:07 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, FlowBean, Text, FlowBean&gt; &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="type">FlowBean</span> <span class="variable">outV</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FlowBean</span>();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">long</span> <span class="variable">totalUpFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">long</span> <span class="variable">totalDownFlow</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1.遍历values,将其中的上行流量和下行流量分别累计</span></span><br><span class="line">        <span class="keyword">for</span> (FlowBean value : values) &#123;</span><br><span class="line">            totalUpFlow += value.getUpFlow();</span><br><span class="line">            totalDownFlow += value.getDownFlow();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.封装 outKV</span></span><br><span class="line">        outV.setUpFlow(totalUpFlow);</span><br><span class="line">        outV.setDownFlow(totalDownFlow);</span><br><span class="line">        outV.setSumFlow();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3.写出 outK outV</span></span><br><span class="line">        context.write(key, outV);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="（6）实体类"><a href="#（6）实体类" class="headerlink" title="（6）实体类"></a>（6）实体类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.aiyingke.mapreduce.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> lombok.Getter;</span><br><span class="line"><span class="keyword">import</span> lombok.Setter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Author: Rupert Tears</span></span><br><span class="line"><span class="comment"> * Date: Created in 21:40 2022/12/17</span></span><br><span class="line"><span class="comment"> * Description: Thought is already is late, exactly is the earliest time.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Getter</span></span><br><span class="line"><span class="meta">@Setter</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FlowBean</span> <span class="keyword">implements</span> <span class="title class_">Writable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downFlow;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> sumFlow;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">FlowBean</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSumFlow</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = <span class="built_in">this</span>.upFlow + <span class="built_in">this</span>.downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        dataOutput.writeLong(upFlow);</span><br><span class="line">        dataOutput.writeLong(downFlow);</span><br><span class="line">        dataOutput.writeLong(sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="built_in">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="built_in">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow + <span class="string">&quot;\t&quot;</span> + downFlow + <span class="string">&quot;\t&quot;</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一：问题引出&quot;&gt;&lt;a href=&quot;#一：问题引出&quot; class=&quot;headerlink&quot; title=&quot;一：问题引出&quot;&gt;&lt;/a&gt;一：问题引出&lt;/h1&gt;&lt;p&gt;要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机 归属地不同省份输出到不同文件中（分</summary>
      
    
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/categories/Hadoop/"/>
    
    
    <category term="Hadoop" scheme="https://aiyingke.cn/tags/Hadoop/"/>
    
  </entry>
  
</feed>
